---
title: "sPLS performance"
output: html_document
date: "2025-07-30"
---



# Introduction

This page presents an application of the sPLS performance assessment. The sPLS method is a quite particular method : there are several predictions according to the components number selected in the model. The goal is almost to choose the best number of component in sPLS regression in order to compute the best possible predictions but also to select the best number of variables. For that, we will use three datasets: 

- one is a dataset with only one response variable $Y$.

- the other is a dataset with four response variables $Y = (Y1,Y2,Y3,Y4)$.

- the last dataset contains real data about NIR spectra.

To access to predefined functions from sgPLSdevelop package and manipulate these datasets, run these lines :

```{r data, message=FALSE, warning=FALSE}
library(sgPLSdevelop)
library(pls)

data1 <- data.create(p = 10, list = TRUE)
data2 <- data.create(p = 10, q = 4, list = TRUE)
data(yarn)
data3 <- yarn
```
```{r, echo=FALSE}
print(paste("First dataset dimensions :",nrow(data1$D),"x",ncol(data1$D)))
print(paste("Second dataset dimensions :",nrow(data2$D),"x",ncol(data2$D)))
print(paste("Yarn dataset dimensions :",nrow(data3),"x",ncol(data3)))
```


For the two first datasets, the population is set to $n = 40$ by default, which is close to actual conditions. 
Let's also notice that, on average, the response $Y$ is a linear combination from the predictors $X$. Indeed, the function includes a matrix product $Y = XB + E$ with $B$ the weight matrix and $E$ matrix the gaussian noise. This linearity condition is important in order to have a good performance of the model, the PLS method using linearity combinaison. 

Now, it's time to train a PLS model for each dataset built or imported.

```{r pls, message=FALSE, warning=FALSE}
ncomp.max <- 8

# First model
X <- data1$X
Y <- data1$Y
model1 <- sPLS(X,Y,mode = "regression", ncomp = ncomp.max)

# Second model
X <- data2$X
Y <- data2$Y
model2 <- sPLS(X,Y,mode = "regression", ncomp = ncomp.max)

# Third model
X <- data3$NIR
Y <- data3$density
model3 <- sPLS(X,Y,mode = "regression", ncomp = ncomp.max)
```


# sPLS performance assessment using MSEP

An good way to assess such a model performance consists by using $MSEP$ criterion. $MSEP$ is computed as follow :

$MSEP = \frac{1}{nq} \sum_{i=1}^{n} \sum_{j=1}^{q} (Y_{i,j} - \hat{Y}_{i,j})^2$

```{r, echo=FALSE}
tuning.sPLS.XY <- function(object, K=nrow(object$X), ncomp = object$ncomp){
  
  X <- object$X
  Y <- object$Y
  n <- nrow(X)
  p <- ncol(X)
  q <- ncol(Y)
  keepX = object$keepX
  keepY = object$keepY
  
  # PART 1 : number of components selection
  
  ## prediction analysis
  err <- matrix(NA, nrow = K, ncol = ncomp)
  
  b <- floor(n/K) # block size
  ind <- sample(n)
  
  for(k in seq_len(K)){
    
    ## bloc definition
    ind.beg <- (k-1) * b + 1 # block k beginning
    ind.end <- k * b # block k end
    ind.test <- ind[ind.beg:ind.end]
    nk <- length(ind.test)
    X.train <- X[-ind.test,]
    Y.train <- Y[-ind.test,]
    X.test <- X[ind.test,]
    Y.test <- Y[ind.test,]
    modele <- sPLS(X = X.train, Y = Y.train, ncomp = ncomp, mode = "regression", keepX = keepX, keepY = keepY)  
    
    for(h in 1:ncomp){  
      
      ## predictions
      pred <- predict.sPLS(modele, newdata = X.test)$predict[,,h]
      err[k,h] <- sum(colSums(as.matrix((Y.test - pred)^2)))
      
    }
  }
  
  err.moy <- colSums(err)/b/K
  
  h.best <- min(which.min(err.moy))
  
  par(mfrow = c(1,2))
  plot(err.moy, col="blue", pch = 16, type = "b", main = "MSEP of the model", xlab = "number of components", ylab = "MSEP")
  
  
  # PART 2 : number of variables selection
  
  ## prediction analysis
  err <- matrix(0, nrow = q, ncol = p)
  
  for(k in seq_len(K)){
    
    ## bloc definition
    ind.beg <- (k-1) * b + 1 # block k beginning
    ind.end <- k * b # block k end
    ind.test <- ind[ind.beg:ind.end]
    X.train <- X[-ind.test,]
    Y.train <- Y[-ind.test,]
    X.test <- X[ind.test,]
    Y.test <- Y[ind.test,]
    
    
    for(i in 1:q){  
      
      for(j in 1:p){
        
        modele <- sPLS(X = X.train, Y = Y.train, ncomp = h.best, mode = "regression", keepX = rep(j,h.best), keepY = rep(i,h.best))  
      
        ## predictions
        pred <- predict.sPLS(modele, newdata = X.test)$predict[,,h.best]
        err[i,j] <- err[i,j] + sum(colSums(as.matrix((Y.test - pred)^2)))
      
      }
    }
  }
  
  err.moy2 <- err/b/K
  err.min <- min(err.moy2)
  
  # min error coordinates identification
  ind.best <- numeric(2)
  for(i in 1:q){
    for(j in 1:p){
      if(err.moy2[i,j] == err.min){ind.best <- c(j,i)}
    }
  }

  nb.keepX <- ind.best[1]
  nb.keepY <- ind.best[2]
  keepX.best <- rep(nb.keepX,h.best)
  keepY.best <- rep(nb.keepY,h.best)

  round.err <- round(err.moy2,3)
  
  xmin <- max(nb.keepX-2.5,0.5)
  xmax <- min(nb.keepX+2.5,p+0.5)
  ymin <- max(nb.keepY-4.5,0.5)
  ymax <- min(nb.keepY+4.5,q+0.5)
  
  if(q==1){
    plot(as.vector(err.moy2), col="darkorange", pch = 16, type = "b", main = "MSEP of the model", ylab = "MSEP", xlab = "Number of variables selected in X (keepX)")
  }else{
    plot(0, main = "MSEP of the model", xlim = c(xmin,xmax), ylim = c(ymin,ymax), ylab = "Number of variables selected in Y (keepY)", xlab = "Number of variables selected in X (keepX)")
    text(gl(p,q), rep(1:q,p), labels = as.vector(round.err), col = (err.moy2==min(err.moy2))+1)
    abline(v = xmin:xmax, h = ymin:ymax, lty = 3, col = "yellow")
  }
  
  return(setNames(list(err.moy,h.best,err.moy2,keepX.best,keepY.best),c("MSEP.h","h.best","MSEP.q.p","keepX.best","keepY.best")))
  
  
}
```

The function named `perf.sPLS` will allow to assess the performance and to choose the best parameters. This function outputs the best tuning parameters but also two MSEP plots : the one shows the MSEP according to the number of components and the other shows the MSEP according to the number of selected variables in $X$ and $Y$. Particularly, in multivariate sPLS ($q>1$), the "plot" is a table of MSEP values.

### First model MSEP


```{r}
perf.res1 <- tuning.sPLS.XY(model1)
h.best <- perf.res1$h.best
keepX.best <- perf.res1$keepX.best
keepY.best <- perf.res1$keepY.best
```

The `perf.sPLS` gives us a optimal components number equal to $H =$ `r h.best`, therefore we suggest to select `r h.best` components in our first model. It indicates us also to select `r unique(keepX.best)` variables for each component.

### Second model MSEP

```{r}
perf.res2 <- tuning.sPLS.XY(model2)
h.best <- perf.res2$h.best
keepX.best <- perf.res2$keepX.best
keepY.best <- perf.res2$keepY.best
```

The `perf.sPLS` gives us a optimal components number equal to $H =$ `r h.best`, therefore we suggest to select `r h.best` components in our first model. It indicates us also to select `r unique(keepX.best)` variables in $X$ and `r unique(keepY.best)` variables in $Y$.
