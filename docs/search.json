[{"path":"/articles/PLS_performance.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"PLS performance","text":"page presents application PLS performance assessment. PLS method quite particular method : several predictions according components number selected model. goal almost choose best number component PLS regression order compute best possible predictions. , use three datasets: one dataset one response variable YY. dataset four response variables Y=(Y1,Y2,Y3,Y4)Y = (Y1,Y2,Y3,Y4). last dataset contains real data NIR spectra. access predefined functions sgPLSdevelop package manipulate datasets, run lines : two first datasets, population set n=40n = 40 default, close actual conditions. Let’s also notice , average, response YY linear combination predictors XX. Indeed, function includes matrix product Y=XB+EY = XB + E BB weight matrix EE matrix gaussian noise. linearity condition important order good performance model, PLS method using linearity combinaison. Now, ’s time train PLS model dataset built imported.","code":"library(sgPLSdevelop) library(pls)  data1 <- data.create(p = 10, list = TRUE) data2 <- data.create(p = 10, q = 4, list = TRUE) data(yarn) data3 <- yarn ## [1] \"First dataset dimensions : 40 x 11\" ## [1] \"Second dataset dimensions : 40 x 14\" ## [1] \"Yarn dataset dimensions : 28 x 3\" ncomp.max <- 8  # First model X <- data1$X Y <- data1$Y model1 <- PLS(X,Y,mode = \"regression\", ncomp = ncomp.max)  # Second model X <- data2$X Y <- data2$Y model2 <- PLS(X,Y,mode = \"regression\", ncomp = ncomp.max)  # Third model X <- data3$NIR Y <- data3$density model3 <- PLS(X,Y,mode = \"regression\", ncomp = ncomp.max)"},{"path":"/articles/PLS_performance.html","id":"pls-performance-assessment-using-q2","dir":"Articles","previous_headings":"","what":"PLS performance assessment using Q2","title":"PLS performance","text":"Q² indicator ? Q2Q^2 assessment indicator PLS models; new component hh, new matrix Y(h)Y^{(h)} obtained deflation compared corresponding prediction matrix Ŷ(h)\\hat{Y}^{(h)}. Q2 therefore takes comparison account. Q2Q^2 value close 11 indicates good performance. compute figure, must compute two indicators : RSSRSS PRESSPRESS. RSSh=∑=1n∑j=1q(Y(h)−Ŷ)2=∑=1n∑j=1q(Yi,j(h+1))2RSS_h = \\sum_{=1}^{n} \\sum_{j=1}^{q} (Y^{(h)} - \\hat{Y})^2 =  \\sum_{=1}^{n} \\sum_{j=1}^{q} (Y^{(h+1)}_{,j})^2 PRESSh=∑∈testn∑j=1q(Yi,j(h)−Yi,ĵ)2=∑∈testn∑j=1q(Yi,j(h+1))2PRESS_h = \\sum_{\\test}^{n} \\sum_{j=1}^{q} (Y_{,j}^{(h)} - \\hat{Y_{,j}})^2 =  \\sum_{\\test}^{n} \\sum_{j=1}^{q} (Y^{(h+1)}_{,j})^2 , Q2Q^2 defined formula : Qh2=1−PRESShRSSh−1Q^2_h = 1 - \\frac{PRESS_h}{RSS_{h-1}} use Q² ? compare value criterion certain limit ll ; limit conventionally equal 1−0.952=0.09751-0.95^2 = 0.0975. long inequality Qh2≥lQ^2_h \\geq l, keep following iteration ; therefore stop Qh2<lQ^2_h < l. Using Q² R Q2Q^2 function, available named q2.PLS(), takes three parameters : model (“pls” class object) mode : must choose “regression” “canonical” number maximal components","code":""},{"path":"/articles/PLS_performance.html","id":"first-model-q2","dir":"Articles","previous_headings":"PLS performance assessment using Q2","what":"First model Q2","title":"PLS performance","text":"q2.pls function gives us optimal components number select equal H=H = 2.","code":"par(mfrow = c(1,2)) q2.res1 <- q2.PLS(model1) h.best <- q2.res1$h.best q2.PLS(model1, ncomp.max = h.best+1) ## $suggestion ## [1] \"best number of components : H = 2\" ##  ## $h.best ## [1] 2 ##  ## $q2 ## [1]  0.7920627  0.3108891 -1.8832754 ##  ## $PRESS ## [1] 8.109555 3.115062 2.267844 ##  ## $RSS ## [1] 4.5204066 0.7865514 0.1783655 ##  ## $PRESSj ##            Y1 ## [1,] 8.109555 ## [2,] 3.115062 ## [3,] 2.267844 ##  ## $RSSj ##             Y1 ## [1,] 4.5204066 ## [2,] 0.7865514 ## [3,] 0.1783655"},{"path":"/articles/PLS_performance.html","id":"second-model-q2","dir":"Articles","previous_headings":"PLS performance assessment using Q2","what":"Second model Q2","title":"PLS performance","text":"q2.pls function gives us optimal components number select equal H=H = 1.","code":"par(mfrow = c(1,2)) q2.res2 <- q2.PLS(model2) h.best <- q2.res2$h.best q2.PLS(model2, ncomp.max = h.best+1) ## $suggestion ## [1] \"best number of components : H = 1\" ##  ## $h.best ## [1] 1 ##  ## $q2 ## [1]  0.350568774 -0.002592037 ##  ## $PRESS ## [1] 101.31127  88.23925 ##  ## $RSS ## [1] 88.01112 56.01199 ##  ## $PRESSj ##            Y1       Y2       Y3       Y4 ## [1,] 21.86576 38.32656 12.25569 28.86326 ## [2,] 20.60824 44.18225 10.95299 12.49577 ##  ## $RSSj ##            Y1       Y2       Y3        Y4 ## [1,] 19.52016 32.81233 9.757528 25.921106 ## [2,] 11.02321 32.05002 8.321697  4.617062"},{"path":"/articles/PLS_performance.html","id":"third-model-q2","dir":"Articles","previous_headings":"PLS performance assessment using Q2","what":"Third model Q2","title":"PLS performance","text":"q2.pls function gives us optimal components number select equal H=H = 2.","code":"par(mfrow = c(1,2)) q2.res3 <- q2.PLS(model3) h.best <- q2.res3$h.best q2.PLS(model3, ncomp.max = h.best+1) ## $suggestion ## [1] \"best number of components : H = 2\" ##  ## $h.best ## [1] 2 ##  ## $q2 ## [1]  0.8954745  0.5508883 -3.7792077 ##  ## $PRESS ## [1] 2.8221888 0.9738703 0.8721606 ##  ## $RSS ## [1] 2.16843657 0.18249063 0.09026228 ##  ## $PRESSj ##             Y1 ## [1,] 2.8221888 ## [2,] 0.9738703 ## [3,] 0.8721606 ##  ## $RSSj ##              Y1 ## [1,] 2.16843657 ## [2,] 0.18249063 ## [3,] 0.09026228"},{"path":"/articles/PLS_performance.html","id":"pls-performance-assessment-using-msep","dir":"Articles","previous_headings":"","what":"PLS performance assessment using MSEP","title":"PLS performance","text":"way assess model performance consists using MSEPMSEP criterion. MSEPMSEP computed follow : MSEP=1nq∑=1n∑j=1q(Yi,j−Ŷ,j)2MSEP = \\frac{1}{nq} \\sum_{=1}^{n} \\sum_{j=1}^{q} (Y_{,j} - \\hat{Y}_{,j})^2","code":""},{"path":"/articles/PLS_performance.html","id":"first-model-msep","dir":"Articles","previous_headings":"PLS performance assessment using MSEP","what":"First model MSEP","title":"PLS performance","text":"perf.PLS gives us optimal components number equal H=H = 7, therefore suggest select 7 components first model.","code":"perf.res1 <- perf.PLS(model1) h.best <- perf.res1$h.best msep.best <- perf.res1$MSEP[h.best]"},{"path":"/articles/PLS_performance.html","id":"second-model-msep","dir":"Articles","previous_headings":"PLS performance assessment using MSEP","what":"Second model MSEP","title":"PLS performance","text":"perf.PLS gives us optimal components number equal H=H = 8, therefore suggest select 8 components second model.","code":"perf.res2 <- perf.PLS(model2) h.best <- perf.res2$h.best msep.best <- perf.res2$MSEP[h.best]"},{"path":"/articles/PLS_performance.html","id":"third-model-msep","dir":"Articles","previous_headings":"PLS performance assessment using MSEP","what":"Third model MSEP","title":"PLS performance","text":"perf.PLS gives us optimal components number equal H=H = 8, therefore suggest select 8 components third model.","code":"perf.res3 <- perf.PLS(model3) h.best <- perf.res3$h.best msep.best <- perf.res3$MSEP[h.best]"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Benoit Liquet. Maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Liquet B, de Micheaux PL, Broc C (2023). sgPLS: Sparse Group Partial Least Square Methods. doi:10.32614/CRAN.package.sgPLS, R package version 1.8, https://CRAN.R-project.org/package=sgPLS.","code":"@Manual{,   title = {sgPLS: Sparse Group Partial Least Square Methods},   author = {Benoit Liquet and Pierre Lafaye {de Micheaux} and Camilo Broc},   year = {2023},   note = {R package version 1.8},   url = {https://CRAN.R-project.org/package=sgPLS},   doi = {10.32614/CRAN.package.sgPLS}, }"},{"path":"/reference/data.create.html","id":null,"dir":"Reference","previous_headings":"","what":"Dataset simulation — data.create","title":"Dataset simulation — data.create","text":"functions allow generate dataset linear dependance \\(Y\\) \\(X\\). data.create used quantitative response data.cl.create used qualitative response.","code":""},{"path":"/reference/data.create.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset simulation — data.create","text":"","code":"data.create(n = 40, p = 10, q = 1, list = FALSE) data.cl.create(n = 40, p = 10, classes = 2, list = FALSE)"},{"path":"/reference/data.create.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dataset simulation — data.create","text":"n number dataset rows p number \\(X\\) variables q number \\(Y\\) variables data.create function classes number classes generate data.cl.create function list default, return dataframe. argument list set TRUE, list including dataframe returned.","code":""},{"path":"/reference/data.create.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Dataset simulation — data.create","text":"default, population set \\(n=40\\) close actual conditions. case, \\(p<n\\). data.create function, \\(Y\\) linear combinaison gaussian variable \\(X_j\\) \\(X\\). Indeed, function includes matrix product compute response : \\(Y = XB+E\\) \\(B\\) weight (coefficients) matrix \\(E\\) matrix gaussian noise. \\(B\\) matrix can found list returned function (list = TRUE). data.cl.create function, link \\(X\\) classes \\(Y\\). list returns also Y.f version Y factor class.","code":""},{"path":"/reference/data.create.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Dataset simulation — data.create","text":"","code":"library(sgPLS) library(sgPLSdevelop) #> Registered S3 methods overwritten by 'sgPLSdevelop': #>   method          from  #>   predict.sPLS    sgPLS #>   predict.gPLS    sgPLS #>   predict.sgPLS   sgPLS #>   predict.sPLSda  sgPLS #>   predict.gPLSda  sgPLS #>   predict.sgPLSda sgPLS #>   perf.sPLS       sgPLS #>   perf.gPLS       sgPLS #>   perf.sgPLS      sgPLS #>   perf.sPLSda     sgPLS #>   perf.gPLSda     sgPLS #>   perf.sgPLSda    sgPLS #>  #> Attaching package: 'sgPLSdevelop' #> The following objects are masked from 'package:sgPLS': #>  #>     gPLS, gPLSda, lambda.quadra, normv, per.variance, perf.gPLS, #>     perf.sPLS, perf.sgPLS, plotcim, predict.gPLS, predict.sPLS, #>     predict.sgPLS, sPLS, sPLSda, select.sgpls, select.spls, sgPLS, #>     sgPLSda, soft.thresholding, soft.thresholding.group, #>     soft.thresholding.sparse.group, step1.group.spls.sparsity, #>     step1.sparse.group.spls.sparsity, step1.spls.sparsity, step2.spls, #>     tuning.gPLS.X, tuning.sPLS.X, tuning.sgPLS.X  # data.create data <- data.create(n = 20, p = 5, q = 2, list = TRUE) X <- data$X Y <- data$Y  # data.cl.create data.cl <- data.cl.create(n = 20, p = 5, classes = 3, list = TRUE) X <- data.cl$X Y <- data.cl$Y"},{"path":"/reference/gPLS.html","id":null,"dir":"Reference","previous_headings":"","what":"Group Partial Least Squares (gPLS) — gPLS","title":"Group Partial Least Squares (gPLS) — gPLS","text":"Function perform group Partial Least Squares (gPLS)   context two datasets divided groups   variables. gPLS approach aims select groups   variables one dataset linearly related groups variables second dataset.","code":""},{"path":"/reference/gPLS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group Partial Least Squares (gPLS) — gPLS","text":"","code":"gPLS(X, Y, ncomp, mode = \"regression\",      max.iter = 500, tol = 1e-06, keepX,       keepY = NULL, ind.block.x, ind.block.y = NULL,scale=TRUE)"},{"path":"/reference/gPLS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group Partial Least Squares (gPLS) — gPLS","text":"X numeric matrix predictors. Y numeric vector matrix responses (multi-response models). ncomp number components include model (see Details). mode character string. type algorithm use, (partially) matching     one \"regression\" \"canonical\". See Details. max.iter integer, maximum number iterations. tol positive real, tolerance used iterative algorithm. keepX numeric vector length ncomp, number variables     keep \\(X\\)-loadings. default variables kept model. keepY numeric vector length ncomp, number variables     keep \\(Y\\)-loadings. default variables kept model. ind.block.x vector integers describing grouping \\(X\\)-variables. (see example Details section) ind.block.y vector consecutive integers describing grouping \\(Y\\)-variables (see example Details section) scale logical indicating orignal data set need scaled. default scale=TRUE","code":""},{"path":"/reference/gPLS.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Group Partial Least Squares (gPLS) — gPLS","text":"gPLS function fits gPLS models \\(1, \\ldots ,\\)ncomp components. Multi-response models fully supported. type algorithm use specified mode argument. Two gPLS algorithms available: gPLS regression (\"regression\") gPLS canonical analysis (\"canonical\") (see References). ind.block.x <- c(3,10,15) means \\(X\\) structured 4 groups: X1 X3; X4 X10, X11 X15 X16 X\\(p\\) \\(p\\) number variables \\(X\\) matrix.","code":""},{"path":"/reference/gPLS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Group Partial Least Squares (gPLS) — gPLS","text":"gPLS returns object class \"gPLS\", list contains following components:  X centered standardized original predictor matrix. Y centered standardized original response vector matrix. ncomp number components included model. mode algorithm used fit model. keepX number \\(X\\) variables kept model component. keepY number \\(Y\\) variables kept model component. mat.c matrix coefficients used internally predict. variates list containing variates. loadings list containing estimated loadings \\(X\\) \t\\(Y\\) variates. names list containing names used individuals variables. tol tolerance used iterative algorithm, used subsequent S3 methods. max.iter maximum number iterations, used subsequent S3 methods. iter vector containing number iterations convergence component. ind.block.x vector integers describing grouping X variables. ind.block.y vector consecutive integers describing grouping Y variables.","code":""},{"path":"/reference/gPLS.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Group Partial Least Squares (gPLS) — gPLS","text":"Liquet Benoit, Lafaye de Micheaux Pierre , Hejblum Boris, Thiebaut Rodolphe. group Sparse Group Partial Least Square approach applied Genomics context. Submitted. Le Cao, K.-., Martin, P.G.P., Robert-Grani\\'e, C. Besse, P. (2009). Sparse canonical methods biological data integration: application cross-platform study. BMC Bioinformatics 10:34. Le Cao, K.-., Rossouw, D., Robert-Grani\\'e, C. Besse, P. (2008). sparse PLS variable selection integrating Omics data. Statistical Applications Genetics Molecular Biology 7, article 35. Shen, H. Huang, J. Z. (2008). Sparse principal component analysis via regularized low rank matrix approximation. Journal Multivariate Analysis 99, 1015-1034. Tenenhaus, M. (1998). La r\\'egression PLS: th\\'eorie et pratique. Paris: Editions Technic. Wold H. (1966). Estimation principal components related models iterative least squares. : Krishnaiah, P. R. (editors), Multivariate Analysis. Academic Press, N.Y., 391-420.","code":""},{"path":"/reference/gPLS.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Group Partial Least Squares (gPLS) — gPLS","text":"Benoit Liquet Pierre Lafaye de Micheaux.","code":""},{"path":[]},{"path":"/reference/gPLS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group Partial Least Squares (gPLS) — gPLS","text":"","code":"## Simulation of datasets X and Y with group variables n <- 100 sigma.gamma <- 1 sigma.e <- 1.5 p <- 400 q <- 500 theta.x1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5), rep(1.5,15),                rep(0, 5), rep(-1.5, 15), rep(0, 325)) theta.x2 <- c(rep(0, 320), rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),               rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 5))  theta.y1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5), rep(1.5, 15),               rep(0, 5), rep(-1.5, 15), rep(0, 425)) theta.y2 <- c(rep(0, 420), rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),               rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 5))                               Sigmax <- matrix(0, nrow = p, ncol = p) diag(Sigmax) <- sigma.e ^ 2 Sigmay <- matrix(0,nrow = q, ncol = q) diag(Sigmay) <- sigma.e ^ 2  set.seed(125)  gam1 <- rnorm(n) gam2 <- rnorm(n)  X <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.x1, theta.x2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, p), sigma =      Sigmax, method = \"svd\") Y <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.y1, theta.y2),       nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, q), sigma =      Sigmay, method = \"svd\")   ind.block.x <- seq(20, 380, 20) ind.block.y <- seq(20, 480, 20) ##   #### gPLS model model.gPLS <- gPLS(X, Y, ncomp = 2, mode = \"regression\", keepX = c(4, 4),       keepY = c(4, 4), ind.block.x = ind.block.x , ind.block.y = ind.block.y)  result.gPLS <- select.sgpls(model.gPLS) result.gPLS$group.size.X #>    size comp1 comp2 #> 1    20    20     0 #> 2    20    20     0 #> 3    20    20     0 #> 4    20    20     0 #> 5    20     0     0 #> 6    20     0     0 #> 7    20     0     0 #> 8    20     0     0 #> 9    20     0     0 #> 10   20     0     0 #> 11   20     0     0 #> 12   20     0     0 #> 13   20     0     0 #> 14   20     0     0 #> 15   20     0     0 #> 16   20     0     0 #> 17   20     0    20 #> 18   20     0    20 #> 19   20     0    20 #> 20   20     0    20 result.gPLS$group.size.Y #>    size comp1 comp2 #> 1    20    20     0 #> 2    20    20     0 #> 3    20    20     0 #> 4    20    20     0 #> 5    20     0     0 #> 6    20     0     0 #> 7    20     0     0 #> 8    20     0     0 #> 9    20     0     0 #> 10   20     0     0 #> 11   20     0     0 #> 12   20     0     0 #> 13   20     0     0 #> 14   20     0     0 #> 15   20     0     0 #> 16   20     0     0 #> 17   20     0     0 #> 18   20     0     0 #> 19   20     0     0 #> 20   20     0     0 #> 21   20     0     0 #> 22   20     0    20 #> 23   20     0    20 #> 24   20     0    20 #> 25   20     0    20"},{"path":"/reference/gPLSda.html","id":null,"dir":"Reference","previous_headings":"","what":"Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — gPLSda","title":"Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — gPLSda","text":"Function perform group Partial Least Squares classify samples (supervised analysis) select variables.","code":""},{"path":"/reference/gPLSda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — gPLSda","text":"","code":"gPLSda(X, Y, ncomp = 2, keepX = rep(ncol(X), ncomp),        max.iter = 500, tol = 1e-06, ind.block.x)"},{"path":"/reference/gPLSda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — gPLSda","text":"X numeric matrix predictors. NAs allowed. Y factor class vector discrete outcome. ncomp number components include model (see Details). keepX numeric vector length ncomp, number variables     keep \\(X\\)-loadings. default variables kept model. max.iter integer, maximum number iterations. tol positive real, tolerance used iterative algorithm. ind.block.x vector integers describing grouping \\(X\\)-variables. (see example Details section)","code":""},{"path":"/reference/gPLSda.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — gPLSda","text":"gPLSda function fit gPLS models \\(1, \\ldots ,\\)ncomp components factor class vector Y. appropriate indicator (dummy) matrix created. ind.block.x <- c(3,10,15) means \\(X\\) structured 4 groups: X1 X3; X4 X10, X11 X15 X16 X\\(p\\) \\(p\\) number variables \\(X\\) matrix.","code":""},{"path":"/reference/gPLSda.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — gPLSda","text":"sPLSda returns object class \"sPLSda\", list contains following components:  X centered standardized original predictor matrix. Y centered standardized indicator response vector matrix. ind.mat indicator matrix. ncomp number components included model. keepX number \\(X\\) variables kept model component. mat.c matrix coefficients used internally predict. variates list containing variates. loadings list containing estimated loadings X \tY variates. names list containing names used individuals variables. tol tolerance used iterative algorithm, used subsequent S3 methods max.iter maximum number iterations, used subsequent S3 methods iter Number iterations algorthm component ind.block.x vector integers describing grouping X variables.","code":""},{"path":"/reference/gPLSda.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — gPLSda","text":"Liquet Benoit, Lafaye de Micheaux Pierre , Hejblum Boris, Thiebaut Rodolphe (2016). group Sparse Group Partial Least Square approach applied Genomics context. Bioinformatics. sPLS-DA: Le Cao, K.-., Boitard, S. Besse, P. (2011). Sparse PLS Discriminant Analysis: biologically relevant feature selection graphical displays multiclass problems. BMC Bioinformatics 12:253.","code":""},{"path":"/reference/gPLSda.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — gPLSda","text":"Benoit Liquet  Pierre Lafaye de Micheaux.","code":""},{"path":[]},{"path":"/reference/gPLSda.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — gPLSda","text":"","code":"data(simuData) X <- simuData$X Y <- simuData$Y ind.block.x <- seq(100, 900, 100) model <- gPLSda(X, Y, ncomp = 3,ind.block.x=ind.block.x, keepX = c(2, 2, 2)) result.gPLSda <- select.sgpls(model) result.gPLSda$group.size.X #>    size comp1 comp2 comp3 #> 1   100     0   100     0 #> 2   100     0     0   100 #> 3   100     0     0     0 #> 4   100   100     0     0 #> 5   100     0     0     0 #> 6   100     0   100     0 #> 7   100     0     0   100 #> 8   100     0     0     0 #> 9   100   100     0     0 #> 10  100     0     0     0  # perf(model,criterion=\"all\",validation=\"loo\") -> res # res$error.rate"},{"path":"/reference/per.variance.html","id":null,"dir":"Reference","previous_headings":"","what":"Percentage of variance of the \\(Y\\) matrix explained by the score-vectors obtained by PLS approaches — per.variance","title":"Percentage of variance of the \\(Y\\) matrix explained by the score-vectors obtained by PLS approaches — per.variance","text":"per.variance function computes percentage variance \\(Y\\) matrix explained score-vectors obtained PLS approaches (sPLS, gPLS sgPLS) regression mode.","code":""},{"path":"/reference/per.variance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Percentage of variance of the \\(Y\\) matrix explained by the score-vectors obtained by PLS approaches — per.variance","text":"","code":"per.variance(object)"},{"path":"/reference/per.variance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Percentage of variance of the \\(Y\\) matrix explained by the score-vectors obtained by PLS approaches — per.variance","text":"object object class inheriting \"sPLS\", \"gPLS\",    \"sgPLS\". function retrieve key parameters stored object.","code":""},{"path":"/reference/per.variance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Percentage of variance of the \\(Y\\) matrix explained by the score-vectors obtained by PLS approaches — per.variance","text":"per.variance produces list following components: perX Percentage variance \\(Y\\) matrix explained score-vectors. cum.perX cumulative percentage variance \\(Y\\) matrix explained score-vectors.","code":""},{"path":"/reference/per.variance.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Percentage of variance of the \\(Y\\) matrix explained by the score-vectors obtained by PLS approaches — per.variance","text":"Benoit Liquet, b.liquet@uq.edu.au,  Pierre Lafaye de Micheaux lafaye@dms.umontreal.ca","code":""},{"path":"/reference/per.variance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Percentage of variance of the \\(Y\\) matrix explained by the score-vectors obtained by PLS approaches — per.variance","text":"","code":"if (FALSE) { # \\dontrun{   ## Simulation of datasets X and Y with group variables n <- 100 sigma.gamma <- 1 sigma.e <- 1.5 p <- 400 q <- 500 theta.x1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5), rep(1.5, 15),               rep(0, 5), rep(-1.5, 15), rep(0, 325)) theta.x2 <- c(rep(0, 320), rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),               rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 5))  theta.y1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5), rep(1.5, 15),               rep(0, 5), rep(-1.5, 15), rep(0, 425)) theta.y2 <- c(rep(0, 420), rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),               rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 5))  Sigmax <- matrix(0, nrow = p, ncol = p) diag(Sigmax) <- sigma.e ^ 2 Sigmay <- matrix(0, nrow = q, ncol = q) diag(Sigmay) <- sigma.e ^ 2  set.seed(125)  gam1 <- rnorm(n) gam2 <- rnorm(n)  X <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.x1, theta.x2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, p), sigma =      Sigmax, method = \"svd\") Y <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.y1, theta.y2),       nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, q), sigma =      Sigmay, method = \"svd\")  ind.block.x <- seq(20, 380, 20) ind.block.y <- seq(20, 480, 20)  #### gPLS model model.sgPLS <- sgPLS(X, Y, ncomp = 2, mode = \"regression\", keepX = c(4, 4),                     keepY = c(4, 4), ind.block.x = ind.block.x,                    ind.block.y = ind.block.y,                    alpha.x = c(0.5, 0.5), alpha.y = c(0.5, 0.5))  result.sgPLS <- select.sgpls(model.sgPLS) result.sgPLS$group.size.X result.sgPLS$group.size.Y  #### gPLS model model.gPLS <- gPLS(X, Y, ncomp = 2, mode = \"regression\", keepX = c(4, 4),       keepY = c(4, 4), ind.block.x = ind.block.x ,ind.block.y = ind.block.y)  result.gPLS <- select.sgpls(model.gPLS) result.gPLS$group.size.X result.gPLS$group.size.Y  per.variance(model.gPLS) per.variance(model.sgPLS)  } # }"},{"path":"/reference/perf.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute evaluation criteria for PLS, sPLS, PLS-DA and sPLS-DA — perf","title":"Compute evaluation criteria for PLS, sPLS, PLS-DA and sPLS-DA — perf","text":"Function evaluate performance fitted sparse PLS, group PLS, sparse group PLS, sparse PLS-DA, group PLS-DA sparse group PLS-DA models using various criteria.","code":""},{"path":"/reference/perf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute evaluation criteria for PLS, sPLS, PLS-DA and sPLS-DA — perf","text":"","code":"# S3 method for class 'sPLS' perf(object,            criterion = c(\"all\", \"MSEP\", \"R2\", \"Q2\"),            validation = c(\"Mfold\", \"loo\"),            folds = 10, progressBar = TRUE, setseed = 1,...)            # S3 method for class 'gPLS' perf(object,            criterion = c(\"all\", \"MSEP\", \"R2\", \"Q2\"),            validation = c(\"Mfold\", \"loo\"),            folds = 10, progressBar = TRUE, setseed = 1, ...)            # S3 method for class 'sgPLS' perf(object,            criterion = c(\"all\", \"MSEP\", \"R2\", \"Q2\"),            validation = c(\"Mfold\", \"loo\"),            folds = 10, progressBar = TRUE,setseed = 1, ...)                       # S3 method for class 'sPLSda' perf(object,           method.predict = c(\"all\", \"max.dist\", \"centroids.dist\", \"mahalanobis.dist\"),           validation = c(\"Mfold\", \"loo\"),            folds = 10, progressBar = TRUE, ...)    # S3 method for class 'gPLSda' perf(object,           method.predict = c(\"all\", \"max.dist\", \"centroids.dist\", \"mahalanobis.dist\"),           validation = c(\"Mfold\", \"loo\"),            folds = 10, progressBar = TRUE, ...)            # S3 method for class 'sgPLSda' perf(object,           method.predict = c(\"all\", \"max.dist\", \"centroids.dist\", \"mahalanobis.dist\"),           validation = c(\"Mfold\", \"loo\"),            folds = 10, progressBar = TRUE, ...)"},{"path":"/reference/perf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute evaluation criteria for PLS, sPLS, PLS-DA and sPLS-DA — perf","text":"object Object class inheriting \"sPLS\", \"gPLS\", \"sgPLS\", \"sPLSda\", \"gPLSda\" \"sgPLSda\". function retrieve key parameters stored object. criterion criteria measures calculated (see Details). Can set either \"\", \"MSEP\", \"R2\", \"Q2\". default set \"\". applies object inheriting \"sPLS\", \"gPLS\" \"sgPLS\" method.predict applies object inheriting \"PLSda\", \"gPLSda\" \"sgPLSda\" evaluate classification performance model. subset \"max.dist\", \"centroids.dist\", \"mahalanobis.dist\". Default \"\". See predict. validation Character.  kind (internal) validation use, matching one \"Mfold\"     \"loo\" (see ). Default \"Mfold\". folds folds Mfold cross-validation. See Details. progressBar default set TRUE output progress bar computation. setseed Integer value specify random generator state. ... used moment.","code":""},{"path":"/reference/perf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute evaluation criteria for PLS, sPLS, PLS-DA and sPLS-DA — perf","text":"method perf created Sebastien Dejean, Ignacio Gonzalez, Amrit Singh Kim-Anh Le Cao pls spls models performed mixOmics package. Similar code adapted sPLS, gPLS sgPLS package sgPLS. perf estimates mean squared error prediction (MSEP), \\(R^2\\), \\(Q^2\\) assess predictive performance model using M-fold leave-one-cross-validation. Note classic, regression  invariant modes can applied. validation = \"Mfold\", M-fold cross-validation performed. many folds generate selected specifying number folds folds. folds also can supplied list vectors containing indexes defining fold produced split. validation = \"loo\", leave-one-cross-validation performed. fitted sPLS-DA, gPLS-DA sgPLS-DA models, perf estimates classification error rate using cross-validation. Note perf function retrieve keepX keepY inputs previously run object. sPLS, gPLS, sgPLS, sPLSda, gPLSda sgPLSda functions run several different subsets data (cross-folds) certainly different subset selected features. sPLS, MSEP, \\(R^2\\), \\(Q^2\\) criteria averaged across folds. feature stability measure output user assess often variables selected across folds. sPLS-DA, classification erro rate averaged across folds.","code":""},{"path":"/reference/perf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute evaluation criteria for PLS, sPLS, PLS-DA and sPLS-DA — perf","text":"perf produces list following components: MSEP Mean Square Error Prediction \\(Y\\) variable, applies object inherited \"sPLS\", \"gPLS\" \"sgPLS\". R2 matrix \\(R^2\\) values \\(Y\\)-variables models     \\(1, \\ldots ,\\)ncomp components, applies object inherited \"sPLS\", \"gPLS\" \"sgPLS\". Q2 \\(Y\\) contains one variable, vector \\(Q^2\\) values else list     matrix \\(Q^2\\) values \\(Y\\)-variable. Note specific case sPLS model, better look Q2.total criterion, applies object inherited \"sPLS\", \"gPLS\" \"sgPLS\". Q2.total vector \\(Q^2\\)-total values models \\(1, \\ldots ,\\)ncomp components, applies object inherited \"sPLS\", \"gPLS\" \"sgPLS\". features list features selected across folds ($stable.X $stable.Y) whole data set ($final) keepX keepY parameters input object. error.rate sPLS-DA, gPLS-DA sgPLS-DA models, perf produces matrix classification error rate estimation. dimensions correspond components model prediction method used, respectively. Note error rates reported component include performance model earlier components specified keepX parameters (e.g. error rate reported component 3 keepX = 20 already includes fitted model components 1 2  keepX = 20). advanced usage perf function, see mixOmics package consider using predict function.","code":""},{"path":"/reference/perf.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute evaluation criteria for PLS, sPLS, PLS-DA and sPLS-DA — perf","text":"Tenenhaus, M. (1998). La r\\'egression PLS: th\\'eorie et pratique. Paris: Editions Technic. Le Cao, K.-., Rossouw, D., Robert-Grani\\'e, C. Besse, P. (2008). sparse PLS variable selection integrating Omics data. Statistical Applications Genetics Molecular Biology 7, article 35. Mevik, B.-H., Cederkvist, H. R. (2004). Mean Squared Error Prediction (MSEP) Estimates Principal Component Regression (PCR) Partial Least Squares Regression (PLSR). Journal Chemometrics 18(9), 422-429.","code":""},{"path":"/reference/perf.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Compute evaluation criteria for PLS, sPLS, PLS-DA and sPLS-DA — perf","text":"Benoit Liquet Pierre Lafaye de Micheaux","code":""},{"path":[]},{"path":"/reference/perf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute evaluation criteria for PLS, sPLS, PLS-DA and sPLS-DA — perf","text":"","code":"## validation for objects of class 'sPLS' (regression) ## Example from mixOmics package  # ---------------------------------------- if (FALSE) { # \\dontrun{ data(liver.toxicity) X <- liver.toxicity$gene Y <- liver.toxicity$clinic   ## validation for objects of class 'spls' (regression) # ---------------------------------------- ncomp <- 7 # first, learn the model on the whole data set model.spls <- sPLS(X, Y, ncomp = ncomp, mode = 'regression',    keepX = c(rep(5, ncomp)), keepY = c(rep(2, ncomp)))   # with leave-one-out cross validation set.seed(45) model.spls.loo.val <- perf(model.spls, validation = \"loo\")  #Q2 total model.spls.loo.val$Q2.total  # R2:we can see how the performance degrades when ncomp increases # results are similar to 5-fold model.spls.loo.val$R2  } # }"},{"path":"/reference/perf.PLS.html","id":null,"dir":"Reference","previous_headings":"","what":"PLS function performance assessment using \\(MSEP\\) indicator — perf.PLS","title":"PLS function performance assessment using \\(MSEP\\) indicator — perf.PLS","text":"perf.PLS function allows assess PLS models using\\(MSEP\\) criterion.   function returns list including \\(MSEP\\) values number components.   perf.PLS gives also suggestion number components selection.   plot allows visualize model performance according number components.","code":""},{"path":"/reference/perf.PLS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"PLS function performance assessment using \\(MSEP\\) indicator — perf.PLS","text":"","code":"perf.PLS(object, ncomp = object$ncomp, K=nrow(object$X))"},{"path":"/reference/perf.PLS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"PLS function performance assessment using \\(MSEP\\) indicator — perf.PLS","text":"object Object class inheriting \"pls\". ncomp number components desired MSEP computing (number components computed model). K number blocks cross-validation (leave-one-default).","code":""},{"path":"/reference/perf.PLS.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"PLS function performance assessment using \\(MSEP\\) indicator — perf.PLS","text":"\\(K\\) must value 2 number rows dataset used training model. MSEP defined mean squared error true \\(Y\\) values associated predictions.","code":""},{"path":[]},{"path":"/reference/perf.PLS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"PLS function performance assessment using \\(MSEP\\) indicator — perf.PLS","text":"","code":"library(sgPLSdevelop)  ## data and model creation  d <- data.create(p = 10, list = TRUE) n <- nrow(d$X) ncomp.max <- 10 X <- d$X Y <- d$Y modele <- PLS(X,Y,ncomp = ncomp.max, mode = \"regression\")  ## using perf.PLS function perf.res <- perf.PLS(modele) #> Error in perf.PLS(modele): object 'ncomp.max' not found perf.res$MSEP  #> Error: object 'perf.res' not found perf.res$h.best #number of components suggestion  #> Error: object 'perf.res' not found"},{"path":"/reference/plotcim.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots a cluster image mapping of correlations between outcomes and all predictors — plotcim","title":"Plots a cluster image mapping of correlations between outcomes and all predictors — plotcim","text":"plotcim function plots cluster image   mapping correlations outcomes predictors.","code":""},{"path":"/reference/plotcim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots a cluster image mapping of correlations between outcomes and all predictors — plotcim","text":"","code":"plotcim(matX, matY, cexCol = 0.5, cexRow = 1)"},{"path":"/reference/plotcim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots a cluster image mapping of correlations between outcomes and all predictors — plotcim","text":"matX data frame corresponding predictors. matY data frame corresponding outcomes. cexRow, cexCol positive numbers, used cex.axis row column \taxis labeling. defaults currently use number rows columns, respectively.","code":""},{"path":"/reference/plotcim.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plots a cluster image mapping of correlations between outcomes and all predictors — plotcim","text":"used small number predictors (<1,000).","code":""},{"path":"/reference/plotcim.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plots a cluster image mapping of correlations between outcomes and all predictors — plotcim","text":"Benoit Liquet, b.liquet@uq.edu.au,  Pierre Lafaye de Micheaux lafaye@dms.umontreal.ca","code":""},{"path":"/reference/PLS.html","id":null,"dir":"Reference","previous_headings":"","what":"Partial Least Squares (PLS) — PLS","title":"Partial Least Squares (PLS) — PLS","text":"function performing partial least squares (OLS) method two data sets columns rows. PLS approach therefore useful data sets OLS regression possible.","code":""},{"path":"/reference/PLS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Partial Least Squares (PLS) — PLS","text":"","code":"PLS(X, Y, ncomp, mode = \"regression\",scale=TRUE)"},{"path":"/reference/PLS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Partial Least Squares (PLS) — PLS","text":"X numeric matrix predictors. Y numeric vector matrix responses (multi-response models). ncomp number components include model (see Details). mode character string. type algorithm use, (partially) matching     one \"regression\" \"canonical\". See Details. scale logical indicating orignal data set need scaled. default scale=TRUE","code":""},{"path":"/reference/PLS.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Partial Least Squares (PLS) — PLS","text":"PLS function fits PLS models \\(1, \\ldots ,\\)ncomp components. Multi-response models fully supported. type algorithm use specified mode argument. Two PLS algorithms available: PLS regression (\"regression\") PLS canonical analysis (\"canonical\") (see References).","code":""},{"path":"/reference/PLS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Partial Least Squares (PLS) — PLS","text":"PLS returns object class \"PLS\", list contains following components:  X centered standardized original predictor matrix. Y centered standardized original response vector matrix. ncomp number components included model. mode algorithm used fit model. mat.c matrix coefficients used internally predict. variates list containing variates. loadings list containing estimated loadings \\(X\\) \t\\(Y\\) variates. names list containing names used individuals variables.","code":""},{"path":"/reference/PLS.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Partial Least Squares (PLS) — PLS","text":"Liquet Benoit, Lafaye de Micheaux Pierre, Hejblum Boris, Thiebaut Rodolphe. group Sparse Group Partial Least Square approach applied Genomics context. Submitted. Le Cao, K.-., Martin, P.G.P., Robert-Grani\\'e, C. Besse, P. (2009). Sparse canonical methods biological data integration: application cross-platform study. BMC Bioinformatics 10:34. Le Cao, K.-., Rossouw, D., Robert-Grani\\'e, C. Besse, P. (2008). sparse PLS variable selection integrating Omics data. Statistical Applications Genetics Molecular Biology 7, article 35. Shen, H. Huang, J. Z. (2008). Sparse principal component analysis via regularized low rank matrix approximation. Journal Multivariate Analysis 99, 1015-1034. Tenenhaus, M. (1998). La r\\'egression PLS: th\\'eorie et pratique. Paris: Editions Technic. Wold H. (1966). Estimation principal components related models iterative least squares. : Krishnaiah, P. R. (editors), Multivariate Analysis. Academic Press, N.Y., 391-420.","code":""},{"path":"/reference/PLS.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Partial Least Squares (PLS) — PLS","text":"Benoit Liquet Pierre Lafaye de Micheaux.","code":""},{"path":[]},{"path":"/reference/PLS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Partial Least Squares (PLS) — PLS","text":"","code":"## Simulation of datasets X and Y with group variables  ### First example   n <- 100 sigma.gamma <- 1 sigma.e <- 1.5 p <- 400 q <- 500 theta.x1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5), rep(1.5,15),                rep(0, 5), rep(-1.5, 15), rep(0, 325)) theta.x2 <- c(rep(0, 320), rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),               rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 5))  theta.y1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5), rep(1.5, 15),               rep(0, 5), rep(-1.5, 15), rep(0, 425)) theta.y2 <- c(rep(0, 420), rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),               rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 5))                               Sigmax <- matrix(0, nrow = p, ncol = p) diag(Sigmax) <- sigma.e ^ 2 Sigmay <- matrix(0,nrow = q, ncol = q) diag(Sigmay) <- sigma.e ^ 2  set.seed(125)  gam1 <- rnorm(n) gam2 <- rnorm(n)  X <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.x1, theta.x2),                                                                  nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, p), sigma =                                                                                                      Sigmax, method = \"svd\") Y <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.y1, theta.y2),                                                                   nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, q), sigma =                                                                                                      Sigmay, method = \"svd\") #### PLS model model.PLS <- PLS(X, Y, ncomp = 2, mode = \"regression\")   ### Second example  library(sgPLSdevelop)  train <- 1:40 test <- 41:50 n.test <- length(test)  d <- data.create(n = 50, p = 10, q = 2, list = TRUE)  X <- d$X[train,] Y <- d$Y[train,] X.test <- d$X[test,] Y.test <- d$Y[test,]  ncompmax <- 10 model.pls <- PLS(X = X, Y = Y, ncomp = ncompmax, mode = \"regression\") pred <- predict.PLS(model.pls, newdata = X.test)$predict"},{"path":"/reference/pls_perf.html","id":null,"dir":"Reference","previous_headings":"","what":"A Capitalized Title (ideally limited to 65 characters) — pls perf","title":"A Capitalized Title (ideally limited to 65 characters) — pls perf","text":"function ....","code":""},{"path":"/reference/pls_perf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A Capitalized Title (ideally limited to 65 characters) — pls perf","text":"","code":"pls perf(x)"},{"path":"/reference/pls_perf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A Capitalized Title (ideally limited to 65 characters) — pls perf","text":"x","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"/reference/pls_perf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A Capitalized Title (ideally limited to 65 characters) — pls perf","text":"","code":"##---- Should be DIRECTLY executable !! ---- ##-- ==>  Define data, use random, ##--  or standard data sets, see data().  ## The function is currently defined as function (x)  {   } #> function (x)  #> { #> } #> <environment: 0x000002023b754a80>"},{"path":"/reference/predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for PLS, sPLS, gPLS, sgPLS, sPLDda, gPLSda, sgPLSda — predict","title":"Predict Method for PLS, sPLS, gPLS, sgPLS, sPLDda, gPLSda, sgPLSda — predict","text":"Predicted values based PLS, sparse PLS, group PLS, sparse group PLS, PLSda, sparse PLSda, group PLSda, sparse group PLSda models. New responses variates predicted using fitted model new matrix observations.","code":""},{"path":"/reference/predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for PLS, sPLS, gPLS, sgPLS, sPLDda, gPLSda, sgPLSda — predict","text":"","code":"# S3 method for class 'PLS' predict(object, newdata, ...)  # S3 method for class 'sPLS' predict(object, newdata, ...)  # S3 method for class 'gPLS' predict(object, newdata, ...)  # S3 method for class 'sgPLS' predict(object, newdata, ...)  # S3 method for class 'PLSda' predict(object, newdata, method = c(\"all\", \"max.dist\",          \"centroids.dist\", \"mahalanobis.dist\"), ...)  # S3 method for class 'sPLSda' predict(object, newdata, method = c(\"all\", \"max.dist\",          \"centroids.dist\", \"mahalanobis.dist\"), ...)  # S3 method for class 'gPLSda' predict(object, newdata, method = c(\"all\", \"max.dist\",          \"centroids.dist\", \"mahalanobis.dist\"), ...)  # S3 method for class 'sgPLSda' predict(object, newdata, method = c(\"all\", \"max.dist\",          \"centroids.dist\", \"mahalanobis.dist\"), ...)"},{"path":"/reference/predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for PLS, sPLS, gPLS, sgPLS, sPLDda, gPLSda, sgPLSda — predict","text":"object object class inheriting \"PLS\", \"sPLS\", \"gPLS\", \"sgPLS\", \"PLSda\", \"sPLSda\", \"gPLSda\"  \"sgPLSda\". newdata data matrix look explanatory variables used prediction. method method applied \"PLSda\", sPLSda, gPLSda sgPLSda predict class new data,     subset \"centroids.dist\", \"mahalanobis.dist\" \"max.dist\" (see Details). \tDefaults \"\". ... used currently.","code":""},{"path":"/reference/predict.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict Method for PLS, sPLS, gPLS, sgPLS, sPLDda, gPLSda, sgPLSda — predict","text":"predict function pls spls object created Sebastien Dejean, Ignacio Gonzalez, Amrit Singh Kim-Anh Le Cao mixOmics package. Similar code used sPLS, gPLS, sgPLS, sPLSda, gPLSda, sgPLSda models performed sgPLS package. predict function produces predicted values, obtained evaluating sparse PLS, group PLS sparse group PLS model returned \"PLS\", sPLS, gPLS sgPLS frame newdata. Variates newdata also returned. prediction values calculated based regression coefficients object$Y onto object$variates$X. Different class prediction methods proposed \"PLSda\", sPLSda, gPLSda sgPLSda: \"max.dist\" naive method predict class. based predicted matrix (object$predict) can seen probability matrix assign test data class. class largest class value predicted class. \"centroids.dist\" allocates individual \\(x\\) class \\(Y\\) minimizing \\(dist(\\code{x-variate}, G_l)\\), \\(G_l\\), \\(l = 1,...,L\\) centroids classes calculated \\(X\\)-variates model. \"mahalanobis.dist\" allocates individual \\(x\\) class \\(Y\\) \"centroids.dist\" using Mahalanobis metric calculation distance.","code":""},{"path":"/reference/predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for PLS, sPLS, gPLS, sgPLS, sPLDda, gPLSda, sgPLSda — predict","text":"predict produces list following components: predict three dimensional array predicted response values. dimensions \tcorrespond observations, response variables model dimension, respectively. variates Matrix predicted variates. B.hat Matrix regression coefficients (without intercept). class vector matrix predicted class using \\(1,...,\\)ncomp     (sparse)PLS-DA components. centroids matrix coordinates centroids.","code":""},{"path":"/reference/predict.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Predict Method for PLS, sPLS, gPLS, sgPLS, sPLDda, gPLSda, sgPLSda — predict","text":"Tenenhaus, M. (1998). La r\\'egression PLS: th\\'eorie et pratique. Paris: Editions Technic.","code":""},{"path":"/reference/predict.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predict Method for PLS, sPLS, gPLS, sgPLS, sPLDda, gPLSda, sgPLSda — predict","text":"Benoit Liquet Pierre Lafaye de Micheaux","code":""},{"path":[]},{"path":"/reference/q2.PLS.html","id":null,"dir":"Reference","previous_headings":"","what":"PLS function performance assessment using Q2 indicator. — q2.PLS_doc","title":"PLS function performance assessment using Q2 indicator. — q2.PLS_doc","text":"q2.PLS function allows assess PLS models using \\(Q2\\) criterion. function returns list including \\(Q2\\) values number components. plot allows visualize model performance according number components.","code":""},{"path":"/reference/q2.PLS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"PLS function performance assessment using Q2 indicator. — q2.PLS_doc","text":"","code":"q2.PLS(object, mode = \"regression\", ncomp.max = object$ncomp)"},{"path":"/reference/q2.PLS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"PLS function performance assessment using Q2 indicator. — q2.PLS_doc","text":"object Object class inheriting \"pls\". mode Character string. type algorithm use, (partially) matching one \"regression\" \"canonical\". ncomp.max number components desired q2 computing (number components computed model).","code":""},{"path":"/reference/q2.PLS.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"PLS function performance assessment using Q2 indicator. — q2.PLS_doc","text":"suggestion number components selection also given Q2 criterion. Endly, indicators given PRESS RSS number components \\(PRESSj\\) \\(RSSj\\) (matrices) given number components given column Y dataset.","code":""},{"path":"/reference/q2.PLS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"PLS function performance assessment using Q2 indicator. — q2.PLS_doc","text":"\\(PRESSj\\) \\(RSSj\\) matrices size \\(ncomp.max\\) x \\(q\\). row sums \\(PRESSj\\) \\(RSSj\\) give respectively PRESS RSS.","code":""},{"path":"/reference/q2.PLS.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"PLS function performance assessment using Q2 indicator. — q2.PLS_doc","text":"K.-. Le Cao, Zoe Welham, Multivariate data integration using R (pages 172 174), MixOmics","code":""},{"path":[]},{"path":"/reference/q2.PLS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"PLS function performance assessment using Q2 indicator. — q2.PLS_doc","text":"","code":"library(sgPLS) library(sgPLSdevelop)  d <- data.create(n = 50, p = 10, q = 2, list = TRUE) X <- d$X Y <- d$Y  ncomp.max <- 10 model.pls <- PLS(X = X, Y = Y, ncomp = ncomp.max, mode = \"regression\")  q2.res <- q2.PLS(model.pls, ncomp.max = ncomp.max, mode = \"regression\")   # q2 values q2.res$q2 #>  [1]  5.539087e-01  4.057099e-01 -5.844426e-01 -1.584254e+00 -6.826693e+00 #>  [6] -1.345286e+01 -3.202578e+01 -4.689224e+01 -6.246794e+01 -1.676202e+04  # PRESS values q2.res$PRESS #>  [1]   43.716949   20.947409   11.857546    9.278030    8.962343    9.101646 #>  [7]    9.931746   11.114458   13.215030 3432.696560  # RSS values q2.res$RSS #>  [1] 35.2477836  7.4837334  3.5902166  1.1450996  0.6297472  0.3007271 #>  [7]  0.2320722  0.2082158  0.2047779  0.2040215  # PRESS values by column q2.res$PRESSj #>                Y1         Y2 #>  [1,]   37.297558   6.419391 #>  [2,]   17.648120   3.299289 #>  [3,]    9.556047   2.301499 #>  [4,]    7.830616   1.447415 #>  [5,]    7.494771   1.467572 #>  [6,]    7.378585   1.723061 #>  [7,]    7.870608   2.061138 #>  [8,]    8.666772   2.447686 #>  [9,]    9.616132   3.598897 #> [10,] 2989.355370 443.341190  # RSS values by column q2.res$RSSj #>               Y1         Y2 #>  [1,] 30.5485479 4.69923571 #>  [2,]  4.6530060 2.83072744 #>  [3,]  1.7242712 1.86594546 #>  [4,]  0.6831842 0.46191536 #>  [5,]  0.4972907 0.13245646 #>  [6,]  0.1774595 0.12326751 #>  [7,]  0.1265549 0.10551728 #>  [8,]  0.1155939 0.09262194 #>  [9,]  0.1146747 0.09010324 #> [10,]  0.1145965 0.08942498"},{"path":"/reference/select.sgpls.html","id":null,"dir":"Reference","previous_headings":"","what":"Output of selected variables from a gPLS model or a sgPLS model — select.sgpls","title":"Output of selected variables from a gPLS model or a sgPLS model — select.sgpls","text":"function outputs selected variables component group sparse group PLS.","code":""},{"path":"/reference/select.sgpls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Output of selected variables from a gPLS model or a sgPLS model — select.sgpls","text":"","code":"select.sgpls(model)"},{"path":"/reference/select.sgpls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Output of selected variables from a gPLS model or a sgPLS model — select.sgpls","text":"model object class inheriting  \"gPLS\" \"sgPLS\".","code":""},{"path":"/reference/select.sgpls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Output of selected variables from a gPLS model or a sgPLS model — select.sgpls","text":"select.sgpls produces list following components: group.size.X matrix containing first column size groups \\(X\\) dataset. , next columns indicate size groups selected component. select.group.X list containing element (corresponding group \\(X\\) dataset) indices variables selected. group.size.Y matrix containing first column size groups \\(Y\\) dataset. next columns indicate size groups selected component. select.group.Y list containing element (corresponding group \\(Y\\) dataset) indices variables selected. select.X list containing element (corresponding component gPLS sgPLS model) names selected variables \\(X\\) dataset. select.Y list containing element (corresponding component gPLS sgPLS model) names selected variables \\(Y\\) dataset. select.X.total names variables selected gPLS sgPLS model regarding \\(X\\) matrix. select.Y.total names variables selected gPLS sgPLS model regarding \\(Y\\) matrix.","code":""},{"path":"/reference/select.sgpls.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Output of selected variables from a gPLS model or a sgPLS model — select.sgpls","text":"Benoit Liquet, b.liquet@uq.edu.au,  Pierre Lafaye de Micheaux lafaye@dms.umontreal.ca","code":""},{"path":"/reference/select.sgpls.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Output of selected variables from a gPLS model or a sgPLS model — select.sgpls","text":"","code":"if (FALSE) { # \\dontrun{   ## Simulation of datasets X and Y with group variables n <- 100 sigma.gamma <- 1 sigma.e <- 1.5 p <- 400 q <- 500 theta.x1 <- c(rep(1,15),rep(0,5),rep(-1,15),rep(0,5),rep(1.5,15)       ,rep(0,5),rep(-1.5,15),rep(0,325)) theta.x2 <- c(rep(0,320),rep(1,15),rep(0,5),rep(-1,15),rep(0,5),       rep(1.5,15),rep(0,5),rep(-1.5,15),rep(0,5))  theta.y1 <- c(rep(1,15),rep(0,5),rep(-1,15),rep(0,5),rep(1.5,15)       ,rep(0,5),rep(-1.5,15),rep(0,425)) theta.y2 <- c(rep(0,420),rep(1,15),rep(0,5),rep(-1,15),rep(0,5),       rep(1.5,15),rep(0,5),rep(-1.5,15),rep(0,5))                               Sigmax <- matrix(0, nrow = p, ncol = p) diag(Sigmax) <- sigma.e ^ 2 Sigmay <- matrix(0, nrow = q, ncol = q) diag(Sigmay) <- sigma.e ^ 2  set.seed(125)  gam1 <- rnorm(n) gam2 <- rnorm(n)  X <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.x1, theta.x2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, p), sigma =      Sigmax, method = \"svd\") Y <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.y1, theta.y2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, q), sigma =      Sigmay, method = \"svd\")  ind.block.x <- seq(20, 380, 20) ind.block.y <- seq(20, 480, 20)  #### gPLS model model.sgPLS <- sgPLS(X, Y, ncomp = 2, mode = \"regression\", keepX = c(4, 4),                     keepY = c(4, 4), ind.block.x = ind.block.x,                    ind.block.y = ind.block.y,                    alpha.x = c(0.5, 0.5), alpha.y = c(0.5, 0.5))  result.sgPLS <- select.sgpls(model.sgPLS) result.sgPLS$group.size.X result.sgPLS$group.size.Y  #### gPLS model model.gPLS <- gPLS(X, Y, ncomp = 2, mode = \"regression\", keepX = c(4, 4),       keepY = c(4,4), ind.block.x = ind.block.x ,ind.block.y = ind.block.y)  result.gPLS <- select.sgpls(model.gPLS) result.gPLS$group.size.X result.gPLS$group.size.Y    } # }"},{"path":"/reference/select.spls.html","id":null,"dir":"Reference","previous_headings":"","what":"Output of selected variables from a sPLS model — select.spls","title":"Output of selected variables from a sPLS model — select.spls","text":"function outputs selected variables component sPLS.","code":""},{"path":"/reference/select.spls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Output of selected variables from a sPLS model — select.spls","text":"","code":"select.spls(model)"},{"path":"/reference/select.spls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Output of selected variables from a sPLS model — select.spls","text":"model object class inheriting \"sPLS\".","code":""},{"path":"/reference/select.spls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Output of selected variables from a sPLS model — select.spls","text":"select.spls produces list following components: select.X list containing element (corresponding component sPLS model) names selected variables \\(X\\) dataset. select.Y list containing element (corresponding component sPLS model) names selected variables \\(Y\\) dataset. select.X.total names variables selected sPLS model regarding \\(X\\) matrix. select.Y.total names variables selected sPLS model regarding \\(Y\\) matrix.","code":""},{"path":"/reference/select.spls.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Output of selected variables from a sPLS model — select.spls","text":"Benoit Liquet, b.liquet@uq.edu.au,  Pierre Lafaye de Micheaux lafaye@dms.umontreal.ca","code":""},{"path":"/reference/select.spls.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Output of selected variables from a sPLS model — select.spls","text":"","code":"if (FALSE) { # \\dontrun{   ## Simulation of datasets X and Y with group variables n <- 100 sigma.gamma <- 1 sigma.e <- 1.5 p <- 400 q <- 500 theta.x1 <- c(rep(1,15),rep(0,5),rep(-1,15),rep(0,5),rep(1.5,15)              ,rep(0,5),rep(-1.5,15),rep(0,325)) theta.x2 <- c(rep(0,320),rep(1,15),rep(0,5),rep(-1,15),rep(0,5)              ,rep(1.5,15),rep(0,5),rep(-1.5,15),rep(0,5))  theta.y1 <- c(rep(1,15),rep(0,5),rep(-1,15),rep(0,5),rep(1.5,15)              ,rep(0,5),rep(-1.5,15),rep(0,425)) theta.y2 <- c(rep(0,420),rep(1,15),rep(0,5),rep(-1,15),rep(0,5),              rep(1.5,15),rep(0,5),rep(-1.5,15),rep(0,5))                               temp <-  matrix(c(theta.y1, theta.y2), nrow = 2, byrow = TRUE)  Sigmax <- matrix(0, nrow = p, ncol = p) diag(Sigmax) <- sigma.e ^ 2 Sigmay <- matrix(0, nrow = q, ncol = q) diag(Sigmay) <- sigma.e ^ 2  set.seed(125)  gam1 <- rnorm(n) gam2 <- rnorm(n)  X <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.x1, theta.x2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, p), sigma =      Sigmax, method = \"svd\") Y <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.y1, theta.y2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, q), sigma =      Sigmay, method = \"svd\")  ind.block.x <- seq(20, 380, 20) ind.block.y <- seq(20, 480, 20)  #### sPLS model model.sPLS <- sPLS(X, Y, ncomp = 2, mode = \"regression\", keepX = c(60, 60),                       keepY = c(60, 60)) result.sPLS <- select.spls(model.sPLS) result.sPLS$select.X result.sPLS$select.Y    } # }"},{"path":"/reference/sgPLS-internal.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal Functions — sgPLS-internal","title":"Internal Functions — sgPLS-internal","text":"Internal functions used user.","code":""},{"path":"/reference/sgPLS-package.html","id":null,"dir":"Reference","previous_headings":"","what":"Group and Sparse Group Partial Least Square Model — sgPLS-package","title":"Group and Sparse Group Partial Least Square Model — sgPLS-package","text":"sgPLS package provides sparse, group sparse group   version PLS approaches. \tmain functions :  sPLS sparse PLS, gPLS group PLS sgPLS sparse group PLS.","code":""},{"path":"/reference/sgPLS-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Group and Sparse Group Partial Least Square Model — sgPLS-package","text":"Benoit Liquet <b.liquet@uq.edu.au>, Pierre Lafaye de Micheaux","code":""},{"path":"/reference/sgPLS-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Group and Sparse Group Partial Least Square Model — sgPLS-package","text":"Liquet Benoit, Lafaye de Micheaux Pierre, Hejblum Boris, Thiebaut Rodolphe. group Sparse Group Partial Least Square approach applied Genomics context. Submitted.","code":""},{"path":[]},{"path":"/reference/sgPLS.html","id":null,"dir":"Reference","previous_headings":"","what":"Sparse Group Partial Least Squares (sgPLS) — sgPLS","title":"Sparse Group Partial Least Squares (sgPLS) — sgPLS","text":"Function perform  sparse group Partial Least Squares (sgPLS) conext datasets divided groups variables. sgPLS approach enables selection groups single feature levels.","code":""},{"path":"/reference/sgPLS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sparse Group Partial Least Squares (sgPLS) — sgPLS","text":"","code":"sgPLS(X, Y, ncomp, mode = \"regression\",      max.iter = 500, tol = 1e-06, keepX,       keepY = NULL,ind.block.x, ind.block.y = NULL, alpha.x, alpha.y = NULL,      upper.lambda = 10 ^ 5,scale=TRUE)"},{"path":"/reference/sgPLS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sparse Group Partial Least Squares (sgPLS) — sgPLS","text":"X Numeric matrix predictors. Y Numeric vector matrix responses (multi-response models). ncomp number components include model (see Details). mode character string. type algorithm use, (partially) matching     one \"regression\" \"canonical\". See Details. max.iter Integer, maximum number iterations. tol positive real, tolerance used iterative algorithm. keepX Numeric vector length ncomp, number variables     keep \\(X\\)-loadings. default variables kept model. keepY Numeric vector length ncomp, number variables     keep \\(Y\\)-loadings. default variables kept model. ind.block.x vector integers describing grouping \\(X\\) variables. (see example Details section). ind.block.y vector integers describing grouping \\(Y\\) variables (see example Details section). alpha.x mixing parameter (value 0 1) related sparsity within group \\(X\\) dataset. alpha.y mixing parameter (value 0 1) related sparsity within group \\(Y\\) dataset. upper.lambda default upper.lambda=10 ^ 5. large value specifying upper bound intervall lambda values searching value tuning parameter (lambda) corresponding non-zero group variables. scale logical indicating orignal data set need scaled. default scale=TRUE","code":""},{"path":"/reference/sgPLS.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sparse Group Partial Least Squares (sgPLS) — sgPLS","text":"sgPLS function fit gPLS models \\(1, \\ldots ,\\)ncomp components. Multi-response models fully supported. type algorithm use specified mode argument. Two gPLS algorithms available: gPLS regression (\"regression\") gPLS canonical analysis (\"canonical\") (see References). ind.block.x <- c(3, 10, 15) means \\(X\\) structured 4 groups: X1 X3; X4 X10, X11 X15 X16 X\\(p\\) \\(p\\) number variables \\(X\\) matrix.","code":""},{"path":"/reference/sgPLS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sparse Group Partial Least Squares (sgPLS) — sgPLS","text":"sgPLS returns object class \"sgPLS\", list contains following components:  X centered standardized original predictor matrix. Y centered standardized original response vector matrix. ncomp number components included model. mode algorithm used fit model. keepX Number \\(X\\) variables kept model component. keepY Number \\(Y\\) variables kept model component. mat.c Matrix coefficients used internally predict. variates List containing variates. loadings List containing estimated loadings \\(X\\) \t\\(Y\\) variates. names List containing names used individuals variables. tol tolerance used iterative algorithm, used subsequent S3 methods. max.iter maximum number iterations, used subsequent S3 methods. iter Vector containing number iterations convergence component. ind.block.x vector integers describing grouping \\(X\\) variables. ind.block.y vector consecutive integers describing grouping \\(Y\\) variables. alpha.x mixing parameter related sparsity within group \\(X\\) dataset. alpha.y mixing parameter related sparsity within group \\(Y\\) dataset. upper.lambda upper bound intervall lambda values searching value tuning parameter (lambda) corresponding non-zero group variables.","code":""},{"path":"/reference/sgPLS.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sparse Group Partial Least Squares (sgPLS) — sgPLS","text":"Liquet Benoit, Lafaye de Micheaux, Boris Hejblum, Rodolphe Thiebaut (2016). group Sparse Group Partial Least Square approach applied Genomics context. Bioinformatics. Le Cao, K.-., Martin, P.G.P., Robert-Grani\\'e, C. Besse, P. (2009). Sparse canonical methods biological data integration: application cross-platform study. BMC Bioinformatics 10:34. Le Cao, K.-., Rossouw, D., Robert-Grani\\'e, C. Besse, P. (2008). sparse PLS variable selection integrating Omics data. Statistical Applications Genetics Molecular Biology 7, article 35. Shen, H. Huang, J. Z. (2008). Sparse principal component analysis via regularized low rank matrix approximation. Journal Multivariate Analysis 99, 1015-1034. Tenenhaus, M. (1998). La r\\'egression PLS: th\\'eorie et pratique. Paris: Editions Technic. Wold H. (1966). Estimation principal components related models iterative least squares. : Krishnaiah, P. R. (editors), Multivariate Analysis. Academic Press, N.Y., 391-420.","code":""},{"path":"/reference/sgPLS.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Sparse Group Partial Least Squares (sgPLS) — sgPLS","text":"Benoit Liquet Pierre Lafaye de Micheaux.","code":""},{"path":[]},{"path":"/reference/sgPLS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sparse Group Partial Least Squares (sgPLS) — sgPLS","text":"","code":"## Simulation of datasets X and Y with group variables n <- 100 sigma.gamma <- 1 sigma.e <- 1.5 p <- 400 q <- 500 theta.x1 <- c(rep(1,15),rep(0,5),rep(-1,15),rep(0,5),rep(1.5,15)              ,rep(0,5),rep(-1.5,15),rep(0,325)) theta.x2 <- c(rep(0,320),rep(1,15),rep(0,5),rep(-1,15),rep(0,5)              ,rep(1.5,15),rep(0,5),rep(-1.5,15),rep(0,5))  theta.y1 <- c(rep(1,15),rep(0,5),rep(-1,15),rep(0,5),rep(1.5,15)              ,rep(0,5),rep(-1.5,15),rep(0,425)) theta.y2 <- c(rep(0,420),rep(1,15),rep(0,5),rep(-1,15),rep(0,5),       rep(1.5,15),rep(0,5),rep(-1.5,15),rep(0,5))                                Sigmax <- matrix(0, nrow = p, ncol = p) diag(Sigmax) <- sigma.e ^ 2 Sigmay <- matrix(0, nrow = q, ncol = q) diag(Sigmay) <- sigma.e ^ 2  set.seed(125)  gam1 <- rnorm(n) gam2 <- rnorm(n)  X <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.x1, theta.x2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, p), sigma =      Sigmax, method = \"svd\") Y <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.y1, theta.y2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, q), sigma =      Sigmay, method = \"svd\")   ind.block.x <- seq(20, 380, 20) ind.block.y <- seq(20, 480, 20) ##   model.sgPLS <- sgPLS(X, Y, ncomp = 2, mode = \"regression\", keepX = c(4, 4),                     keepY = c(4, 4), ind.block.x = ind.block.x                    ,ind.block.y = ind.block.y,                    alpha.x = c(0.95, 0.95), alpha.y = c(0.95, 0.95))  result.sgPLS <- select.sgpls(model.sgPLS) result.sgPLS$group.size.X #>    size comp1 comp2 #> 1    20    15     0 #> 2    20    15     0 #> 3    20    16     0 #> 4    20    15     0 #> 5    20     0     0 #> 6    20     0     0 #> 7    20     0     0 #> 8    20     0     0 #> 9    20     0     0 #> 10   20     0     0 #> 11   20     0     0 #> 12   20     0     0 #> 13   20     0     0 #> 14   20     0     0 #> 15   20     0     0 #> 16   20     0     0 #> 17   20     0    15 #> 18   20     0    15 #> 19   20     0    16 #> 20   20     0    15 result.sgPLS$group.size.Y #>    size comp1 comp2 #> 1    20    15     0 #> 2    20    15     0 #> 3    20    16     0 #> 4    20    16     0 #> 5    20     0     0 #> 6    20     0     0 #> 7    20     0     0 #> 8    20     0     0 #> 9    20     0     0 #> 10   20     0     0 #> 11   20     0     0 #> 12   20     0     0 #> 13   20     0     0 #> 14   20     0     0 #> 15   20     0     0 #> 16   20     0     0 #> 17   20     0     0 #> 18   20     0     0 #> 19   20     0     0 #> 20   20     0     0 #> 21   20     0     0 #> 22   20     0    15 #> 23   20     0    15 #> 24   20     0    15 #> 25   20     0    15"},{"path":"/reference/sgPLSda.html","id":null,"dir":"Reference","previous_headings":"","what":"Sparse Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — sgPLSda","title":"Sparse Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — sgPLSda","text":"Function perform sparse group Partial Least Squares classify samples (supervised analysis) select variables.","code":""},{"path":"/reference/sgPLSda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sparse Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — sgPLSda","text":"","code":"sgPLSda(X, Y, ncomp = 2, keepX = rep(ncol(X), ncomp),        max.iter = 500, tol = 1e-06, ind.block.x,      alpha.x, upper.lambda = 10 ^ 5)"},{"path":"/reference/sgPLSda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sparse Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — sgPLSda","text":"X numeric matrix predictors. NAs allowed. Y factor class vector discrete outcome. ncomp number components include model (see Details). keepX numeric vector length ncomp, number variables     keep \\(X\\)-loadings. default variables kept model. max.iter integer, maximum number iterations. tol positive real, tolerance used iterative algorithm. ind.block.x vector integers describing grouping \\(X\\)-variables. (see example Details section) alpha.x mixing parameter (value 0 1) related sparsity within group \\(X\\) dataset. upper.lambda default upper.lambda=10 ^ 5. large value specifying upper bound intervall lambda values searching value tuning parameter (lambda) corresponding non-zero group variables.","code":""},{"path":"/reference/sgPLSda.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sparse Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — sgPLSda","text":"sgPLSda function fit sgPLS models \\(1, \\ldots ,\\)ncomp components factor class vector Y. appropriate indicator (dummy) matrix created. ind.block.x <- c(3,10,15) means \\(X\\) structured 4 groups: X1 X3; X4 X10, X11 X15 X16 X\\(p\\) \\(p\\) number variables \\(X\\) matrix.","code":""},{"path":"/reference/sgPLSda.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sparse Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — sgPLSda","text":"sPLSda returns object class \"sPLSda\", list contains following components:  X centered standardized original predictor matrix. Y centered standardized indicator response vector matrix. ind.mat indicator matrix. ncomp number components included model. keepX number \\(X\\) variables kept model component. mat.c matrix coefficients used internally predict. variates list containing variates. loadings list containing estimated loadings X \tY variates. names list containing names used individuals variables. tol tolerance used iterative algorithm, used subsequent S3 methods max.iter maximum number iterations, used subsequent S3 methods iter Number iterations algorthm component ind.block.x vector integers describing grouping X variables. alpha.x mixing parameter related sparsity within group \\(X\\) dataset. upper.lambda upper bound intervall lambda values searching value tuning parameter (lambda) corresponding non-zero group variables.","code":""},{"path":"/reference/sgPLSda.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sparse Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — sgPLSda","text":"Liquet Benoit, Lafaye de Micheaux Pierre , Hejblum Boris, Thiebaut Rodolphe (2016). group Sparse Group Partial Least Square approach applied Genomics context. Bioinformatics. sPLS-DA: Le Cao, K.-., Boitard, S. Besse, P. (2011). Sparse PLS Discriminant Analysis: biologically relevant feature selection graphical displays multiclass problems. BMC Bioinformatics 12:253.","code":""},{"path":"/reference/sgPLSda.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Sparse Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — sgPLSda","text":"Benoit Liquet Pierre Lafaye de Micheaux.","code":""},{"path":[]},{"path":"/reference/sgPLSda.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sparse Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — sgPLSda","text":"","code":"data(simuData) X <- simuData$X Y <- simuData$Y ind.block.x <- seq(100, 900, 100) ind.block.x[2] <- 250 #To add some noise in the second group model <- sgPLSda(X, Y, ncomp = 3,ind.block.x=ind.block.x, keepX = c(2, 2, 2) , alpha.x = c(0.5,0.5,0.99)) result.sgPLSda <- select.sgpls(model) result.sgPLSda$group.size.X #>    size comp1 comp2 comp3 #> 1   100     0   100     0 #> 2   150     0     0   101 #> 3    50     0     0     0 #> 4   100   100     0     0 #> 5   100     0     0     0 #> 6   100     0   100     0 #> 7   100     0     0   100 #> 8   100     0     0     0 #> 9   100   100     0     0 #> 10  100     0     0     0 ##perf(model,criterion=\"all\",validation=\"loo\") -> res ##res$error.rate"},{"path":"/reference/simuData.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulated Data for group PLS-DA model — simuData","title":"Simulated Data for group PLS-DA model — simuData","text":"simulated data set contains expression 1000 genes 4 clusters 48 different individuals.","code":""},{"path":"/reference/simuData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulated Data for group PLS-DA model — simuData","text":"","code":"data(simuData)"},{"path":"/reference/simuData.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Simulated Data for group PLS-DA model — simuData","text":"list containing following components: X data matrix 48 rows 1000 columns. row represents       experimental sample, column single gene. Y factor variable indicating cluster subject","code":""},{"path":"/reference/simuData.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulated Data for group PLS-DA model — simuData","text":"data simulated 6 groups 100 genes linked 4 clusters. others 4 groups 100 genes added represent noise. relevant groups group 1,2,4,6,7 9. groups 3,5,8, 10 noise groups.","code":""},{"path":"/reference/sPLS.html","id":null,"dir":"Reference","previous_headings":"","what":"Sparse Partial Least Squares (sPLS) — sPLS","title":"Sparse Partial Least Squares (sPLS) — sPLS","text":"Function perform sparse Partial Least Squares (sPLS). sPLS approach combines integration variable selection simultaneously two data sets one-step strategy.","code":""},{"path":"/reference/sPLS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sparse Partial Least Squares (sPLS) — sPLS","text":"","code":"sPLS(X, Y, ncomp, mode = \"regression\",      max.iter = 500, tol = 1e-06, keepX = rep(ncol(X), ncomp),       keepY = rep(ncol(Y), ncomp),scale=TRUE)"},{"path":"/reference/sPLS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sparse Partial Least Squares (sPLS) — sPLS","text":"X Numeric matrix predictors. Y Numeric vector matrix responses (multi-response models). ncomp number components include model (see Details). mode Character string. type algorithm use, (partially) matching     one \"regression\" \"canonical\". See Details. max.iter Integer, maximum number iterations. tol positive real, tolerance used iterative algorithm. keepX Numeric vector length ncomp, number variables     keep \\(X\\)-loadings. default variables kept model. keepY Numeric vector length ncomp, number variables     keep \\(Y\\)-loadings. default variables kept model. scale logical indicating orignal data set need scaled. default scale=TRUE","code":""},{"path":"/reference/sPLS.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sparse Partial Least Squares (sPLS) — sPLS","text":"sPLS function fit sPLS models \\(1, \\ldots ,\\)ncomp components. Multi-response models fully supported. type algorithm use specified mode argument. Two sPLS algorithms available: sPLS regression (\"regression\") sPLS canonical analysis (\"canonical\") (see References).","code":""},{"path":"/reference/sPLS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sparse Partial Least Squares (sPLS) — sPLS","text":"sPLS returns object class \"sPLS\", list contains following components:  X centered standardized original predictor matrix. Y centered standardized original response vector matrix. ncomp number components included model. mode algorithm used fit model. keepX Number \\(X\\) variables kept model component. keepY Number \\(Y\\) variables kept model component. mat.c Matrix coefficients used internally predict. variates List containing variates. loadings List containing estimated loadings \\(X\\) \t\\(Y\\) variates. names List containing names used individuals variables. tol tolerance used iterative algorithm, used subsequent S3 methods max.iter maximum number iterations, used subsequent S3 methods","code":""},{"path":"/reference/sPLS.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sparse Partial Least Squares (sPLS) — sPLS","text":"Liquet Benoit, Lafaye de Micheaux Pierre, Hejblum Boris, Thiebaut Rodolphe. group Sparse Group Partial Least Square approach applied Genomics context. Submitted. Le Cao, K.-., Martin, P.G.P., Robert-Grani\\', C. Besse, P. (2009). Sparse canonical methods biological data integration: application cross-platform study. BMC Bioinformatics 10:34. Le Cao, K.-., Rossouw, D., Robert-Grani\\'e, C. Besse, P. (2008). sparse PLS variable selection integrating Omics data. Statistical Applications Genetics Molecular Biology 7, article 35. Shen, H. Huang, J. Z. (2008). Sparse principal component analysis via regularized low rank matrix approximation. Journal Multivariate Analysis 99, 1015-1034. Tenenhaus, M. (1998). La r\\'egression PLS: th\\'eorie et pratique. Paris: Editions Technic. Wold H. (1966). Estimation principal components related models iterative least squares. : Krishnaiah, P. R. (editors), Multivariate Analysis. Academic Press, N.Y., 391-420.","code":""},{"path":"/reference/sPLS.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Sparse Partial Least Squares (sPLS) — sPLS","text":"Benoit Liquet Pierre Lafaye de Micheaux.","code":""},{"path":[]},{"path":"/reference/sPLS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sparse Partial Least Squares (sPLS) — sPLS","text":"","code":"## Simulation of datasets X and Y with group variables n <- 100 sigma.gamma <- 1 sigma.e <- 1.5 p <- 400 q <- 500 theta.x1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),       rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 325)) theta.x2 <- c(rep(0, 320), rep(1, 15), rep(0, 5), rep(-1, 15),       rep(0, 5), rep(1.5, 15), rep(0, 5), rep(-1.5, 15),       rep(0, 5))  theta.y1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),        rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 425)) theta.y2 <- c(rep(0, 420), rep(1, 15), rep(0, 5), rep(-1, 15)       ,rep(0, 5), rep(1.5, 15), rep(0, 5), rep(-1.5, 15)       , rep(0, 5))                               Sigmax <- matrix(0, nrow = p, ncol = p) diag(Sigmax) <- sigma.e ^ 2 Sigmay <- matrix(0, nrow = q, ncol = q) diag(Sigmay) <- sigma.e ^ 2  set.seed(125)  gam1 <- rnorm(n) gam2 <- rnorm(n)  X <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.x1, theta.x2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, p), sigma =      Sigmax, method = \"svd\") Y <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.y1, theta.y2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, q), sigma =      Sigmay, method = \"svd\")   ind.block.x <- seq(20, 380, 20) ind.block.y <- seq(20, 480, 20)   #### sPLS model model.sPLS <- sPLS(X, Y, ncomp = 2, mode = \"regression\", keepX = c(60, 60),                       keepY = c(60, 60)) result.sPLS <- select.spls(model.sPLS) result.sPLS$select.X #> [[1]] #>  X1  X2  X3  X4  X5  X6  X7  X8  X9 X10 X11 X12 X13 X14 X15 X21 X22 X23 X24 X25  #>   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  21  22  23  24  25  #> X26 X27 X28 X29 X30 X31 X32 X33 X34 X35 X41 X42 X43 X44 X45 X46 X47 X48 X49 X50  #>  26  27  28  29  30  31  32  33  34  35  41  42  43  44  45  46  47  48  49  50  #> X51 X52 X53 X54 X55 X61 X62 X63 X64 X65 X66 X67 X68 X69 X70 X71 X72 X73 X74 X75  #>  51  52  53  54  55  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  #>  #> [[2]] #> X321 X322 X323 X324 X325 X326 X327 X328 X329 X330 X331 X332 X333 X334 X335 X341  #>  321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  341  #> X342 X343 X344 X345 X346 X347 X348 X349 X350 X351 X352 X353 X354 X355 X361 X362  #>  342  343  344  345  346  347  348  349  350  351  352  353  354  355  361  362  #> X363 X364 X365 X366 X367 X368 X369 X370 X371 X372 X373 X374 X375 X381 X382 X383  #>  363  364  365  366  367  368  369  370  371  372  373  374  375  381  382  383  #> X384 X385 X386 X387 X388 X389 X390 X391 X392 X393 X394 X395  #>  384  385  386  387  388  389  390  391  392  393  394  395  #>  result.sPLS$select.Y #> [[1]] #>  Y1  Y2  Y3  Y4  Y5  Y6  Y7  Y8  Y9 Y10 Y11 Y12 Y13 Y14 Y15 Y21 Y22 Y23 Y24 Y25  #>   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  21  22  23  24  25  #> Y26 Y27 Y28 Y29 Y30 Y31 Y32 Y33 Y34 Y35 Y41 Y42 Y43 Y44 Y45 Y46 Y47 Y48 Y49 Y50  #>  26  27  28  29  30  31  32  33  34  35  41  42  43  44  45  46  47  48  49  50  #> Y51 Y52 Y53 Y54 Y55 Y61 Y62 Y63 Y64 Y65 Y66 Y67 Y68 Y69 Y70 Y71 Y72 Y73 Y74 Y75  #>  51  52  53  54  55  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  #>  #> [[2]] #> Y421 Y422 Y423 Y424 Y425 Y426 Y427 Y428 Y429 Y430 Y431 Y432 Y433 Y434 Y435 Y441  #>  421  422  423  424  425  426  427  428  429  430  431  432  433  434  435  441  #> Y442 Y443 Y444 Y445 Y446 Y447 Y448 Y449 Y450 Y451 Y452 Y453 Y454 Y455 Y461 Y462  #>  442  443  444  445  446  447  448  449  450  451  452  453  454  455  461  462  #> Y463 Y464 Y465 Y466 Y467 Y468 Y469 Y470 Y471 Y472 Y473 Y474 Y475 Y481 Y482 Y483  #>  463  464  465  466  467  468  469  470  471  472  473  474  475  481  482  483  #> Y484 Y485 Y486 Y487 Y488 Y489 Y490 Y491 Y492 Y493 Y494 Y495  #>  484  485  486  487  488  489  490  491  492  493  494  495  #>"},{"path":"/reference/sPLSda.html","id":null,"dir":"Reference","previous_headings":"","what":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — sPLSda","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — sPLSda","text":"Function perform sparse Partial Least Squares classify samples (supervised analysis) select variables.","code":""},{"path":"/reference/sPLSda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — sPLSda","text":"","code":"sPLSda(X, Y, ncomp = 2, keepX = rep(ncol(X), ncomp),        max.iter = 500, tol = 1e-06)"},{"path":"/reference/sPLSda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — sPLSda","text":"X numeric matrix predictors. NAs allowed. Y factor class vector discrete outcome. ncomp number components include model (see Details). keepX numeric vector length ncomp, number variables     keep \\(X\\)-loadings. default variables kept model. max.iter integer, maximum number iterations. tol positive real, tolerance used iterative algorithm.","code":""},{"path":"/reference/sPLSda.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — sPLSda","text":"sPLSda function fit sPLS models \\(1, \\ldots ,\\)ncomp components factor class vector Y. appropriate indicator (dummy) matrix created.","code":""},{"path":"/reference/sPLSda.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — sPLSda","text":"sPLSda returns object class \"sPLSda\", list contains following components:  X centered standardized original predictor matrix. Y centered standardized indicator response vector matrix. ind.mat indicator matrix. ncomp number components included model. keepX number \\(X\\) variables kept model component. mat.c matrix coefficients used internally predict. variates list containing variates. loadings list containing estimated loadings X \tY variates. names list containing names used individuals variables. tol tolerance used iterative algorithm, used subsequent S3 methods max.iter maximum number iterations, used subsequent S3 methods iter Number iterations algorthm component","code":""},{"path":"/reference/sPLSda.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — sPLSda","text":"sPLS-DA: Le Cao, K.-., Boitard, S. Besse, P. (2011). Sparse PLS Discriminant Analysis: biologically relevant feature selection graphical displays multiclass problems. BMC Bioinformatics 12:253.","code":""},{"path":"/reference/sPLSda.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — sPLSda","text":"Benoit Liquet Pierre Lafaye de Micheaux.","code":""},{"path":[]},{"path":"/reference/sPLSda.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) — sPLSda","text":"","code":"### Examples from mixOmics packages     data(liver.toxicity) X <- as.matrix(liver.toxicity$gene) # Y will be transformed as a factor in the function, # but we set it as a factor to set up the colors. Y <- as.factor(liver.toxicity$treatment[, 4])  model <- sPLSda(X, Y, ncomp = 2, keepX = c(20, 20))"},{"path":"/reference/tuning.gPLS.X.html","id":null,"dir":"Reference","previous_headings":"","what":"Choice of the tuning parameter (number of groups) related to predictor matrix for gPLS model (regression mode) — tuning.gPLS.X","title":"Choice of the tuning parameter (number of groups) related to predictor matrix for gPLS model (regression mode) — tuning.gPLS.X","text":"grid tuning parameter, function computes leave-one-M-fold cross-validation MSEP (Mean Square Error Prediction) gPLS model.","code":""},{"path":"/reference/tuning.gPLS.X.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Choice of the tuning parameter (number of groups) related to predictor matrix for gPLS model (regression mode) — tuning.gPLS.X","text":"","code":"tuning.gPLS.X(X,Y,folds=10,validation=c(\"Mfold\",\"loo\"),     ncomp,keepX=NULL,grid.X,setseed,progressBar=FALSE,     ind.block.x=ind.block.x)"},{"path":"/reference/tuning.gPLS.X.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Choice of the tuning parameter (number of groups) related to predictor matrix for gPLS model (regression mode) — tuning.gPLS.X","text":"X Numeric matrix data frame \\((n \\times p)\\), observations \\(X\\) variables. Y Numeric matrix data frame \\((n \\times q)\\), observations \\(Y\\) variables. folds Positive integer. Number folds use validation=\"Mfold\". Defaults folds=10. validation Character string. kind (internal) cross-validation method use, (partially) matching one \"Mfolds\" (M-folds) \"loo\" (leave-one-). ncomp Number component investigating choice tuning parameter. keepX Vector integer indicating number group variables keep component. See details information. grid.X Vector integers defining values tuning parameter (corresponding number group variables select) cross-validation score computed. setseed Integer indicating random number generation state. progressBar default set FALSE output progress bar computation. ind.block.x vector integers describing grouping X variables. (see example details section)","code":""},{"path":"/reference/tuning.gPLS.X.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Choice of the tuning parameter (number of groups) related to predictor matrix for gPLS model (regression mode) — tuning.gPLS.X","text":"validation=\"Mfolds\", M-fold cross-validation performed calling Mfold. folds generated. number cross-validation folds specified argument folds. validation=\"loo\", leave-one-cross-validation performed calling loo function. case arguments folds ignored. keepX specified (default NULL), element keepX indicates value tuning parameter corresponding component. choice tuning parameters corresponding remaining components investigating evaluating cross-validation score different values defining grid.X.","code":""},{"path":"/reference/tuning.gPLS.X.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Choice of the tuning parameter (number of groups) related to predictor matrix for gPLS model (regression mode) — tuning.gPLS.X","text":"returned value list components: MSEP Matrix containing cross-validation score computed grid. keepX Value tuning parameter (lambda)     cross-validation method reached minimum.","code":""},{"path":"/reference/tuning.gPLS.X.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Choice of the tuning parameter (number of groups) related to predictor matrix for gPLS model (regression mode) — tuning.gPLS.X","text":"Benoit Liquet Pierre Lafaye de Micheaux","code":""},{"path":"/reference/tuning.gPLS.X.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Choice of the tuning parameter (number of groups) related to predictor matrix for gPLS model (regression mode) — tuning.gPLS.X","text":"","code":"if (FALSE) { # \\dontrun{   ## Simulation of Datasets X (with group variables) and Y a multivariate response variable  n <- 200 sigma.e <- 0.5 p <- 400 q <- 10 theta.x1 <- c(rep(1,15),rep(0,5),rep(-1,15),rep(0,5),rep(1.5,15),       rep(0,5),rep(-1.5,15),rep(0,325)) theta.x2 <- c(rep(0,320),rep(1,15),rep(0,5),rep(-1,15),rep(0,5),       rep(1.5,15),rep(0,5),rep(-1.5,15),rep(0,5))  set.seed(125) theta.y1 <- runif(10,0.5,2) theta.y2 <- runif(10,0.5,2)    temp <-  matrix(c(theta.y1,theta.y2),nrow=2,byrow=TRUE)  Sigmax <- matrix(0,nrow=p,ncol=p) diag(Sigmax) <- sigma.e^2 Sigmay <- matrix(0,nrow=q,ncol=q) diag(Sigmay) <- sigma.e^2  gam1 <- rnorm(n,0,1) gam2 <- rnorm(n,0,1)  X <- matrix(c(gam1,gam2),ncol=2,byrow=FALSE)%*%matrix(c(theta.x1,theta.x2),nrow=2,byrow=TRUE) +rmvnorm(n,mean=rep(0,p),sigma=Sigmax,method=\"svd\") Y <- matrix(c(gam1,gam2),ncol=2,byrow=FALSE)%*%t(svd(temp)$v) +rmvnorm(n,mean=rep(0,q),sigma=Sigmay,method=\"svd\")  ind.block.x <- seq(20,380,20)  grid.X <- 1:16  ## Strategy with same value for both components tun.gPLS <- tuning.gPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"),      ncomp=2,keepX = NULL, grid.X=grid.X, setseed=1, progressBar = FALSE,      ind.block.x = ind.block.x)   tun.gPLS$keepX # for each component  ##For a sequential strategy tun.gPLS.1 <- tuning.gPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"),        ncomp=1, keepX = NULL, grid.X=grid.X, setseed=1,                              ind.block.x = ind.block.x)  tun.gPLS.1$keepX # for the first component  tun.gPLS.2 <- tuning.gPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"), ncomp=2,                              keepX = tun.gPLS.1$keepX , grid.X=grid.X, setseed=1,                              ind.block.x = ind.block.x)   tun.gPLS.2$keepX # for the second component } # }"},{"path":"/reference/tuning.sgPLS.X.html","id":null,"dir":"Reference","previous_headings":"","what":"Choice of the tuning parameters (number of groups and mixing parameter) related to predictor matrix for sgPLS model (regression mode) — tuning.sgPLS.X","title":"Choice of the tuning parameters (number of groups and mixing parameter) related to predictor matrix for sgPLS model (regression mode) — tuning.sgPLS.X","text":"grid two dimension tuning parameters, function computes leave-one-M-fold cross-validation MSEP (Mean Square Error Prediction) sgPLS model.","code":""},{"path":"/reference/tuning.sgPLS.X.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Choice of the tuning parameters (number of groups and mixing parameter) related to predictor matrix for sgPLS model (regression mode) — tuning.sgPLS.X","text":"","code":"tuning.sgPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"), ncomp,         keepX = NULL, alpha.x = NULL, grid.gX, grid.alpha.X,         setseed, progressBar = FALSE, ind.block.x = ind.block.x,         upper.lambda = 10 ^ 9)"},{"path":"/reference/tuning.sgPLS.X.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Choice of the tuning parameters (number of groups and mixing parameter) related to predictor matrix for sgPLS model (regression mode) — tuning.sgPLS.X","text":"X Numeric matrix data frame \\((n \\times p)\\), observations \\(X\\) variables. Y Numeric matrix data frame \\((n \\times q)\\), observations \\(Y\\) variables. folds Positive integer. Number folds use validation=\"Mfold\". Defaults     folds=10. validation Character string. kind (internal) cross-validation method use,     (partially) matching one \"Mfolds\" (M-folds) \"loo\" (leave-one-). ncomp Number component investigating choice tuning parameter. keepX Vector integer indicating number group variables keep component. See Details information. alpha.x Numeric vector indicating number group variables keep component. See Details information. grid.gX,grid.alpha.X Vector numeric defining values   tuning parameter lambda (number groups select) tuning   parameter alpha (mixing paramter values 0 1) cross-validation score computed setseed Integer indicating random number generation state. progressBar default set FALSE output progress bar computation. ind.block.x vector integers describing grouping X variables. (see example Details section). upper.lambda default upper.lambda=10 ^ 9. large value specifying upper bound intervall lambda values searching value tuning parameter (lambda) corresponding non-zero group variables.","code":""},{"path":"/reference/tuning.sgPLS.X.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Choice of the tuning parameters (number of groups and mixing parameter) related to predictor matrix for sgPLS model (regression mode) — tuning.sgPLS.X","text":"validation = \"Mfolds\", M-fold cross-validation performed calling Mfold. folds generated. number cross-validation folds specified argument folds. validation = \"loo\", leave-one-cross-validation performed calling loo function. case arguments folds ignored. keepX specified (default NULL), element keepX indicates value tuning parameter corresponding component. choice tuning parameters corresponding remaining components investigating evaluating cross-validation score different values defining grid.X. alpha.x specified (default NULL), element alpha.x indicates value tuning parameter (alpha) corresponding component. choice tuning parameters corresponding remaining components investigating evaluating cross-vlidation score different values defining grid.alpha.X.","code":""},{"path":"/reference/tuning.sgPLS.X.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Choice of the tuning parameters (number of groups and mixing parameter) related to predictor matrix for sgPLS model (regression mode) — tuning.sgPLS.X","text":"returned value list components: MSEP vector containing cross-validation score computed grid keepX value tuning parameter     cross-validation method reached minimum. alphaX value tuning parameter (alpha)     cross-validation method reached minimum.","code":""},{"path":"/reference/tuning.sgPLS.X.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Choice of the tuning parameters (number of groups and mixing parameter) related to predictor matrix for sgPLS model (regression mode) — tuning.sgPLS.X","text":"Benoit Liquet Pierre Lafaye de Micheaux","code":""},{"path":"/reference/tuning.sgPLS.X.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Choice of the tuning parameters (number of groups and mixing parameter) related to predictor matrix for sgPLS model (regression mode) — tuning.sgPLS.X","text":"","code":"if (FALSE) { # \\dontrun{   ## Simulation of datasets X (with group variables) and Y a multivariate response variable  n <- 200 sigma.e <- 0.5 p <- 400 q <- 10 theta.x1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5), rep(1.5, 15),       rep(0, 5), rep(-1.5, 15), rep(0, 325)) theta.x2 <- c(rep(0, 320), rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),       rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 5))  set.seed(125) theta.y1 <- runif(10, 0.5, 2) theta.y2 <- runif(10, 0.5, 2)    temp <-  matrix(c(theta.y1, theta.y2), nrow = 2, byrow = TRUE)  Sigmax <- matrix(0, nrow = p, ncol = p) diag(Sigmax) <- sigma.e ^ 2 Sigmay <- matrix(0, nrow = q, ncol = q) diag(Sigmay) <- sigma.e ^ 2  gam1 <- rnorm(n) gam2 <- rnorm(n)  X <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.x1, theta.x2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, p), sigma =      Sigmax, method = \"svd\") Y <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% t(svd(temp)$v)       + rmvnorm(n, mean = rep(0, q), sigma = Sigmay, method = \"svd\")  ind.block.x <- seq(20, 380, 20)  grid.X <- 2:16 grid.alpha.X <- c(0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8, 0.95) ## Strategy with same value of each tuning parameter for both components tun.sgPLS <- tuning.sgPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"),          ncomp = 2,keepX = NULL, alpha.x = NULL,grid.gX = grid.X,          grid.alpha.X = grid.alpha.X, setseed = 1, progressBar = FALSE,          ind.block.x = ind.block.x)   tun.sgPLS$keepX # for each component tun.sgPLS$alphaX # for each component ##For a sequential strategy tun.sgPLS.1 <- tuning.sgPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"),                ncomp = 1, keepX = NULL,  alpha.x = NULL, grid.gX = grid.X,          grid.alpha.X = grid.alpha.X, setseed = 1,           ind.block.x = ind.block.x)              tun.sgPLS.1$keepX # for the first component tun.sgPLS.1$alphaX # for the first component  tun.sgPLS.2 <- tuning.sgPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"),            ncomp = 2, keepX = tun.sgPLS.1$keepX,           alpha.x = tun.sgPLS.1$alphaX,           grid.gX = grid.X,           grid.alpha.X = grid.alpha.X,           setseed = 1,           ind.block.x = ind.block.x)   tun.sgPLS.2$keepX # for the second component tun.sgPLS.2$alphaX # for the second component } # }"},{"path":"/reference/tuning.sPLS.X.html","id":null,"dir":"Reference","previous_headings":"","what":"Choice of the tuning parameter (number of variables) related to predictor matrix for sPLS model (regression mode) — tuning.sPLS.X","title":"Choice of the tuning parameter (number of variables) related to predictor matrix for sPLS model (regression mode) — tuning.sPLS.X","text":"grid tuning parameter, function computes leave-one-M-fold cross-validation MSEP (Mean Square Error Prediction) sPLS model.","code":""},{"path":"/reference/tuning.sPLS.X.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Choice of the tuning parameter (number of variables) related to predictor matrix for sPLS model (regression mode) — tuning.sPLS.X","text":"","code":"tuning.sPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"), ncomp,     keepX = NULL, grid.X, setseed, progressBar = FALSE)"},{"path":"/reference/tuning.sPLS.X.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Choice of the tuning parameter (number of variables) related to predictor matrix for sPLS model (regression mode) — tuning.sPLS.X","text":"X Numeric matrix data frame \\((n \\times p)\\), observations \\(X\\) variables. Y Numeric matrix data frame \\((n \\times q)\\), observations \\(Y\\) variables. folds Positive integer. Number folds use validation=\"Mfold\". Defaults folds=10. validation Character string. kind (internal) cross-validation method use, (partially) matching one \"Mfolds\" (M-folds) \"loo\" (leave-one-). ncomp Number component investigating choice tuning parameter. keepX Vector integer indicating number variables keep component. See Details information. grid.X Vector integers defining values tuning parameter (corresponding number variables select) cross-validation score computed. setseed Integer indicating random number generation state. progressBar default set FALSE output progress bar computation.","code":""},{"path":"/reference/tuning.sPLS.X.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Choice of the tuning parameter (number of variables) related to predictor matrix for sPLS model (regression mode) — tuning.sPLS.X","text":"validation=\"Mfolds\", M-fold cross-validation performed calling Mfold. folds generated. number cross-validation folds specified argument folds. validation=\"loo\", leave-one-cross-validation performed calling loo function. case arguments folds ignored. keepX specified (default NULL), element keepX indicates value tuning parameter corresponding component. choice tuning parameters corresponding remaining components investigating evaluating cross-validation score different values defining grid.X.","code":""},{"path":"/reference/tuning.sPLS.X.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Choice of the tuning parameter (number of variables) related to predictor matrix for sPLS model (regression mode) — tuning.sPLS.X","text":"returned value list components: MSEP Vector containing cross-validation score computed grid keepX Value tuning parameter     cross-validation method reached minimum.","code":""},{"path":"/reference/tuning.sPLS.X.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Choice of the tuning parameter (number of variables) related to predictor matrix for sPLS model (regression mode) — tuning.sPLS.X","text":"Benoit Liquet Pierre Lafaye de Micheaux","code":""},{"path":"/reference/tuning.sPLS.X.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Choice of the tuning parameter (number of variables) related to predictor matrix for sPLS model (regression mode) — tuning.sPLS.X","text":"","code":"if (FALSE) { # \\dontrun{   ## Simulation of Datasets X (with group variables) and Y a multivariate response variable  n <- 200 sigma.e <- 0.5 p <- 400 q <- 10 theta.x1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5), rep(1.5, 15),       rep(0, 5), rep(-1.5, 15), rep(0, 325)) theta.x2 <- c(rep(0, 320), rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),       rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 5))  set.seed(125) theta.y1 <- runif(10, 0.5, 2) theta.y2 <- runif(10, 0.5, 2)    temp <-  matrix(c(theta.y1, theta.y2), nrow = 2, byrow = TRUE)  Sigmax <- matrix(0, nrow = p, ncol = p) diag(Sigmax) <- sigma.e ^ 2 Sigmay <- matrix(0, nrow = q, ncol = q) diag(Sigmay) <- sigma.e ^ 2  gam1 <- rnorm(n) gam2 <- rnorm(n)  X <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.x1, theta.x2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, p), sigma =      Sigmax, method = \"svd\") Y <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% t(svd(temp)$v)      + rmvnorm(n, mean = rep(0, q), sigma = Sigmay, method = \"svd\")   grid.X <- c(20, 30, 40, 50, 60, 70, 80, 90, 100, 120, 150, 200, 250, 300)  ## Strategy with same value for both components tun.sPLS <- tuning.sPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"),      ncomp = 2, keepX = NULL, grid.X = grid.X, setseed = 1) tun.sPLS$keepX # for each component  ##For a sequential strategy tun.sPLS.1 <- tuning.sPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"),      ncomp = 1, keepX = NULL, grid.X = grid.X, setseed = 1)  tun.sPLS.1$keepX # for the first component  tun.sPLS.2 <- tuning.sPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"),      ncomp = 2, keepX = tun.sPLS.1$keepX , grid.X = grid.X, setseed = 1) tun.sPLS.2$keepX # for the second component  } # }"}]
