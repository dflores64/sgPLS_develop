[{"path":"/articles/gPLS_method.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"gPLS method","text":"gPLS method allows dimensionality reduction still creating HH new components selecting groups variables. method therefore ideally suited context several groups predictors (XX) correlated several groups responses (YY); groups must selected. Concretely, matrix XX divided KK blocks representing groups variables. true YY, divided LL blocks. denoted X=(X1,X2,...,XK)X = (X_1, X_2, ..., X_K) Y=(Y1,Y2,...,YL)Y = (Y_1, Y_2, ..., Y_L). scores therefore given : th=âˆ‘k=1KXk(h)uk(h)=Xut_{h} = \\sum_{k = 1}^{K} X^{(h)}_k u^{(h)}_k = Xush=âˆ‘l=1LYl(h)vl(h)=Yvs_{h} = \\sum_{l = 1}^{L} Y^{(h)}_l v^{(h)}_l = Yv Caution: uk(h)u^{(h)}_k vl(h)v^{(h)}_l longer real numbers (dimension 1) vectors dimensions pkp_k qlq_l respectively.","code":""},{"path":"/articles/gPLS_method.html","id":"minimization-problem","dir":"Articles","previous_headings":"","what":"Minimization problem","title":"gPLS method","text":"function minimized written : âˆ‘k=1Kâˆ‘l=1Lâˆ¥Mk,lâˆ’ukvlTâˆ¥F2+PÎ»(u)+PÎ¼(v)\\sum_{k=1}^{K} \\sum_{l=1}^{L} \\left\\| M_{k,l} - u_k v_l^T \\right\\|_F^2 + P_\\lambda(u) + P_\\mu(v) Mk,l=XkTYlM_{k,l} = X_k^T Y_l. penalties : PÎ»(u)=Î»âˆ‘k=1Kpkâˆ¥ukâˆ¥2etPÎ¼(v)=Î¼âˆ‘l=1Lqlâˆ¥vlâˆ¥2P_\\lambda(u) = \\lambda \\sum_{k=1}^{K} \\sqrt{p_k} \\|u_k\\|_2 \\quad \\text{et} \\quad P_\\mu(v) = \\mu \\sum_{l=1}^{L} \\sqrt{q_l} \\|v_l\\|_2 Frobenius norm can also written: âˆ¥Mk,lâˆ’ukvlTâˆ¥F2=Tr(Mk,lMk,lT)âˆ’2Tr(ukvlMk,lT)+Tr(ukukTvlTvl)\\|M_{k,l} - u_k v_l^T\\|_F^2 = \\mathrm{Tr}(M_{k,l}M_{k,l}^T) - 2\\,\\mathrm{Tr}(u_k v_l M_{k,l}^T) + \\mathrm{Tr}(u_k u_k^T v_l^T v_l) function becomes: âˆ‘k=1Kâˆ‘l=1LTr(Mk,lMk,lT)âˆ’2âˆ‘k=1Kâˆ‘l=1LTr(ukvlTMk,lT)+âˆ‘k=1Kâˆ‘l=1LTr(ukukTvlTvl)\\sum_{k=1}^{K} \\sum_{l=1}^{L} \\mathrm{Tr}(M_{k,l}M_{k,l}^T) - 2 \\sum_{k=1}^{K} \\sum_{l=1}^{L} \\mathrm{Tr}(u_k v_l^T M_{k,l}^T) + \\sum_{k=1}^{K} \\sum_{l=1}^{L} \\mathrm{Tr}(u_k u_k^T v_l^T v_l)+Î»âˆ‘k=1Kpkâˆ¥ukâˆ¥2+Î¼âˆ‘l=1Lqlâˆ¥vlâˆ¥2+ \\lambda \\sum_{k=1}^{K} \\sqrt{p_k} \\|u_k\\|_2 + \\mu \\sum_{l=1}^{L} \\sqrt{q_l} \\|v_l\\|_2 hâˆˆ{1,...,H}h \\\\{1, ..., H\\} : u=argminuâˆ‘k=1Kâˆ‘l=1LTr(ukukTvlTvl)âˆ’2âˆ‘k=1Kâˆ‘l=1LTr(ukvlTMk,lT)+Î»âˆ‘k=1Kpkâˆ¥ukâˆ¥2=argminuf(uk)âˆ’2g(uk)+Î»âˆ‘k=1Kpkâˆ¥ukâˆ¥2\\begin{align*} u &= \\operatorname*{argmin}_{u} \\sum_{k=1}^{K} \\sum_{l=1}^{L} \\mathrm{Tr}(u_k u_k^T v_l^T v_l) -2 \\sum_{k=1}^{K} \\sum_{l=1}^{L} \\mathrm{Tr}(u_k v_l^T M_{k,l}^T) + \\lambda \\sum_{k=1}^{K} \\sqrt{p_k} \\|u_k\\|_2 \\\\ &= \\operatorname*{argmin}_{u} f(u_k) - 2g(u_k) + \\lambda \\sum_{k=1}^{K} \\sqrt{p_k} \\|u_k\\|_2 \\end{align*} : f(uk)=âˆ‘k=1Kâˆ‘l=1LTr(ukukTvlTvl)=âˆ‘k=1Kâˆ‘l=1LvlTvlTr(ukukT)=âˆ‘k=1KTr(ukukT)=âˆ‘k=1Kâˆ¥ukâˆ¥2=âˆ¥uâˆ¥2\\begin{align*} f(u_k) &= \\sum_{k=1}^{K} \\sum_{l=1}^{L} \\mathrm{Tr}(u_k u_k^T v_l^T v_l) = \\sum_{k=1}^{K} \\sum_{l=1}^{L} v_l^T v_l \\, \\mathrm{Tr}(u_k u_k^T) \\\\ &= \\sum_{k=1}^{K} \\mathrm{Tr}(u_k u_k^T) = \\sum_{k=1}^{K} \\|u_k\\|^2 = \\|u\\|^2 \\end{align*} (âˆ¥v(hâˆ’1)âˆ¥=1\\|v^{(h-1)}\\| = 1) g(uk)=âˆ‘k=1Kâˆ‘l=1LTr(ukvlTMk,lT)=âˆ‘k=1KTr(ukâˆ‘l=1LvlTMk,lT)=âˆ‘k=1KTr(ukvTMkT)=Tr(uvTMT)\\begin{align*} g(u_k) &= \\sum_{k=1}^{K} \\sum_{l=1}^{L} \\mathrm{Tr}(u_k v_l^T M_{k,l}^T) = \\sum_{k=1}^{K} \\mathrm{Tr} \\left( u_k \\sum_{l=1}^{L} v_l^T M_{k,l}^T \\right)\\\\ &= \\sum_{k=1}^{K} \\mathrm{Tr}(u_k v^T M_k^T) = \\mathrm{Tr}(u v^T M^T) \\end{align*} Mk=XkTZM_k = X_k^T Z Hence : u=argminukâˆ¥uâˆ¥2âˆ’2Tr(uvTMT)+Î»âˆ‘k=1Kpkâˆ¥ukâˆ¥2â‡”uk=argminukâˆ¥ukâˆ¥2âˆ’2Tr(ukvTMkT)+Î»pkâˆ¥ukâˆ¥2\\begin{align*} u &= \\operatorname*{argmin}_{u_k} \\|u\\|^2 - 2\\,\\mathrm{Tr}(u v^T M^T) + \\lambda \\sum_{k=1}^{K} \\sqrt{p_k} \\|u_k\\|_2 \\\\ \\iff u_k &= \\operatorname*{argmin}_{u_k} \\|u_k\\|^2 - 2\\,\\mathrm{Tr}(u_k v^T M_k^T) + \\lambda \\sqrt{p_k} \\|u_k\\|_2 \\end{align*} way : vl=argminvlâˆ¥vlâˆ¥2âˆ’2Tr(vluTMl)+Î¼qlâˆ¥vlâˆ¥2\\begin{align*} v_l &= \\operatorname*{argmin}_{v_l} \\|v_l\\|^2 - 2\\mathrm{Tr}(v_l u^T M_l) + \\mu \\sqrt{q_l} \\|v_l\\|_2 \\end{align*} Ml=XlTZM_l = X_l^T Z.","code":""},{"path":"/articles/gPLS_method.html","id":"problem-solving","dir":"Articles","previous_headings":"","what":"Problem solving","title":"gPLS method","text":"values uku_k vlv_l must cancel gradient. therefore solve: dduk(âˆ¥ukâˆ¥2âˆ’2Ã—Tr(ukvTMkT)+Î»pkâˆ¥ukâˆ¥2)=0 \\frac{d}{du_k} \\left( \\|u_k\\|^2 - 2 \\times \\mathrm{Tr}(u_k v^T M_k^T) + \\lambda \\sqrt{p_k} \\|u_k\\|_2 \\right) = 0 â‡”ddukâˆ¥ukâˆ¥2âˆ’2ddukTr(ukvTMkT)+Î»pkddukâˆ¥ukâˆ¥2=0 \\iff \\frac{d}{du_k} \\|u_k\\|^2 - 2 \\frac{d}{du_k} \\mathrm{Tr}(u_k v^T M_k^T) + \\lambda \\sqrt{p_k} \\frac{d}{du_k} \\|u_k\\|_2 = 0 â‡”2ukâˆ’2Mkv+Î»pkukâˆ¥ukâˆ¥2=0 \\iff 2u_k - 2M_k v + \\lambda \\sqrt{p_k} \\frac{u_k}{\\|u_k\\|_2} = 0 â‡”2uk+Î»pkukâˆ¥ukâˆ¥2=2Mkv \\iff 2u_k + \\lambda \\sqrt{p_k} \\frac{u_k}{\\|u_k\\|_2} = 2M_k v ...... â‡”uk=MkvÃ—(1âˆ’Î»pk2âˆ¥Mkvâˆ¥) \\iff u_k = M_k v \\times \\left(1 - \\frac{\\lambda \\sqrt{p_k}}{2 \\|M_k v\\|} \\right) way, also find : vl=MlTuÃ—(1âˆ’Î¼ql2âˆ¥MlTuâˆ¥) v_l = M_l^T u \\times \\left(1 - \\frac{\\mu \\sqrt{q_l}}{2 \\|M_l^T u\\|} \\right)","code":""},{"path":"/articles/gPLS_method.html","id":"convergence-algorithm","dir":"Articles","previous_headings":"","what":"Convergence algorithm","title":"gPLS method","text":"solve minimization problem, first perform SVD decomposition (first column matrices UU VV). vectors found yet solutions. must therefore apply, component hh, group kk, group ll, convergence algorithm. way , must calculate: uÌƒnew,k(h)=Mk(h)vold(h)(1âˆ’Î»hpk2âˆ¥Mk(h)vold(h)âˆ¥) \\tilde{u}^{(h)}_{new,k} = M_k^{(h)} v^{(h)}_{old} \\left(1 - \\frac{\\lambda_h \\sqrt{p_k}}{2 \\|M_k^{(h)} v^{(h)}_{old}\\|} \\right) , calculated last uÌƒnew,k(h)\\tilde{u}^{(h)}_{new,k} : uÌƒnew(h)=(uÌƒnew,1(h)...uÌƒnew,pk(h)) \\tilde{u}^{(h)}_{new} = \\begin{pmatrix}     \\tilde{u}^{(h)}_{new,1} \\\\     ...\\\\     \\tilde{u}^{(h)}_{new,p_k} \\end{pmatrix} way, also find : vÌƒnew,l(h)=Ml(h)Tuold(h)(1âˆ’Î¼hql2âˆ¥Ml(h)Tuold(h)âˆ¥)\\tilde{v}^{(h)}_{new,l} = M_l^{(h)T} u^{(h)}_{old} \\left(1 - \\frac{\\mu_h \\sqrt{q_l}}{2 \\|M_l^{(h)T} u^{(h)}_{old}\\|} \\right) , calculated last vÌƒnew,l(h)\\tilde{v}^{(h)}_{new,l} : vÌƒnew(h)=(vÌƒnew,1(h)...vÌƒnew,ql(h)) \\tilde{v}^{(h)}_{new} = \\begin{pmatrix}     \\tilde{v}^{(h)}_{new,1} \\\\     ...\\\\     \\tilde{v}^{(h)}_{new,q_l} \\end{pmatrix} Finally, solutions found must standardized : unew(h)=uÌƒnew(h)âˆ¥uÌƒnew(h)âˆ¥2vnew(h)=vÌƒnew(h)âˆ¥vÌƒnew(h)âˆ¥2\\begin{align*} u^{(h)}_{new} &= \\frac{\\tilde{u}^{(h)}_{new}}{\\|\\tilde{u}^{(h)}_{new}\\|_2} \\\\ v^{(h)}_{new} &= \\frac{\\tilde{v}^{(h)}_{new}}{\\|\\tilde{v}^{(h)}_{new}\\|_2} \\end{align*} select unew(h)u^{(h)}_{new} vnew(h)v^{(h)}_{new} respectively |unew(h)âˆ’uold(h)|<eps|u^{(h)}_{new} - u^{(h)}_{old}| < eps |vnew(h)âˆ’vold(h)|<eps|v^{(h)}_{new} - v^{(h)}_{old}| < eps |unew(h)âˆ’uold(h)|>eps|u^{(h)}_{new} - u^{(h)}_{old}| > eps |vnew(h)âˆ’vold(h)|>eps|v^{(h)}_{new} - v^{(h)}_{old}| > eps: assign following values: uold(h)=unew(h)u^{(h)}_{old} = u^{(h)}_{new} vold(h)=vnew(h)v^{(h)}_{old} = v^{(h)}_{new} repeat one loop.","code":""},{"path":"/articles/PLSDA_method.html","id":"what-is-pls-da","dir":"Articles","previous_headings":"","what":"What is PLS-DA ?","title":"PLS-DA method","text":"ending acronym Discriminant Analysis, PLS-DA turns classification method using dimension reduction discriminant analysis. often shows good results possible discriminate categories (also called classes) variables YY based variables XX. time, consider dataset (X,Y)(X,Y) variable YY categorical composed kk classes. YY recoded one-hot model represented matrix size nÃ—kn \\times k; YÌƒ=(YÌƒ1|YÌƒ2|...|YÌƒk) \\tilde{Y} = (\\tilde{Y}_1 | \\tilde{Y}_2 | ... | \\tilde{Y}_k) latent variables t(h)t^{(h)} s(h)s^{(h)} determined using procedure PLS, time using matrix YÌƒ\\tilde{Y} one-hot encoding. therefore particular s(h)=YÌƒ(h)u(h)s^{(h)} = \\tilde{Y}^{(h)}u^{(h)}, âˆ€hâˆˆ1,...,H\\forall h \\1,...,H.","code":""},{"path":"/articles/PLSDA_method.html","id":"how-to-make-predictions","dir":"Articles","previous_headings":"","what":"How to make predictions ?","title":"PLS-DA method","text":"H-component predictions given YÌƒÌ‚new=XnewU(CTU)âˆ’1B\\hat{\\tilde{Y}}_{new} = X_{new}U(C^TU)^{-1}B : â†’ UU pÃ—Hp \\times H matrix component weights XX â†’ CC pÃ—Hp \\times H matrix coefficients XX TT â†’ BB HÃ—kH \\times k matrix coefficients TT YY therefore find columns CC following expression: ch=X(h)TththTth c_h = \\frac{X^{(h)T} t_h}{t^T_h t_h} also expression : B=(TTT)âˆ’1TTYÌƒ B = (T^TT)^{-1}T^T\\tilde{Y} RemarkRemark can acknowledge expression YÌƒÌ‚new=XnewU(CTU)âˆ’1B\\hat{\\tilde{Y}}_{new} = X_{new}U(C^TU)^{-1}B PLS method except YY YÌ‚new\\hat{Y}_{new} one-hot encoded. Since matrix YÌƒÌ‚new\\hat{\\tilde{Y}}_{new} size nnewÃ—kn_{new} \\times k, assignment classes YÌ‚new\\hat{Y}_{new} therefore done using distance calculation method called ; three : Maximum distance Centroid distance Mahalanobis distance","code":""},{"path":"/articles/PLSDA_method.html","id":"maximum-distance","dir":"Articles","previous_headings":"How to make predictions ?","what":"Maximum distance","title":"PLS-DA method","text":"distance maximum simplest. start YÌƒÌ‚new\\hat{\\tilde{Y}}_{new} values represent scores individual class. class predict therefore one highest score. words, : cÌ‚=argmaxl(YÌƒÌ‚new),lâˆ€âˆˆ1,...,n\\hat{c}_i = \\operatorname*{argmax}_l (\\hat{\\tilde{Y}}_{new})_{,l} \\forall \\{1,...,n} ii row index ll column index (YÌƒÌ‚new),l(\\hat{\\tilde{Y}}_{new})_{,l}.","code":""},{"path":"/articles/PLSDA_method.html","id":"centroÃ¯d-and-mahalanobis-distance","dir":"Articles","previous_headings":"How to make predictions ?","what":"CentroÃ¯d and Mahalanobis distance","title":"PLS-DA method","text":"centroid Mahalanobis distances based trained latent variable tÌ‚(h)\\hat{t}^{(h)} predicted latent variables tÌ‚new(1)=Xnew(1)u\\hat{t}^{(1)}_{new} = X^{(1)}_{new}u tÌ‚new(h)=Xnew(h)u\\hat{t}^{(h)}_{new} = X^{(h)}_{new}u. Concretely, involves calculating Euclidean distance center gravity t(h)t^{(h)} predicted latent variables. class distance smallest selected. TcT_c denotes portion matrix TT containing individuals class cc. predicted component matrix TÌ‚=TÌ‚new=XnewU(CTU)âˆ’1\\hat{T} = \\hat{T}_{new} = X_{new}U(C^TU)^{-1} also denoted.","code":""},{"path":"/articles/PLSDA_method.html","id":"centroÃ¯d-distance","dir":"Articles","previous_headings":"How to make predictions ?","what":"CentroÃ¯d distance","title":"PLS-DA method","text":"therefore calculate TcT_c calculate centroid vector Gc=1ncğŸ™ncTTc=(1nc1nc...1nc)TcGc = \\frac{1}{n_c}\\mathbb{1}_{n_c}^TT_c = (\\frac{1}{n_c} \\frac{1}{n_c}...\\frac{1}{n_c})T_c ncn_c frequency class cc. find matrix GG composed row vectors GcG_c row vector TÌ‚new\\hat{T}_{new}, calculate Euclidean distance row vectors GG. Finally, assign class cc distance smallest. words: YÌ‚=argmin1â‰¤câ‰¤kdist(TÌ‚,Gc)=argmin1â‰¤câ‰¤kâˆ‘h=1H(TÌ‚,hâˆ’Gc,h)2\\hat{Y}_i = \\operatorname*{argmin}_{1\\leq c \\leq k} dist(\\hat{T}_i,G_c) = \\operatorname*{argmin}_{1\\leq c \\leq k} \\sqrt{\\sum_{h=1}^{H} (\\hat{T}_{,h} - G_{c,h})^2} âˆ€âˆˆ1,...,n\\forall \\{1,...,n}","code":""},{"path":"/articles/PLSDA_method.html","id":"mahalanobis-distance","dir":"Articles","previous_headings":"How to make predictions ?","what":"Mahalanobis distance","title":"PLS-DA method","text":"distance, calculate matrix GG , row vector TÌ‚\\hat{T} calculate Mahalanobis distance row vectors GG. YÌ‚=argmin1â‰¤câ‰¤kdist(TÌ‚,Gc)=argmin1â‰¤câ‰¤k(TÌ‚âˆ’Gc)TCovâˆ’1(TÌ‚âˆ’Gc)\\hat{Y}_i = \\operatorname*{argmin}_{1\\leq c \\leq k} dist(\\hat{T}_i,G_c) = \\operatorname*{argmin}_{1\\leq c \\leq k} \\sqrt{(\\hat{T}_i - G_c)^TCov^{-1}(\\hat{T}_i - G_c)} âˆ€âˆˆ1,...,n\\forall \\{1,...,n} Covâˆ’1Cov^{-1} inverse covariance matrix CovCov applied vector (TÌ‚âˆ’Gc)(\\hat{T}_i - G_c).","code":""},{"path":"/articles/PLSDA_performance.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"PLS-DA performance","text":"page presents application PLSDA performance assessment. PLS method quite particular method : several predictions according number components selected model. PLSDA. goal almost choose best number components PLS regression order compute best possible predictions. , use two datasets: one dataset ten predictor variables X=(X1,X2,...,X10)X = (X1,X2,...,X10) two classes. dataset forty predictor variables X=(X1,X2,...,X40)X = (X1,X2,...,X40) three classes. p=40>n=30p = 40 > n = 30, dataset approches realist conditions PLS training. access predefined functions sgPLSdevelop package manipulate datasets, run lines : continuation article, show PLS-DA performance assessment mean error rate using leave-one-cross-validation (LOOCV), 10-fold CV 5-fold CV. perf.PLSda function allow compute error rate application case. NB : three possible distances computing error rate : ğ‘šğ‘ğ‘¥ğ‘–ğ‘šğ‘¢ğ‘š\\textit{maximum} distance, ğ‘ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘œÃ¯ğ‘‘ğ‘ \\textit{centroÃ¯ds} distance ğ‘€ğ‘hğ‘ğ‘™ğ‘ğ‘›ğ‘œğ‘ğ‘–ğ‘ \\textit{Mahalanobis} distance. default, function uses ğ‘šğ‘ğ‘¥ğ‘–ğ‘šğ‘¢ğ‘š\\textit{maximum} distance. complicated cases, advisable choose ğ‘€ğ‘hğ‘ğ‘™ğ‘ğ‘›ğ‘œğ‘ğ‘–ğ‘ \\textit{Mahalanobis} distance gives accurate results.","code":"library(sgPLSdevelop)  data1 <- data.cl.create(p = 10) # 2 classes by default data2 <- data.cl.create(n = 30, p = 40, classes = 3)  ncomp.max <- 8  # First model X <- data1$X Y <- data1$Y.f model1 <- PLSda(X,Y, ncomp = ncomp.max)  # Second model X <- data2$X Y <- data2$Y.f model2 <- PLSda(X,Y, ncomp = ncomp.max)"},{"path":"/articles/PLSDA_performance.html","id":"leave-one-out-cv","dir":"Articles","previous_headings":"","what":"Leave-one-out CV","title":"PLS-DA performance","text":"Leave-one-CV (LOOCV) builts nn models test set composed single row (never row model).","code":""},{"path":"/articles/PLSDA_performance.html","id":"first-model","dir":"Articles","previous_headings":"Leave-one-out CV","what":"First model","title":"PLS-DA performance","text":"Letâ€™s start first model. Error rate first model LOOCV perf.PLSda function gives us optimal number components equal H=H = 1, therefore suggest select 1 component(s) first model.","code":"perf.res1 <- perf.PLSda(model1, validation = \"loo\", progressBar = FALSE) h.best <- perf.res1$h.best"},{"path":"/articles/PLSDA_performance.html","id":"second-model","dir":"Articles","previous_headings":"Leave-one-out CV","what":"Second model","title":"PLS-DA performance","text":"Letâ€™s continue second model. Error rate second model LOOCV perf.PLSda function gives us optimal number components equal H=H = 1, therefore suggest select 1 component(s) first model. LOOCV efficient way assess performance requires large computing capacity. K-fold CV (create K blocks) reduces number models built also execution time.","code":"perf.res2 <- perf.PLSda(model2, validation = \"loo\", progressBar = FALSE) h.best <- perf.res2$h.best"},{"path":"/articles/PLSDA_performance.html","id":"fold-cv","dir":"Articles","previous_headings":"","what":"10-fold CV","title":"PLS-DA performance","text":"10-fold CV builts 10 models. case, model, test set composed 4 observations first model 3 second.","code":""},{"path":"/articles/PLSDA_performance.html","id":"first-model-1","dir":"Articles","previous_headings":"10-fold CV","what":"First model","title":"PLS-DA performance","text":"Error rate first model 10-fold CV perf.PLSda function gives us optimal number components equal H=H = 1.","code":"perf.res1 <- perf.PLSda(model1, folds = 10, progressBar = FALSE) h.best <- perf.res1$h.best"},{"path":"/articles/PLSDA_performance.html","id":"second-model-1","dir":"Articles","previous_headings":"10-fold CV","what":"Second model","title":"PLS-DA performance","text":"Error rate second model 10-fold CV perf.PLSda function gives us optimal number components equal H=H = 1.","code":"perf.res2 <- perf.PLSda(model2, folds = 10, progressBar = FALSE) h.best <- perf.res2$h.best"},{"path":"/articles/PLSDA_performance.html","id":"fold-cv-1","dir":"Articles","previous_headings":"","what":"5-fold CV","title":"PLS-DA performance","text":"5-fold CV builts 5 models. case, model, test set composed 8 observations first model 6 second.","code":""},{"path":"/articles/PLSDA_performance.html","id":"first-model-2","dir":"Articles","previous_headings":"5-fold CV","what":"First model","title":"PLS-DA performance","text":"Error rate first model 5-fold CV perf.PLSda function gives us optimal number components equal H=H = 1.","code":"perf.res1 <- perf.PLSda(model1, folds = 5, progressBar = FALSE) h.best <- perf.res1$h.best"},{"path":"/articles/PLSDA_performance.html","id":"second-model-2","dir":"Articles","previous_headings":"5-fold CV","what":"Second model","title":"PLS-DA performance","text":"Error rate second model 5-fold CV perf.PLSda function gives us optimal number components equal H=H = 1.","code":"perf.res2 <- perf.PLSda(model1, folds = 5, progressBar = FALSE) h.best <- perf.res2$h.best"},{"path":"/articles/PLS_method.html","id":"what-is-pls-regression","dir":"Articles","previous_headings":"","what":"What is PLS regression ?","title":"PLS regression method","text":"PLS dimension reduction method equivalent PCA, creation HH new components, remains supervised method. PLS can adapted regression (case referred PLS1), classification (PLS-DA), can also multivariate (PLS2). consider dataset divided two centered standardized datasets XX YY sizes nÃ—pn \\times p nÃ—qn \\times q respectively. HH new components created denoted t1t_1, t2t_2, â€¦, tHt_H XX dataset u1u_1, u2u_2, â€¦, uHu_H YY dataset. linear combination variables X1X_1, â€¦, XpX_p well variables Y1Y_1, â€¦, YqY_q, respectively.","code":"library(sgPLSdevelop) data <- data.create(n = 50, q = 5) X <- data$X Y <- data$Y"},{"path":"/articles/PLS_method.html","id":"how-to-compute-the-first-component","dir":"Articles","previous_headings":"","what":"How to compute the first component ?","title":"PLS regression method","text":"expressions first component XX YY. t1=âˆ‘j=1pujXj=Xut_1 = \\sum_{j=1}^{p} u_j X_j = Xu s1=âˆ‘j=1qvjYj=Yvs_1 = \\sum_{j=1}^{q} v_j Y_j = Yv u1u_1,â€¦,upu_p v1v_1,â€¦,vqv_q associated weights. Theses weights therefore obtained maximizing covariance t1t_1 s1s_1, , determining: argmaxu,vCov(Xu,Yv)\\operatorname*{argmax}_{u,v} \\; \\operatorname{Cov}(Xu, Yv) constraint u=v=1u = v = 1. , one must decompose matrix product M=XTYM = X^T Y singular values: M=UÎ”VTM = U \\Delta V^T. Î”\\Delta diagonal matrix containing singular values MM, .e., square roots eigenvalues MMTMM^T. UU matrix containing eigenvectors MMTMM^T. VV matrix containing eigenvectors MTMM^TM. vectors uu vv maximize covariance respectively first columns matrices UU VV.","code":"svd <- svd(t(X)%*%Y) u <- svd$u[,1] v <- svd$v[,1] t <- X%*%u s <- Y%*%v"},{"path":"/articles/PLS_method.html","id":"how-to-compute-all-components","dir":"Articles","previous_headings":"","what":"How to compute all components ?","title":"PLS regression method","text":"second component onward, hh becomes greater 11, new matrices new weights defined; therefore necessary introduce new notations.âˆ€hâˆˆ1,...,H\\forall h \\1,...,H, expressions become: th=âˆ‘j=1puj(h)Xj(h)=X(h)u(h) t_{h} = \\sum_{j=1}^{p} u^{(h)}_j X^{(h)}_{j} = X^{(h)}u^{(h)} sh=âˆ‘j=1qvj(h)Yj(h)=Y(h)v(h) s_{h} = \\sum_{j=1}^{q} v^{(h)}_j Y^{(h)}_{j} = Y^{(h)}v^{(h)} thus define matrices X(1),â€¦,X(h)X^{(1)}, \\ldots, X^{(h)}, Y(1),â€¦,Y(h)Y^{(1)}, \\ldots, Y^{(h)}, well new weight vectors : X(1)=XX^{(1)} = X X(h+1)=X(h)âˆ’thchTX^{(h+1)} = X^{(h)} - t_{h} c^T_{h} u(1)=uu^{(1)} = u Y(1)=YY^{(1)} = Y Y(h+1)=Y(h)âˆ’thdhTY^{(h+1)} = Y^{(h)} - t_{h} d^T_{h} v(1)=vv^{(1)} = v weights thus obtained maximizing covariance tht_h shs_h, , determining: argmaxu(h),v(h)Cov(X(h)u(h),Y(h)v(h)) \\operatorname*{argmax}_{u^{(h)},v^{(h)}} \\; \\operatorname{Cov}(X^{(h)}u^{(h)}, Y^{(h)}v^{(h)}) constraint âˆ¥u(h)âˆ¥=âˆ¥v(h)âˆ¥=1\\| u^{(h)} \\| = \\| v^{(h)} \\| = 1. , matrix product decomposed : M(h)=X(h)TY(h)=U(h)Î”V(h)T M^{(h)} = X^{(h)T}Y^{(h)} = U^{(h)} \\Delta V^{(h)T}","code":""},{"path":"/articles/PLS_method.html","id":"computation-of-xh-and-yh","dir":"Articles","previous_headings":"How to compute all components ?","what":"Computation of X(h)X^{(h)} and Y(h)Y^{(h)}","title":"PLS regression method","text":"determine X(h)X^{(h)} Y(h)Y^{(h)}, first compute vectors châˆ’1c_{h-1} dhâˆ’1d_{h-1} using formulas: ch=X(h)TththTth c_h = \\frac{X^{(h)T} t_h}{t^T_h t_h} dh=Y(h)TththTth d_h = \\frac{Y^{(h)T} t_h}{t^T_h t_h} Remark: case PLS1, since method univariate YY, u=1u = 1 H=1H = 1. canonical mode PLS, matrix deflation expressions : Y(h)=Y(hâˆ’1)âˆ’thâˆ’1dhâˆ’1T=Y(k)âˆ’tkdkT Y^{(h)} = Y^{(h-1)} - t_{h-1} d^T_{h-1} = Y^{(k)} - t_k d^T_k eh=Y(h)TshshTsh e_h = \\frac{Y^{(h)T} s_h}{s^T_h s_h} Theses vectors matrices (except deflated matrices X(h)X^{(h)} Y(h)Y^{(h)} h>1h>1) can found PLS function.","code":"model <- PLS(X,Y,ncomp = 10,mode = \"regression\") mat.t <- model$variates$X mat.s <- model$variates$Y mat.c <- model$mat.c mat.d <- model$mat.d mat.e <- model$mat.e # \"NA\" printed because of regression mode"},{"path":"/articles/PLS_method.html","id":"how-to-make-predictions","dir":"Articles","previous_headings":"","what":"How to make predictions ?","title":"PLS regression method","text":"Predictions given : YÌ‚new=XnewBâ€²=XnewU(CTU)âˆ’1B\\hat{Y}_{new} = X_{new}B' = X_{new}U(C^TU)^{-1}B : â†’ U=(u1|u2|...|uH)U = (u_1 | u_2 | ... | u_H) â†’ C=(c1|c2|...|cH)C = (c_1 | c_2 | ... | c_H), pÃ—Hp \\times H coefficients matrix XX TT â†’ B=(TTT)âˆ’1TTYB = (T^TT)^{-1}T^TY, HÃ—kH \\times k coefficients matrix TT YY. hence find CC columns following expression : ch=X(h)TththTthc_h = \\frac{X^{(h)T} t_h}{t^T_h t_h} also possible make predictions new components calculating : TÌ‚new=XnewU(CTU)âˆ’1\\hat{T}_{new} = X_{new}U(C^TU)^{-1} also relation : YÌ‚new=TÌ‚newB\\hat{Y}_{new} = \\hat{T}_{new}B NB : newdata XnewX_{new} scaled according XX attributes, unscale YÌ‚new\\hat{Y}_{new} using YY attributes. XX YY parts training set context.","code":""},{"path":"/articles/PLS_performance.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"PLS performance","text":"page presents application PLS performance assessment. PLS method quite particular method : several predictions according number components selected model. goal almost choose best number component PLS regression order compute best possible predictions. , use three datasets: one dataset one response variable YY. dataset four response variables Y=(Y1,Y2,Y3,Y4)Y = (Y1,Y2,Y3,Y4). last dataset contains real data NIR spectra. access predefined functions sgPLSdevelop package manipulate datasets, run lines : two first datasets, population set n=40n = 40 default, close actual conditions. Letâ€™s also notice , average, response YY linear combination predictors XX. Indeed, function includes matrix product Y=XB+EY = XB + E BB weight matrix EE matrix gaussian noise. linearity condition important order good performance model, PLS method using linearity combinaison. Now, â€™s time train PLS model dataset built imported. continuation article, show PLS performance assessment using Q2Q_2 criterion MSEPMSEP criterion.","code":"library(sgPLSdevelop) library(pls) library(mixOmics) #NOUVEAU!  data1 <- data.create(p = 10, list = TRUE) data2 <- data.create(p = 10, q = 4, list = TRUE) data(yarn) data3 <- yarn ## [1] \"First dataset dimensions : 40 x 11\" ## [1] \"Second dataset dimensions : 40 x 14\" ## [1] \"Yarn dataset dimensions : 28 x 3\" ncomp.max <- 8  # First model X <- data1$X Y <- data1$Y model1 <- PLS(X,Y,mode = \"regression\", ncomp = ncomp.max) model.mix1 <- pls(X,Y,mode = \"regression\", ncomp = ncomp.max) #NOUVEAU!  # Second model X <- data2$X Y <- data2$Y model2 <- PLS(X,Y,mode = \"regression\", ncomp = ncomp.max) model.mix2 <- pls(X,Y,mode = \"regression\", ncomp = ncomp.max) #NOUVEAU!  # Third model X <- data3$NIR Y <- data3$density model3 <- PLS(X,Y,mode = \"regression\", ncomp = ncomp.max) model.mix3 <- pls(X,Y,mode = \"regression\", ncomp = ncomp.max) #NOUVEAU!"},{"path":"/articles/PLS_performance.html","id":"q2-criterion","dir":"Articles","previous_headings":"","what":"Q2 criterion","title":"PLS performance","text":"QÂ² indicator ? Q2Q^2 assessment indicator PLS models; new component hh, new matrix Y(h)Y^{(h)} obtained deflation compared corresponding prediction matrix YÌ‚(h)\\hat{Y}^{(h)}. Q2 therefore takes comparison account. Q2Q^2 value close 11 indicates good performance. compute figure, must compute two indicators : RSSRSS PRESSPRESS. RSSh=âˆ‘=1nâˆ‘j=1q(Y(h)âˆ’YÌ‚)2=âˆ‘=1nâˆ‘j=1q(Yi,j(h+1))2RSS_h = \\sum_{=1}^{n} \\sum_{j=1}^{q} (Y^{(h)} - \\hat{Y})^2 =  \\sum_{=1}^{n} \\sum_{j=1}^{q} (Y^{(h+1)}_{,j})^2 PRESSh=âˆ‘âˆˆtestnâˆ‘j=1q(Yi,j(h)âˆ’YÌ‚,j)2=âˆ‘âˆˆtestnâˆ‘j=1q(Yi,j(h+1))2PRESS_h = \\sum_{\\test}^{n} \\sum_{j=1}^{q} (Y_{,j}^{(h)} - \\hat{Y}_{,j})^2 =  \\sum_{\\test}^{n} \\sum_{j=1}^{q} (Y^{(h+1)}_{,j})^2 , Q2Q^2 defined formula : Qh2=1âˆ’PRESShRSShâˆ’1Q^2_h = 1 - \\frac{PRESS_h}{RSS_{h-1}} use QÂ² ? compare value criterion certain limit ll ; limit conventionally equal 1âˆ’0.952=0.09751-0.95^2 = 0.0975. long inequality Qh2â‰¥lQ^2_h \\geq l, keep following iteration ; therefore stop Qh2<lQ^2_h < l. Using QÂ² R Q2Q^2 function, available named q2.PLS(), takes three parameters : model (â€œplsâ€ class object) mode : must choose â€œregressionâ€ â€œcanonicalâ€ number maximal components","code":""},{"path":"/articles/PLS_performance.html","id":"first-model-q2","dir":"Articles","previous_headings":"Q2 criterion","what":"First model Q2","title":"PLS performance","text":"Q2 values first model q2.pls function gives us optimal number components select equal H=H = 2, therefore suggest select 2 components first model.","code":"par(mfrow = c(1,2)) q2.res1 <- q2.PLS(model1) h.best <- q2.res1$h.best q2.PLS(model1, ncomp = min(h.best+1, ncomp.max))$q2 ## [1]  0.7920627  0.3108891 -1.8832754"},{"path":"/articles/PLS_performance.html","id":"second-model-q2","dir":"Articles","previous_headings":"Q2 criterion","what":"Second model Q2","title":"PLS performance","text":"Q2 values second model q2.pls function gives us optimal number components select equal H=H = 1.","code":"par(mfrow = c(1,2)) q2.res2 <- q2.PLS(model2) h.best <- q2.res2$h.best q2.PLS(model2, ncomp = min(h.best+1, ncomp.max))$q2 ## [1]  0.350568774 -0.002592037"},{"path":"/articles/PLS_performance.html","id":"third-model-q2","dir":"Articles","previous_headings":"Q2 criterion","what":"Third model Q2","title":"PLS performance","text":"Q2 values third model q2.pls function gives us optimal number components select equal H=H = 2.","code":"par(mfrow = c(1,2)) q2.res3 <- q2.PLS(model3) h.best <- q2.res3$h.best q2.PLS(model3, ncomp = min(h.best+1, ncomp.max))$q2 ## [1]  0.8954745  0.5508883 -3.7792077"},{"path":[]},{"path":"/articles/PLS_performance.html","id":"msep-criterion","dir":"Articles","previous_headings":"","what":"MSEP criterion","title":"PLS performance","text":"way assess model performance consists using MSEPMSEP criterion. MSEPMSEP computed follow : MSEP=1nqâˆ‘=1nâˆ‘j=1q(Yi,jâˆ’YÌ‚,j)2MSEP = \\frac{1}{nq} \\sum_{=1}^{n} \\sum_{j=1}^{q} (Y_{,j} - \\hat{Y}_{,j})^2 , select hbesth_{best} number components corresponds lower MSEPMSEP value.","code":""},{"path":"/articles/PLS_performance.html","id":"first-model-msep","dir":"Articles","previous_headings":"MSEP criterion","what":"First model MSEP","title":"PLS performance","text":"MSEP first model msep.PLS function gives us optimal number components equal H=H = 7, therefore suggest select 7 components first model.","code":"msep.res1 <- msep.PLS(model1) h.best <- msep.res1$h.best msep.best <- msep.res1$MSEP[h.best]"},{"path":"/articles/PLS_performance.html","id":"second-model-msep","dir":"Articles","previous_headings":"MSEP criterion","what":"Second model MSEP","title":"PLS performance","text":"MSEP second model msep.PLS function gives us optimal number components equal H=H = 8.","code":"msep.res2 <- msep.PLS(model2) h.best <- msep.res2$h.best msep.best <- msep.res2$MSEP[h.best]"},{"path":"/articles/PLS_performance.html","id":"third-model-msep","dir":"Articles","previous_headings":"MSEP criterion","what":"Third model MSEP","title":"PLS performance","text":"MSEP third model msep.PLS function gives us optimal number components equal H=H = 8.","code":"msep.res3 <- msep.PLS(model3) h.best <- msep.res3$h.best msep.best <- msep.res3$MSEP[h.best]"},{"path":"/articles/PLS_performance00.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"PLS performance","text":"page presents application PLS performance assessment. PLS method quite particular method : several predictions according number components selected model. goal almost choose best number component PLS regression order compute best possible predictions. , use three datasets: one dataset one response variable YY. dataset four response variables Y=(Y1,Y2,Y3,Y4)Y = (Y1,Y2,Y3,Y4). last dataset contains real data NIR spectra. access predefined functions sgPLSdevelop package manipulate datasets, run lines : two first datasets, population set n=40n = 40 default, close actual conditions. Letâ€™s also notice , average, response YY linear combination predictors XX. Indeed, function includes matrix product Y=XB+EY = XB + E BB weight matrix EE matrix gaussian noise. linearity condition important order good performance model, PLS method using linearity combinaison. Now, â€™s time train PLS model dataset built imported. continuation article, show PLS performance assessment using Q2Q_2 criterion MSEPMSEP criterion.","code":"library(sgPLSdevelop) library(pls)  data1 <- data.create(p = 10, list = TRUE) data2 <- data.create(p = 10, q = 4, list = TRUE) data(yarn) data3 <- yarn ## [1] \"First dataset dimensions : 40 x 11\" ## [1] \"Second dataset dimensions : 40 x 14\" ## [1] \"Yarn dataset dimensions : 28 x 3\" ncomp.max <- 8  # First model X <- data1$X Y <- data1$Y model1 <- PLS(X,Y,mode = \"regression\", ncomp = ncomp.max)  # Second model X <- data2$X Y <- data2$Y model2 <- PLS(X,Y,mode = \"regression\", ncomp = ncomp.max)  # Third model X <- data3$NIR Y <- data3$density model3 <- PLS(X,Y,mode = \"regression\", ncomp = ncomp.max)"},{"path":"/articles/PLS_performance00.html","id":"q2-criterion","dir":"Articles","previous_headings":"","what":"Q2 criterion","title":"PLS performance","text":"QÂ² indicator ? Q2Q^2 assessment indicator PLS models; new component hh, new matrix Y(h)Y^{(h)} obtained deflation compared corresponding prediction matrix YÌ‚(h)\\hat{Y}^{(h)}. Q2 therefore takes comparison account. Q2Q^2 value close 11 indicates good performance. compute figure, must compute two indicators : RSSRSS PRESSPRESS. RSSh=âˆ‘=1nâˆ‘j=1q(Y(h)âˆ’YÌ‚)2=âˆ‘=1nâˆ‘j=1q(Yi,j(h+1))2RSS_h = \\sum_{=1}^{n} \\sum_{j=1}^{q} (Y^{(h)} - \\hat{Y})^2 =  \\sum_{=1}^{n} \\sum_{j=1}^{q} (Y^{(h+1)}_{,j})^2 PRESSh=âˆ‘âˆˆtestnâˆ‘j=1q(Yi,j(h)âˆ’YÌ‚,j)2=âˆ‘âˆˆtestnâˆ‘j=1q(Yi,j(h+1))2PRESS_h = \\sum_{\\test}^{n} \\sum_{j=1}^{q} (Y_{,j}^{(h)} - \\hat{Y}_{,j})^2 =  \\sum_{\\test}^{n} \\sum_{j=1}^{q} (Y^{(h+1)}_{,j})^2 , Q2Q^2 defined formula : Qh2=1âˆ’PRESShRSShâˆ’1Q^2_h = 1 - \\frac{PRESS_h}{RSS_{h-1}} use QÂ² ? compare value criterion certain limit ll ; limit conventionally equal 1âˆ’0.952=0.09751-0.95^2 = 0.0975. long inequality Qh2â‰¥lQ^2_h \\geq l, keep following iteration ; therefore stop Qh2<lQ^2_h < l. Using QÂ² R Q2Q^2 function, available named q2.PLS(), takes three parameters : model (â€œplsâ€ class object) mode : must choose â€œregressionâ€ â€œcanonicalâ€ number maximal components","code":""},{"path":"/articles/PLS_performance00.html","id":"first-model-q2","dir":"Articles","previous_headings":"Q2 criterion","what":"First model Q2","title":"PLS performance","text":"Q2 values first model q2.pls function gives us optimal number components select equal H=H = 2, therefore suggest select 2 components first model.","code":"par(mfrow = c(1,2)) q2.res1 <- perf.PLS.ter(model1, criterion = \"Q2\") h.best <- q2.res1$h.best.q2 perf.PLS.ter(model1, criterion = \"Q2\", ncomp = min(h.best+1, ncomp.max))$q2 ## [1]  0.7920627  0.3108891 -1.8832754"},{"path":"/articles/PLS_performance00.html","id":"second-model-q2","dir":"Articles","previous_headings":"Q2 criterion","what":"Second model Q2","title":"PLS performance","text":"Q2 values second model q2.pls function gives us optimal number components select equal H=H = 1.","code":"par(mfrow = c(1,2)) q2.res2 <- perf.PLS.ter(model2, criterion = \"Q2\") h.best <- q2.res2$h.best.q2 perf.PLS.ter(model2, criterion = \"Q2\", ncomp = min(h.best+1, ncomp.max))$q2 ## [1]  0.350568774 -0.002592037"},{"path":"/articles/PLS_performance00.html","id":"third-model-q2","dir":"Articles","previous_headings":"Q2 criterion","what":"Third model Q2","title":"PLS performance","text":"Q2 values third model q2.pls function gives us optimal number components select equal H=H = 2.","code":"par(mfrow = c(1,2)) q2.res3 <- perf.PLS.ter(model3, criterion = \"Q2\") h.best <- q2.res3$h.best.q2 perf.PLS.ter(model3, criterion = \"Q2\", ncomp = min(h.best+1, ncomp.max))$q2 ## [1]  0.8954745  0.5508883 -3.7792077"},{"path":"/articles/PLS_performance00.html","id":"msep-criterion","dir":"Articles","previous_headings":"","what":"MSEP criterion","title":"PLS performance","text":"way assess model performance consists using MSEPMSEP criterion. MSEPMSEP computed follow : MSEP=1nqâˆ‘=1nâˆ‘j=1q(Yi,jâˆ’YÌ‚,j)2MSEP = \\frac{1}{nq} \\sum_{=1}^{n} \\sum_{j=1}^{q} (Y_{,j} - \\hat{Y}_{,j})^2 , select hbesth_{best} number components corresponds lower MSEPMSEP value.","code":""},{"path":"/articles/PLS_performance00.html","id":"first-model-msep","dir":"Articles","previous_headings":"MSEP criterion","what":"First model MSEP","title":"PLS performance","text":"MSEP first model perf.PLS function gives us optimal number components equal H=H = 7, therefore suggest select 7 components first model.","code":"perf.res1 <- perf.PLS.ter(model1, criterion = \"MSEP\") h.best <- perf.res1$h.best.msep"},{"path":"/articles/PLS_performance00.html","id":"second-model-msep","dir":"Articles","previous_headings":"MSEP criterion","what":"Second model MSEP","title":"PLS performance","text":"MSEP second model perf.PLS function gives us optimal number components equal H=H = 8.","code":"perf.res2 <- perf.PLS.ter(model2, criterion = \"MSEP\") h.best <- perf.res2$h.best.msep"},{"path":"/articles/PLS_performance00.html","id":"third-model-msep","dir":"Articles","previous_headings":"MSEP criterion","what":"Third model MSEP","title":"PLS performance","text":"MSEP third model perf.PLS function gives us optimal number components equal H=H = 8.","code":"perf.res3 <- perf.PLS.ter(model3, criterion = \"MSEP\") h.best <- perf.res3$h.best.msep"},{"path":[]},{"path":"/articles/sPLSDA_performance.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"sPLS-DA performance","text":"page presents application sPLSDA performance assessment. PLS method quite particular method : several predictions according components number selected model. sPLSDA. goal almost choose best number component order compute best possible predictions. , use two datasets: one dataset five predictor variable X=(X1,X2,X3,X4,X5)X = (X1,X2,X3,X4,X5) two classes. dataset forty predictor variables X=(X1,X2,...,X40)X = (X1,X2,...,X40) ans three classes. p>np > n, dataset approches realist conditions PLS training. access predefined functions sgPLSdevelop package manipulate datasets, run lines : Now, â€™s time train PLS model dataset built.","code":"library(sgPLSdevelop)  data1 <- data.cl.create(p = 5, list = TRUE) # 2 classes by default data2 <- data.cl.create(n = 30, p = 40, classes = 3, list = TRUE) ncomp.max <- 5 #keepX <- rep(4,ncompmax)  # First model X <- data1$X Y <- as.factor(data1$Y) model1 <- sPLSda(X,Y, ncomp = ncomp.max)  # Second model X <- data2$X Y <- as.factor(data2$Y) model2 <- sPLSda(X,Y, ncomp = ncomp.max)"},{"path":[]},{"path":"/articles/sPLSDA_performance.html","id":"first-model","dir":"Articles","previous_headings":"Leave-one-out CV","what":"First model","title":"sPLS-DA performance","text":"perf.sPLSda gives us optimal components number equal H=H = 1, therefore suggest select 1 components first model. function also indicates us select 5 variables component.","code":"perf.res1 <- perf.sPLSda(model1) h.best <- perf.res1$h.best keepX.best <- perf.res1$keepX.best"},{"path":"/articles/sPLSDA_performance.html","id":"second-model","dir":"Articles","previous_headings":"Leave-one-out CV","what":"Second model","title":"sPLS-DA performance","text":"perf.sPLSda gives us optimal components number equal H=H = 1, therefore suggest select 1 components first model. function also indicates us select 40 variables component.","code":"perf.res2 <- perf.sPLSda(model2) h.best <- perf.res2$h.best keepX.best <- perf.res2$keepX.best"},{"path":[]},{"path":"/articles/sPLSDA_performance.html","id":"first-model-1","dir":"Articles","previous_headings":"10-fold CV","what":"First model","title":"sPLS-DA performance","text":"perf.sPLSda gives us optimal components number equal H=H = 1, therefore suggest select 1 components first model. function also indicates us select 5 variables component.","code":"perf.res1 <- perf.sPLSda(model1, K = 10) h.best <- perf.res1$h.best keepX.best <- perf.res1$keepX.best"},{"path":"/articles/sPLSDA_performance.html","id":"second-model-1","dir":"Articles","previous_headings":"10-fold CV","what":"Second model","title":"sPLS-DA performance","text":"perf.sPLSda gives us optimal components number equal H=H = 1, therefore suggest select 1 components first model. function also indicates us select 40 variables component.","code":"perf.res2 <- perf.sPLSda(model2, K = 10) h.best <- perf.res2$h.best keepX.best <- perf.res2$keepX.best"},{"path":[]},{"path":"/articles/sPLSDA_performance.html","id":"first-model-2","dir":"Articles","previous_headings":"5-fold CV","what":"First model","title":"sPLS-DA performance","text":"perf.sPLSda gives us optimal components number equal H=H = 1, therefore suggest select 1 components first model. function also indicates us select 5 variables component.","code":"perf.res1 <- perf.sPLSda(model1, K = 5) h.best <- perf.res1$h.best keepX.best <- perf.res1$keepX.best"},{"path":"/articles/sPLSDA_performance.html","id":"second-model-2","dir":"Articles","previous_headings":"5-fold CV","what":"Second model","title":"sPLS-DA performance","text":"perf.sPLSda gives us optimal components number equal H=H = 1, therefore suggest select 1 components first model. function also indicates us select 40 variables component.","code":"perf.res2 <- perf.sPLSda(model2, K = 5) h.best <- perf.res2$h.best keepX.best <- perf.res2$keepX.best"},{"path":"/articles/sPLS_method.html","id":"what-is-spls-method","dir":"Articles","previous_headings":"","what":"What is sPLS method ?","title":"sPLS method","text":"sPLS method (sparse partial least Square) follows principle PLS also involves applying Lasso penalty certain variables order eliminate . set XX, component tht_h, variable Xj(h)X^{(h)}_j eliminated uj(h)=0u^{(h)}_j = 0 . set YY, component shs_h, variable Yj(h)Y^{(h)}_j eliminated vj(h)=0v^{(h)}_j = 0 . h=1h = 1, minimization problem therefore becomes: (u,v)=argminu,vâˆ¥Mâˆ’uvTâˆ¥F2+PÎ»(u)+PÎ¼(v)(u,v) = \\operatorname*{argmin}_{u,v} \\; \\|M - uv^T\\|^2_F + P_\\lambda(u) + P_\\mu(v) Î»\\lambda Î¼\\mu regularization parameters. Î¼\\mu sometimes replaced notation Î»2\\lambda_2. parameters allow us nuance degree sparsity, , rate deleted variables. particular, larger Î»\\lambda Î¼\\mu , variables deleted XX YY, respectively. minimization function biconvex, solved simultaneously uu vv. solution therefore proceed variable variable: example, fix uu minimize vv, vice versa. function can also written : f(u,v)=âˆ¥Mâˆ’uvTâˆ¥F2+PÎ»(u)+PÎ¼(v)=âˆ‘=1pâˆ‘j=1q(mi,jâˆ’uivj)2+PÎ»(u)+PÎ¼(v)=âˆ‘=1pâˆ‘j=1qmi,j2âˆ’2âˆ‘=1pâˆ‘j=1qmi,juivj+âˆ‘=1pâˆ‘j=1qui2vj2+PÎ»(u)+PÎ¼(v)=âˆ‘=1pâˆ‘j=1qmi,j2âˆ’2âˆ‘=1puiâˆ‘j=1qmi,jvj+âˆ‘=1pui2âˆ‘j=1qvj2+PÎ»(u)+PÎ¼(v)=âˆ‘=1pâˆ‘j=1qmi,j2+âˆ‘=1pui2âˆ‘j=1qvj2âˆ’2âˆ‘=1puiâˆ‘j=1qmi,jvj+2Î»âˆ‘=1p|ui|+2Î¼âˆ‘j=1q|vj|\\begin{align*} f(u,v) &= \\|M - uv^T\\|^2_F + P_\\lambda(u) + P_\\mu(v) \\\\ &= \\sum_{=1}^{p} \\sum_{j=1}^{q} (m_{,j} - u_i v_j)^2 + P_\\lambda(u) + P_\\mu(v) \\\\ &= \\sum_{=1}^{p} \\sum_{j=1}^{q} m_{,j}^2 - 2\\sum_{=1}^{p} \\sum_{j=1}^{q} m_{,j} u_i v_j + \\sum_{=1}^{p} \\sum_{j=1}^{q} u_i^2 v_j^2 \\\\ & + P_\\lambda(u) + P_\\mu(v) \\\\ &= \\sum_{=1}^{p} \\sum_{j=1}^{q} m_{,j}^2 - 2\\sum_{=1}^{p} u_i \\sum_{j=1}^{q} m_{,j} v_j + \\sum_{=1}^{p} u_i^2 \\sum_{j=1}^{q} v_j^2 \\\\ & + P_\\lambda(u) + P_\\mu(v) \\\\ &= \\sum_{=1}^{p} \\sum_{j=1}^{q} m_{,j}^2 + \\sum_{=1}^{p} u_i^2 \\sum_{j=1}^{q} v_j^2 - 2\\sum_{=1}^{p} u_i \\sum_{j=1}^{q} m_{,j} v_j \\\\ & + 2\\lambda \\sum_{=1}^{p} |u_i| + 2\\mu \\sum_{j=1}^{q} |v_j| \\end{align*} following relations : PÎ»(u)=2Î»âˆ‘=1p|ui|P_\\lambda(u) = 2\\lambda \\sum_{=1}^{p} |u_i| PÎ¼(v)=2Î¼âˆ‘j=1q|vj|P_\\mu(v) = 2\\mu \\sum_{j=1}^{q} |v_j| h>1h>1, replace expressions mi,jm_{,j}, uiu_i, vjv_j mi,j(h)m^{(h)}_{,j}, ui(h)u^{(h)}_i, vj(h)v^{(h)}_j, respectively.","code":""},{"path":[]},{"path":"/articles/sPLS_method.html","id":"shen-huang-lemma","dir":"Articles","previous_headings":"How to solve minimisation problem ?","what":"Shen & Huang lemma","title":"sPLS method","text":"must therefore solve: uÌƒ=argminu||Mâˆ’uvT||F2+PÎ»(u)=argminu(âˆ‘=1pui2âˆ’2âˆ‘=1puiâˆ‘j=1qmi,jvj+2Î»âˆ‘=1p|ui|)âŸ¹uÌƒ=argminuui2âˆ’2uiâˆ‘j=1qmi,jvj+2Î»|ui|=argminuui2âˆ’2ui(Mv)+2Î»|ui|\\begin{align*}     \\tilde{u} &= \\operatorname*{argmin}_{u} ||M - uv^T||^2_F + P_\\lambda(u) \\\\     &= \\operatorname*{argmin}_u\\left( \\sum_{=1}^{p} u_i^2 -2\\sum_{=1}^{p}u_i\\sum_{j=1}^{q} m_{,j} v_j + 2\\lambda\\sum_{=1}^{p}|u_i| \\right) \\\\     \\implies\\tilde{u}_i &= \\operatorname*{argmin}_u u^2_i  -2u_i\\sum_{j=1}^{q} m_{,j} v_j + 2\\lambda|u_i| \\\\     &= \\operatorname*{argmin}_u u^2_i  -2u_i (Mv)_i     + 2\\lambda|u_i|  \\end{align*} Using first lemma Shen & Huang demonstrated little later, arrive following result: uÌƒ=sign((Mv))Ã—max(|(Mv)|âˆ’Î»,0)=soft(Mv,Î»)\\tilde{u}_i = \\text{sign}((Mv)_i) \\times \\max(|(Mv)_i|-\\lambda,0) = \\text{soft}(Mv,\\lambda) way, vÌƒ=argminv||Mâˆ’vuT||F2+PÎ¼(v)=argminv(âˆ‘j=1qvj2âˆ’2âˆ‘=1puiâˆ‘j=1qmi,jv)+2Î¼âˆ‘j=1q|vj|âŸ¹vÌƒj=argminv(vj2âˆ’2vjâˆ‘=1pmi,jui)+2Î¼|vj|=sign((MTu)j)Ã—max(|(MTu)j|âˆ’Î¼,0)=soft((MTu)j,Î¼)\\begin{align*} \\tilde{v} &= \\operatorname*{argmin}_v ||M - vu^T||^2_F + P_\\mu(v) \\\\ &= \\operatorname*{argmin}_v \\left( \\sum_{j=1}^{q} v_j^2  -2\\sum_{=1}^{p}u_i\\sum_{j=1}^{q} m_{,j} v \\right)+  2\\mu\\sum_{j=1}^{q}|v_j|\\\\       \\implies\\tilde{v}_j &= \\operatorname*{argmin}_v(v^2_j  -2v_j\\sum_{=1}^{p} m_{,j} u_i)+ 2\\mu|v_j| \\\\     &= \\text{sign}((M^Tu)_j ) \\times \\max(|(M^Tu)_j|-\\mu,0) \\\\     &= \\text{soft}((M^Tu)_j,\\mu) \\end{align*} Remarks : problem according uu, sum factor âˆ‘j=1qvj2\\sum_{j=1}^{q} v_j^2 disappears valid condition ||v||=âˆ‘j=1qvj2=1||v|| = \\sqrt{\\sum_{j=1}^{q} v_j^2} = 1. vv. term âˆ‘=1pâˆ‘j=1qmi,j2\\sum_{=1}^{p} \\sum_{j=1}^{q} m_{,j}^2 disappears neither uÌƒ\\tilde{u} vÌƒ\\tilde{v} depending . uÌƒ\\tilde{u}_i vÌƒj\\tilde{v}_j expressions made disappear âˆ‘=1p\\sum_{=1}^{p} âˆ‘j=1q\\sum_{j=1}^{q} respectively. Shen & Huang lemma defined : argminxx2âˆ’2ax+2b|x|=sign()Ã—max(||âˆ’b,0),âˆ€x,âˆˆâ„,b>0\\operatorname*{argmin}_x x^2 - 2ax + 2b|x| = \\text{sign}() \\times \\max(||-b,0), \\forall x,\\\\mathbb{R}, b>0 right expression also called function aa.","code":""},{"path":"/articles/sPLS_method.html","id":"lemma-demonstration","dir":"Articles","previous_headings":"How to solve minimisation problem ?","what":"Lemma demonstration","title":"sPLS method","text":"Let : f(x)=x2âˆ’2ax+2b|x|f(x) =  x^2 - 2ax + 2b|x| want find x*x^* fâ€²(x*)=0f'(x^*) = 0. fâ€²(x){=2xâˆ’2a+2bif x>0,âˆˆ]2xâˆ’2aâˆ’2b;2xâˆ’2a+2b[x=0,=2xâˆ’2aâˆ’2bif x<0.f'(x)  \\begin{cases}  = 2x - 2a +2b & \\text{} x > 0, \\\\ \\]2x - 2a -2b ; 2x - 2a +2b[ & \\text{} x = 0, \\\\ = 2x - 2a -2b & \\text{} x < 0. \\end{cases} fâ€²(x*)=0â‡”{2x*âˆ’2a+2b=0if x*>0,2x*âˆ’2aâˆ’2b=0if x*<0.â‡”{x*âˆ’+b=0if x*>0,x*âˆ’âˆ’b=0if x*<0.f'(x^*) = 0 \\iff \\begin{cases}  2x^* - 2a +2b = 0 & \\text{} x^* > 0, \\\\ 2x^* - 2a -2b = 0 & \\text{} x^* < 0. \\end{cases} \\iff  \\begin{cases}  x^* - +b = 0 & \\text{} x^* > 0, \\\\ x^* - -b = 0 & \\text{} x^* < 0. \\end{cases} â‡”x*={âˆ’bif x*>0,0if x*=0,+bif x*<0.â‡”x*={âˆ’bif >b,0if ||<b,+bif <âˆ’b.â‡”x*=sign()Ã—max(||âˆ’b,0)\\iff x^* = \\begin{cases}  -b & \\text{} x^* > 0, \\\\ 0 & \\text{} x^* = 0, \\\\ +b & \\text{} x^* < 0. \\end{cases} \\iff x^* =  \\begin{cases}  -b & \\text{} > b, \\\\ 0 & \\text{} || < b, \\\\ +b & \\text{} < -b. \\end{cases} \\iff x^* = \\text{sign}() \\times \\max(||-b,0) x*x^*, aa bb play respectively uu, MvMv Î»\\lambda role one hand vv, MTuM^Tu Î¼\\mu hand.","code":""},{"path":"/articles/sPLS_method.html","id":"convergence-algorithm","dir":"Articles","previous_headings":"How to solve minimisation problem ?","what":"Convergence algorithm","title":"sPLS method","text":"solve minimization problem, first perform SVD decomposition (first column matrices UU VV). vectors found yet solutions. Therefore, convergence algorithm must applied component hh. first define: uÌƒold(h)=uÌƒ(h)\\tilde{u}^{(h)}_{old} = \\tilde{u}^{(h)} vÌƒold(h)=vÌƒ(h)\\tilde{v}^{(h)}_{old} = \\tilde{v}^{(h)}. must therefore calculate âˆ€hâˆˆ1,...,H,âˆ€âˆˆ1,...,p,âˆ€jâˆˆ1,...,q\\forall h \\1,...,H, \\forall \\1,...,p, \\forall j \\1,...,q : uÌƒnew(h)=soft(M(h)vold(h),Î»h)âŸ¹uÌƒnew,(h)=soft((M(h)vold(h)),Î»h)vÌƒnew(h)=soft(M(h)Tuold(h),Î¼h)âŸ¹vÌƒnew,j(h)=soft((M(h)Tuold(h))j,Î¼h)\\begin{align*}     \\tilde{u}^{(h)}_{new} &= \\text{soft}(M^{(h)}v^{(h)}_{old},\\lambda_h)     \\implies\\tilde{u}^{(h)}_{new,} = \\text{soft}((M^{(h)}v^{(h)}_{old})_i,\\lambda_h) \\\\      \\tilde{v}^{(h)}_{new} &= \\text{soft}(M^{(h)T}u^{(h)}_{old},\\mu_h)     \\implies\\tilde{v}^{(h)}_{new,j} = \\text{soft}((M^{(h)T}u^{(h)}_{old})_j,\\mu_h)  \\end{align*} normalize weights : unew(h)=uÌƒnew(h)||uÌƒnew(h)||2;vnew(h)=vÌƒnew(h)||vÌƒnew(h)||2\\begin{align*}     u^{(h)}_{new} = \\frac{\\tilde{u}^{(h)}_{new}}{||\\tilde{u}^{(h)}_{new}||_2} ;     v^{(h)}_{new} = \\frac{\\tilde{v}^{(h)}_{new}}{||\\tilde{v}^{(h)}_{new}||_2} \\\\ \\end{align*} hence select unew(h)u^{(h)}_{new} vnew(h)v^{(h)}_{new} respectively |unew(h)âˆ’uold(h)|<eps|u^{(h)}_{new} - u^{(h)}_{old}| < eps |vnew(h)âˆ’vold(h)|<eps|v^{(h)}_{new} - v^{(h)}_{old}| < eps. |unew(h)âˆ’uold(h)|>eps|u^{(h)}_{new} - u^{(h)}_{old}| > eps |vnew(h)âˆ’vold(h)|>eps|v^{(h)}_{new} - v^{(h)}_{old}| > eps: assign following values: uold(h)=unew(h)u^{(h)}_{old} = u^{(h)}_{new} vold(h)=vnew(h)v^{(h)}_{old} = v^{(h)}_{new} repeat one loop.","code":""},{"path":"/articles/sPLS_method.html","id":"special-case-of-pls1","dir":"Articles","previous_headings":"How to solve minimisation problem ?","what":"Special case of PLS1","title":"sPLS method","text":"case PLS1, minimization problem uu becomes, considering m=XTym = X^Ty: uÌƒ=argminu||mâˆ’u||22+PÎ»(u)=argminu(âˆ‘=1pui2âˆ’2âˆ‘=1puimi+2Î»âˆ‘=1p|ui|)âŸ¹uÌƒ=argminuui2âˆ’2uimi+2Î»|ui|=sign(mi)Ã—max(|(mi|âˆ’Î»,0)=soft(mi,Î»)\\begin{align*}     \\tilde{u} &= \\operatorname*{argmin}_{u} ||m - u||^2_2 + P_\\lambda(u) \\\\     &= \\operatorname*{argmin}_u\\left( \\sum_{=1}^{p} u_i^2 -2\\sum_{=1}^{p}u_im_i + 2\\lambda\\sum_{=1}^{p}|u_i| \\right) \\\\     \\implies\\tilde{u}_i &= \\operatorname*{argmin}_u u^2_i  -2u_im_i     + 2\\lambda|u_i| \\\\     &= \\text{sign}(m_i) \\times \\max(|(m_i|-\\lambda,0) = \\text{soft}(m_i,\\lambda) \\end{align*} can now compute (PLS1 PLS2) : th=âˆ‘j=1puj(h)Xj(h)=X(h)u(h)t_{h} = \\sum_{j=1}^{p} u^{(h)}_j X^{(h)}_{j} = X^{(h)}u^{(h)} sh=âˆ‘j=1qvj(h)Yj(h)=Y(h)v(h)s_{h} = \\sum_{j=1}^{q} v^{(h)}_j Y^{(h)}_{j} = Y^{(h)}v^{(h)} principle matrix deflation PLS method.","code":""},{"path":"/articles/sPLS_performance.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"sPLS performance","text":"page presents application sPLS performance assessment. sPLS method quite particular method : several predictions according components number selected model. goal almost choose best number component sPLS regression order compute best possible predictions also select best number variables. , use three datasets: one dataset one response variable YY. dataset four response variables Y=(Y1,Y2,Y3,Y4)Y = (Y1,Y2,Y3,Y4). last dataset contains real data NIR spectra. access predefined functions sgPLSdevelop package manipulate datasets, run lines : two first datasets, population set n=40n = 40 default, close actual conditions. Letâ€™s also notice , average, response YY linear combination predictors XX. Indeed, function includes matrix product Y=XB+EY = XB + E BB weight matrix EE matrix gaussian noise. linearity condition important order good performance model, PLS method using linearity combinaison. Now, â€™s time train PLS model dataset built imported.","code":"library(sgPLSdevelop) library(pls)  data1 <- data.create(p = 10, list = TRUE) data2 <- data.create(p = 10, q = 4, list = TRUE) data(yarn) data3 <- yarn ## [1] \"First dataset dimensions : 40 x 11\" ## [1] \"Second dataset dimensions : 40 x 14\" ## [1] \"Yarn dataset dimensions : 28 x 3\" ncomp.max <- 8  # First model X <- data1$X Y <- data1$Y model1 <- sPLS(X,Y,mode = \"regression\", ncomp = ncomp.max)  # Second model X <- data2$X Y <- data2$Y model2 <- sPLS(X,Y,mode = \"regression\", ncomp = ncomp.max)  # Third model X <- data3$NIR Y <- data3$density model3 <- sPLS(X,Y,mode = \"regression\", ncomp = ncomp.max)"},{"path":"/articles/sPLS_performance.html","id":"spls-performance-assessment-using-msep","dir":"Articles","previous_headings":"","what":"sPLS performance assessment using MSEP","title":"sPLS performance","text":"good way assess model performance consists using MSEPMSEP criterion. MSEPMSEP computed follow : MSEP=1nqâˆ‘=1nâˆ‘j=1q(Yi,jâˆ’YÌ‚,j)2MSEP = \\frac{1}{nq} \\sum_{=1}^{n} \\sum_{j=1}^{q} (Y_{,j} - \\hat{Y}_{,j})^2 function named perf.sPLS allow assess performance choose best parameters. function outputs best tuning parameters also two MSEP plots : one shows MSEP according number components shows MSEP according number selected variables XX YY. Particularly, multivariate sPLS (q>1q>1), â€œplotâ€ table MSEP values.","code":""},{"path":"/articles/sPLS_performance.html","id":"first-model-msep","dir":"Articles","previous_headings":"sPLS performance assessment using MSEP","what":"First model MSEP","title":"sPLS performance","text":"perf.sPLS gives us optimal components number equal H=H = 7, therefore suggest select 7 components first model. indicates us also select 8 variables component.","code":"perf.res1 <- tuning.sPLS.XY(model1) h.best <- perf.res1$h.best keepX.best <- perf.res1$keepX.best keepY.best <- perf.res1$keepY.best"},{"path":"/articles/sPLS_performance.html","id":"second-model-msep","dir":"Articles","previous_headings":"sPLS performance assessment using MSEP","what":"Second model MSEP","title":"sPLS performance","text":"perf.sPLS gives us optimal components number equal H=H = 8, therefore suggest select 8 components first model. indicates us also select 9 variables XX 3 variables YY.","code":"perf.res2 <- tuning.sPLS.XY(model2) h.best <- perf.res2$h.best keepX.best <- perf.res2$keepX.best keepY.best <- perf.res2$keepY.best"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Benoit Liquet. Maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Liquet B, de Micheaux PL, Broc C (2023). sgPLS: Sparse Group Partial Least Square Methods. doi:10.32614/CRAN.package.sgPLS, R package version 1.8, https://CRAN.R-project.org/package=sgPLS.","code":"@Manual{,   title = {sgPLS: Sparse Group Partial Least Square Methods},   author = {Benoit Liquet and Pierre Lafaye {de Micheaux} and Camilo Broc},   year = {2023},   note = {R package version 1.8},   url = {https://CRAN.R-project.org/package=sgPLS},   doi = {10.32614/CRAN.package.sgPLS}, }"},{"path":"/reference/data.create.html","id":null,"dir":"Reference","previous_headings":"","what":"Dataset simulation â€” data.create","title":"Dataset simulation â€” data.create","text":"functions allow generate dataset linear dependance \\(Y\\) \\(X\\). data.create used quantitative response data.cl.create used qualitative response.","code":""},{"path":"/reference/data.create.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset simulation â€” data.create","text":"","code":"data.create(n = 40, p = 10, q = 1, list = TRUE) data.cl.create(n = 40, p = 10, classes = 2, list = TRUE)"},{"path":"/reference/data.create.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dataset simulation â€” data.create","text":"n number dataset rows p number \\(X\\) variables q number \\(Y\\) variables data.create function classes number classes generate data.cl.create function list default, returns list including dataframe. argument list set FALSE, dataframe returned.","code":""},{"path":"/reference/data.create.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Dataset simulation â€” data.create","text":"default, population set \\(n=40\\) close actual conditions. case, \\(p<n\\). data.create function, \\(Y\\) linear combinaison gaussian variable \\(X_j\\) \\(X\\). Indeed, function includes matrix product compute response : \\(Y = XB+E\\) \\(B\\) weight (coefficients) matrix \\(E\\) matrix gaussian noise. \\(B\\) matrix can found list returned function (list = TRUE). data.cl.create function, link \\(X\\) classes \\(Y\\). list returns also Y.f version Y factor class.","code":""},{"path":"/reference/data.create.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Dataset simulation â€” data.create","text":"","code":"library(sgPLSdevelop, warn.conflicts = FALSE, verbose = FALSE, quietly = TRUE) #> Registered S3 methods overwritten by 'sgPLSdevelop': #>   method          from  #>   predict.sPLS    sgPLS #>   predict.gPLS    sgPLS #>   predict.sgPLS   sgPLS #>   predict.sPLSda  sgPLS #>   predict.gPLSda  sgPLS #>   predict.sgPLSda sgPLS #>   perf.sPLS       sgPLS #>   perf.gPLS       sgPLS #>   perf.sgPLS      sgPLS #>   perf.sPLSda     sgPLS #>   perf.gPLSda     sgPLS #>   perf.sgPLSda    sgPLS  # data.create data <- data.create(n = 20, p = 5, q = 2, list = TRUE) X <- data$X Y <- data$Y  # data.cl.create data.cl <- data.cl.create(n = 20, p = 5, classes = 3, list = TRUE) X <- data.cl$X Y <- data.cl$Y"},{"path":"/reference/gPLS.html","id":null,"dir":"Reference","previous_headings":"","what":"Group Partial Least Squares (gPLS) â€” gPLS","title":"Group Partial Least Squares (gPLS) â€” gPLS","text":"Function perform group Partial Least Squares (gPLS)   context two datasets divided groups   variables. gPLS approach aims select groups   variables one dataset linearly related groups variables second dataset.","code":""},{"path":"/reference/gPLS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group Partial Least Squares (gPLS) â€” gPLS","text":"","code":"gPLS(X, Y, ncomp, mode = \"regression\",      max.iter = 500, tol = 1e-06, keepX,       keepY = NULL, ind.block.x, ind.block.y = NULL,scale=TRUE)"},{"path":"/reference/gPLS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group Partial Least Squares (gPLS) â€” gPLS","text":"X numeric matrix predictors. Y numeric vector matrix responses (multi-response models). ncomp number components include model (see Details). mode character string. type algorithm use, (partially) matching     one \"regression\" \"canonical\". See Details. max.iter integer, maximum number iterations. tol positive real, tolerance used iterative algorithm. keepX numeric vector length ncomp, number variables     keep \\(X\\)-loadings. default variables kept model. keepY numeric vector length ncomp, number variables     keep \\(Y\\)-loadings. default variables kept model. ind.block.x vector integers describing grouping \\(X\\)-variables. (see example Details section) ind.block.y vector consecutive integers describing grouping \\(Y\\)-variables (see example Details section) scale logical indicating orignal data set need scaled. default scale=TRUE","code":""},{"path":"/reference/gPLS.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Group Partial Least Squares (gPLS) â€” gPLS","text":"gPLS function fits gPLS models \\(1, \\ldots ,\\)ncomp components. Multi-response models fully supported. type algorithm use specified mode argument. Two gPLS algorithms available: gPLS regression (\"regression\") gPLS canonical analysis (\"canonical\") (see References). ind.block.x <- c(3,10,15) means \\(X\\) structured 4 groups: X1 X3; X4 X10, X11 X15 X16 X\\(p\\) \\(p\\) number variables \\(X\\) matrix.","code":""},{"path":"/reference/gPLS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Group Partial Least Squares (gPLS) â€” gPLS","text":"gPLS returns object class \"gPLS\", list contains following components:  X centered standardized original predictor matrix. Y centered standardized original response vector matrix. ncomp number components included model. mode algorithm used fit model. keepX number \\(X\\) variables kept model component. keepY number \\(Y\\) variables kept model component. mat.c matrix coefficients used internally predict. variates list containing variates. loadings list containing estimated loadings \\(X\\) \t\\(Y\\) variates. names list containing names used individuals variables. tol tolerance used iterative algorithm, used subsequent S3 methods. max.iter maximum number iterations, used subsequent S3 methods. iter vector containing number iterations convergence component. ind.block.x vector integers describing grouping X variables. ind.block.y vector consecutive integers describing grouping Y variables.","code":""},{"path":"/reference/gPLS.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Group Partial Least Squares (gPLS) â€” gPLS","text":"Liquet Benoit, Lafaye de Micheaux Pierre , Hejblum Boris, Thiebaut Rodolphe. group Sparse Group Partial Least Square approach applied Genomics context. Submitted. Le Cao, K.-., Martin, P.G.P., Robert-Grani\\'e, C. Besse, P. (2009). Sparse canonical methods biological data integration: application cross-platform study. BMC Bioinformatics 10:34. Le Cao, K.-., Rossouw, D., Robert-Grani\\'e, C. Besse, P. (2008). sparse PLS variable selection integrating Omics data. Statistical Applications Genetics Molecular Biology 7, article 35. Shen, H. Huang, J. Z. (2008). Sparse principal component analysis via regularized low rank matrix approximation. Journal Multivariate Analysis 99, 1015-1034. Tenenhaus, M. (1998). La r\\'egression PLS: th\\'eorie et pratique. Paris: Editions Technic. Wold H. (1966). Estimation principal components related models iterative least squares. : Krishnaiah, P. R. (editors), Multivariate Analysis. Academic Press, N.Y., 391-420.","code":""},{"path":"/reference/gPLS.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Group Partial Least Squares (gPLS) â€” gPLS","text":"Benoit Liquet Pierre Lafaye de Micheaux.","code":""},{"path":[]},{"path":"/reference/gPLS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group Partial Least Squares (gPLS) â€” gPLS","text":"","code":"## Simulation of datasets X and Y with group variables n <- 100 sigma.gamma <- 1 sigma.e <- 1.5 p <- 400 q <- 500 theta.x1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5), rep(1.5,15),                rep(0, 5), rep(-1.5, 15), rep(0, 325)) theta.x2 <- c(rep(0, 320), rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),               rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 5))  theta.y1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5), rep(1.5, 15),               rep(0, 5), rep(-1.5, 15), rep(0, 425)) theta.y2 <- c(rep(0, 420), rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),               rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 5))                               Sigmax <- matrix(0, nrow = p, ncol = p) diag(Sigmax) <- sigma.e ^ 2 Sigmay <- matrix(0,nrow = q, ncol = q) diag(Sigmay) <- sigma.e ^ 2  set.seed(125)  gam1 <- rnorm(n) gam2 <- rnorm(n)  X <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.x1, theta.x2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, p), sigma =      Sigmax, method = \"svd\") Y <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.y1, theta.y2),       nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, q), sigma =      Sigmay, method = \"svd\")   ind.block.x <- seq(20, 380, 20) ind.block.y <- seq(20, 480, 20) ##   #### gPLS model model.gPLS <- gPLS(X, Y, ncomp = 2, mode = \"regression\", keepX = c(4, 4),       keepY = c(4, 4), ind.block.x = ind.block.x , ind.block.y = ind.block.y)  result.gPLS <- select.sgpls(model.gPLS) result.gPLS$group.size.X #>    size comp1 comp2 #> 1    20    20     0 #> 2    20    20     0 #> 3    20    20     0 #> 4    20    20     0 #> 5    20     0     0 #> 6    20     0     0 #> 7    20     0     0 #> 8    20     0     0 #> 9    20     0     0 #> 10   20     0     0 #> 11   20     0     0 #> 12   20     0     0 #> 13   20     0     0 #> 14   20     0     0 #> 15   20     0     0 #> 16   20     0     0 #> 17   20     0    20 #> 18   20     0    20 #> 19   20     0    20 #> 20   20     0    20 result.gPLS$group.size.Y #>    size comp1 comp2 #> 1    20    20     0 #> 2    20    20     0 #> 3    20    20     0 #> 4    20    20     0 #> 5    20     0     0 #> 6    20     0     0 #> 7    20     0     0 #> 8    20     0     0 #> 9    20     0     0 #> 10   20     0     0 #> 11   20     0     0 #> 12   20     0     0 #> 13   20     0     0 #> 14   20     0     0 #> 15   20     0     0 #> 16   20     0     0 #> 17   20     0     0 #> 18   20     0     0 #> 19   20     0     0 #> 20   20     0     0 #> 21   20     0     0 #> 22   20     0    20 #> 23   20     0    20 #> 24   20     0    20 #> 25   20     0    20"},{"path":"/reference/gPLSda.html","id":null,"dir":"Reference","previous_headings":"","what":"Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” gPLSda","title":"Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” gPLSda","text":"Function perform group Partial Least Squares classify samples (supervised analysis) select variables.","code":""},{"path":"/reference/gPLSda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” gPLSda","text":"","code":"gPLSda(X, Y, ncomp = 2, keepX = rep(ncol(X), ncomp),        max.iter = 500, tol = 1e-06, ind.block.x)"},{"path":"/reference/gPLSda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” gPLSda","text":"X numeric matrix predictors. NAs allowed. Y factor class vector discrete outcome. ncomp number components include model (see Details). keepX numeric vector length ncomp, number variables     keep \\(X\\)-loadings. default variables kept model. max.iter integer, maximum number iterations. tol positive real, tolerance used iterative algorithm. ind.block.x vector integers describing grouping \\(X\\)-variables. (see example Details section)","code":""},{"path":"/reference/gPLSda.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” gPLSda","text":"gPLSda function fit gPLS models \\(1, \\ldots ,\\)ncomp components factor class vector Y. appropriate indicator (dummy) matrix created. ind.block.x <- c(3,10,15) means \\(X\\) structured 4 groups: X1 X3; X4 X10, X11 X15 X16 X\\(p\\) \\(p\\) number variables \\(X\\) matrix.","code":""},{"path":"/reference/gPLSda.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” gPLSda","text":"sPLSda returns object class \"sPLSda\", list contains following components:  X centered standardized original predictor matrix. Y centered standardized indicator response vector matrix. ind.mat indicator matrix. ncomp number components included model. keepX number \\(X\\) variables kept model component. mat.c matrix coefficients used internally predict. variates list containing variates. loadings list containing estimated loadings X \tY variates. names list containing names used individuals variables. tol tolerance used iterative algorithm, used subsequent S3 methods max.iter maximum number iterations, used subsequent S3 methods iter Number iterations algorthm component ind.block.x vector integers describing grouping X variables.","code":""},{"path":"/reference/gPLSda.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” gPLSda","text":"Liquet Benoit, Lafaye de Micheaux Pierre , Hejblum Boris, Thiebaut Rodolphe (2016). group Sparse Group Partial Least Square approach applied Genomics context. Bioinformatics. sPLS-DA: Le Cao, K.-., Boitard, S. Besse, P. (2011). Sparse PLS Discriminant Analysis: biologically relevant feature selection graphical displays multiclass problems. BMC Bioinformatics 12:253.","code":""},{"path":"/reference/gPLSda.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” gPLSda","text":"Benoit Liquet  Pierre Lafaye de Micheaux.","code":""},{"path":[]},{"path":"/reference/gPLSda.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” gPLSda","text":"","code":"data(simuData) X <- simuData$X Y <- simuData$Y ind.block.x <- seq(100, 900, 100) model <- gPLSda(X, Y, ncomp = 3,ind.block.x=ind.block.x, keepX = c(2, 2, 2)) result.gPLSda <- select.sgpls(model) result.gPLSda$group.size.X #>    size comp1 comp2 comp3 #> 1   100     0   100     0 #> 2   100     0     0   100 #> 3   100     0     0     0 #> 4   100   100     0     0 #> 5   100     0     0     0 #> 6   100     0   100     0 #> 7   100     0     0   100 #> 8   100     0     0     0 #> 9   100   100     0     0 #> 10  100     0     0     0  # perf(model,criterion=\"all\",validation=\"loo\") -> res # res$error.rate"},{"path":"/reference/msep.PLS.html","id":null,"dir":"Reference","previous_headings":"","what":"PLS function performance assessment using \\(MSEP\\) indicator â€” msep.PLS","title":"PLS function performance assessment using \\(MSEP\\) indicator â€” msep.PLS","text":"msep.PLS function allows assess PLS models using\\(MSEP\\) criterion.   function returns list including \\(MSEP\\) values number components.   msep.PLS gives also suggestion number components selection.   plot allows visualize model performance according number components.","code":""},{"path":"/reference/msep.PLS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"PLS function performance assessment using \\(MSEP\\) indicator â€” msep.PLS","text":"","code":"msep.PLS(object, ncomp = object$ncomp, K=nrow(object$X))"},{"path":"/reference/msep.PLS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"PLS function performance assessment using \\(MSEP\\) indicator â€” msep.PLS","text":"object Object class inheriting \"pls\". ncomp number components desired MSEP computing (number components computed model). K number blocks cross-validation (leave-one-default).","code":""},{"path":"/reference/msep.PLS.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"PLS function performance assessment using \\(MSEP\\) indicator â€” msep.PLS","text":"\\(K\\) must value 2 number rows dataset used training model. MSEP defined mean squared error true \\(Y\\) values associated predictions.","code":""},{"path":[]},{"path":"/reference/msep.PLS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"PLS function performance assessment using \\(MSEP\\) indicator â€” msep.PLS","text":"","code":"library(sgPLSdevelop)  ## data and model creation  d <- data.create(p = 10, list = TRUE) n <- nrow(d$X) ncomp.max <- 10 X <- d$X Y <- d$Y model <- PLS(X,Y,ncomp = ncomp.max, mode = \"regression\")  ## using msep.PLS function msep.res <- msep.PLS(model, ncomp = ncomp.max, K = n)  msep.res$MSEP  #>  [1] 0.326369075 0.073797494 0.013222233 0.004818591 0.002672382 0.002727054 #>  [7] 0.002773395 0.002791826 0.002797759 0.002797350 msep.res$h.best #number of components suggestion  #> [1] 5"},{"path":"/reference/per.variance.html","id":null,"dir":"Reference","previous_headings":"","what":"Percentage of variance of the \\(Y\\) matrix explained by the score-vectors obtained by PLS approaches â€” per.variance","title":"Percentage of variance of the \\(Y\\) matrix explained by the score-vectors obtained by PLS approaches â€” per.variance","text":"per.variance function computes percentage variance \\(Y\\) matrix explained score-vectors obtained PLS approaches (sPLS, gPLS sgPLS) regression mode.","code":""},{"path":"/reference/per.variance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Percentage of variance of the \\(Y\\) matrix explained by the score-vectors obtained by PLS approaches â€” per.variance","text":"","code":"per.variance(object)"},{"path":"/reference/per.variance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Percentage of variance of the \\(Y\\) matrix explained by the score-vectors obtained by PLS approaches â€” per.variance","text":"object object class inheriting \"sPLS\", \"gPLS\",    \"sgPLS\". function retrieve key parameters stored object.","code":""},{"path":"/reference/per.variance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Percentage of variance of the \\(Y\\) matrix explained by the score-vectors obtained by PLS approaches â€” per.variance","text":"per.variance produces list following components: perX Percentage variance \\(Y\\) matrix explained score-vectors. cum.perX cumulative percentage variance \\(Y\\) matrix explained score-vectors.","code":""},{"path":"/reference/per.variance.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Percentage of variance of the \\(Y\\) matrix explained by the score-vectors obtained by PLS approaches â€” per.variance","text":"Benoit Liquet, b.liquet@uq.edu.au,  Pierre Lafaye de Micheaux lafaye@dms.umontreal.ca","code":""},{"path":"/reference/per.variance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Percentage of variance of the \\(Y\\) matrix explained by the score-vectors obtained by PLS approaches â€” per.variance","text":"","code":"if (FALSE) { # \\dontrun{   ## Simulation of datasets X and Y with group variables n <- 100 sigma.gamma <- 1 sigma.e <- 1.5 p <- 400 q <- 500 theta.x1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5), rep(1.5, 15),               rep(0, 5), rep(-1.5, 15), rep(0, 325)) theta.x2 <- c(rep(0, 320), rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),               rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 5))  theta.y1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5), rep(1.5, 15),               rep(0, 5), rep(-1.5, 15), rep(0, 425)) theta.y2 <- c(rep(0, 420), rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),               rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 5))  Sigmax <- matrix(0, nrow = p, ncol = p) diag(Sigmax) <- sigma.e ^ 2 Sigmay <- matrix(0, nrow = q, ncol = q) diag(Sigmay) <- sigma.e ^ 2  set.seed(125)  gam1 <- rnorm(n) gam2 <- rnorm(n)  X <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.x1, theta.x2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, p), sigma =      Sigmax, method = \"svd\") Y <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.y1, theta.y2),       nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, q), sigma =      Sigmay, method = \"svd\")  ind.block.x <- seq(20, 380, 20) ind.block.y <- seq(20, 480, 20)  #### gPLS model model.sgPLS <- sgPLS(X, Y, ncomp = 2, mode = \"regression\", keepX = c(4, 4),                     keepY = c(4, 4), ind.block.x = ind.block.x,                    ind.block.y = ind.block.y,                    alpha.x = c(0.5, 0.5), alpha.y = c(0.5, 0.5))  result.sgPLS <- select.sgpls(model.sgPLS) result.sgPLS$group.size.X result.sgPLS$group.size.Y  #### gPLS model model.gPLS <- gPLS(X, Y, ncomp = 2, mode = \"regression\", keepX = c(4, 4),       keepY = c(4, 4), ind.block.x = ind.block.x ,ind.block.y = ind.block.y)  result.gPLS <- select.sgpls(model.gPLS) result.gPLS$group.size.X result.gPLS$group.size.Y  per.variance(model.gPLS) per.variance(model.sgPLS)  } # }"},{"path":"/reference/perf.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute evaluation criteria for PLS, sPLS, PLS-DA and sPLS-DA â€” perf","title":"Compute evaluation criteria for PLS, sPLS, PLS-DA and sPLS-DA â€” perf","text":"Function evaluate performance fitted sparse PLS, group PLS, sparse group PLS, sparse PLS-DA, group PLS-DA sparse group PLS-DA models using various criteria.","code":""},{"path":"/reference/perf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute evaluation criteria for PLS, sPLS, PLS-DA and sPLS-DA â€” perf","text":"","code":"# S3 method for class 'PLS' perf(object,            criterion = c(\"all\", \"MSEP\", \"Q2\"),            validation = c(\"Mfold\", \"loo\"),            folds = 10, ncomp = object$ncomp, progressBar = TRUE, setseed = 1,...)             # S3 method for class 'sPLS' perf(object,            criterion = c(\"all\", \"MSEP\", \"R2\", \"Q2\"),            validation = c(\"Mfold\", \"loo\"),            folds = 10, progressBar = TRUE, setseed = 1,...)            # S3 method for class 'gPLS' perf(object,            criterion = c(\"all\", \"MSEP\", \"R2\", \"Q2\"),            validation = c(\"Mfold\", \"loo\"),            folds = 10, progressBar = TRUE, setseed = 1, ...)            # S3 method for class 'sgPLS' perf(object,            criterion = c(\"all\", \"MSEP\", \"R2\", \"Q2\"),            validation = c(\"Mfold\", \"loo\"),            folds = 10, progressBar = TRUE,setseed = 1, ...)            # S3 method for class 'PLSda' perf(object,           method.predict = c(\"all\", \"max.dist\", \"centroids.dist\", \"mahalanobis.dist\"),           validation = c(\"Mfold\", \"loo\"),            folds = 10, progressBar = TRUE, ...)               # S3 method for class 'sPLSda' perf(object,           method.predict = c(\"all\", \"max.dist\", \"centroids.dist\", \"mahalanobis.dist\"),           validation = c(\"Mfold\", \"loo\"),            folds = 10, progressBar = TRUE, ...)    # S3 method for class 'gPLSda' perf(object,           method.predict = c(\"all\", \"max.dist\", \"centroids.dist\", \"mahalanobis.dist\"),           validation = c(\"Mfold\", \"loo\"),            folds = 10, progressBar = TRUE, ...)            # S3 method for class 'sgPLSda' perf(object,           method.predict = c(\"all\", \"max.dist\", \"centroids.dist\", \"mahalanobis.dist\"),           validation = c(\"Mfold\", \"loo\"),            folds = 10, progressBar = TRUE, ...)"},{"path":"/reference/perf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute evaluation criteria for PLS, sPLS, PLS-DA and sPLS-DA â€” perf","text":"object Object class inheriting \"sPLS\", \"gPLS\", \"sgPLS\", \"sPLSda\", \"gPLSda\" \"sgPLSda\". function retrieve key parameters stored object. criterion criteria measures calculated (see Details). Can set either \"\", \"MSEP\", \"R2\", \"Q2\". default set \"\". applies object inheriting \"sPLS\", \"gPLS\" \"sgPLS\" method.predict applies object inheriting \"PLSda\", \"gPLSda\" \"sgPLSda\" evaluate classification performance model. subset \"max.dist\", \"centroids.dist\", \"mahalanobis.dist\". Default \"\". See predict. validation Character.  kind (internal) validation use, matching one \"Mfold\"     \"loo\" (see ). Default \"Mfold\". folds folds Mfold cross-validation. See Details. progressBar default set TRUE output progress bar computation. setseed Integer value specify random generator state. ... used moment.","code":""},{"path":"/reference/perf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute evaluation criteria for PLS, sPLS, PLS-DA and sPLS-DA â€” perf","text":"method perf created Sebastien Dejean, Ignacio Gonzalez, Amrit Singh Kim-Anh Le Cao pls spls models performed mixOmics package. Similar code adapted sPLS, gPLS sgPLS package sgPLS. perf estimates mean squared error prediction (MSEP), \\(R^2\\), \\(Q^2\\) assess predictive performance model using M-fold leave-one-cross-validation. Note classic, regression  invariant modes can applied. validation = \"Mfold\", M-fold cross-validation performed. many folds generate selected specifying number folds folds. folds also can supplied list vectors containing indexes defining fold produced split. validation = \"loo\", leave-one-cross-validation performed. fitted sPLS-DA, gPLS-DA sgPLS-DA models, perf estimates classification error rate using cross-validation. Note perf function retrieve keepX keepY inputs previously run object. sPLS, gPLS, sgPLS, sPLSda, gPLSda sgPLSda functions run several different subsets data (cross-folds) certainly different subset selected features. sPLS, MSEP, \\(R^2\\), \\(Q^2\\) criteria averaged across folds. feature stability measure output user assess often variables selected across folds. sPLS-DA, classification erro rate averaged across folds.","code":""},{"path":"/reference/perf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute evaluation criteria for PLS, sPLS, PLS-DA and sPLS-DA â€” perf","text":"perf produces list following components: MSEP Mean Square Error Prediction \\(Y\\) variable, applies object inherited \"sPLS\", \"gPLS\" \"sgPLS\". R2 matrix \\(R^2\\) values \\(Y\\)-variables models     \\(1, \\ldots ,\\)ncomp components, applies object inherited \"sPLS\", \"gPLS\" \"sgPLS\". Q2 \\(Y\\) contains one variable, vector \\(Q^2\\) values else list     matrix \\(Q^2\\) values \\(Y\\)-variable. Note specific case sPLS model, better look Q2.total criterion, applies object inherited \"sPLS\", \"gPLS\" \"sgPLS\". Q2.total vector \\(Q^2\\)-total values models \\(1, \\ldots ,\\)ncomp components, applies object inherited \"sPLS\", \"gPLS\" \"sgPLS\". features list features selected across folds ($stable.X $stable.Y) whole data set ($final) keepX keepY parameters input object. error.rate sPLS-DA, gPLS-DA sgPLS-DA models, perf produces matrix classification error rate estimation. dimensions correspond components model prediction method used, respectively. Note error rates reported component include performance model earlier components specified keepX parameters (e.g. error rate reported component 3 keepX = 20 already includes fitted model components 1 2  keepX = 20). advanced usage perf function, see mixOmics package consider using predict function.","code":""},{"path":"/reference/perf.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute evaluation criteria for PLS, sPLS, PLS-DA and sPLS-DA â€” perf","text":"Tenenhaus, M. (1998). La r\\'egression PLS: th\\'eorie et pratique. Paris: Editions Technic. Le Cao, K.-., Rossouw, D., Robert-Grani\\'e, C. Besse, P. (2008). sparse PLS variable selection integrating Omics data. Statistical Applications Genetics Molecular Biology 7, article 35. Mevik, B.-H., Cederkvist, H. R. (2004). Mean Squared Error Prediction (MSEP) Estimates Principal Component Regression (PCR) Partial Least Squares Regression (PLSR). Journal Chemometrics 18(9), 422-429.","code":""},{"path":"/reference/perf.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Compute evaluation criteria for PLS, sPLS, PLS-DA and sPLS-DA â€” perf","text":"Benoit Liquet Pierre Lafaye de Micheaux","code":""},{"path":[]},{"path":"/reference/perf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute evaluation criteria for PLS, sPLS, PLS-DA and sPLS-DA â€” perf","text":"","code":"## validation for objects of class 'sPLS' (regression) ## Example from mixOmics package  # ---------------------------------------- if (FALSE) { # \\dontrun{ data(liver.toxicity) X <- liver.toxicity$gene Y <- liver.toxicity$clinic   ## validation for objects of class 'spls' (regression) # ---------------------------------------- ncomp <- 7 # first, learn the model on the whole data set model.spls <- sPLS(X, Y, ncomp = ncomp, mode = 'regression',    keepX = c(rep(5, ncomp)), keepY = c(rep(2, ncomp)))   # with leave-one-out cross validation set.seed(45) model.spls.loo.val <- perf(model.spls, validation = \"loo\")  #Q2 total model.spls.loo.val$Q2.total  # R2:we can see how the performance degrades when ncomp increases # results are similar to 5-fold model.spls.loo.val$R2  } # }"},{"path":"/reference/plotcim.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots a cluster image mapping of correlations between outcomes and all predictors â€” plotcim","title":"Plots a cluster image mapping of correlations between outcomes and all predictors â€” plotcim","text":"plotcim function plots cluster image   mapping correlations outcomes predictors.","code":""},{"path":"/reference/plotcim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots a cluster image mapping of correlations between outcomes and all predictors â€” plotcim","text":"","code":"plotcim(matX, matY, cexCol = 0.5, cexRow = 1)"},{"path":"/reference/plotcim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots a cluster image mapping of correlations between outcomes and all predictors â€” plotcim","text":"matX data frame corresponding predictors. matY data frame corresponding outcomes. cexRow, cexCol positive numbers, used cex.axis row column \taxis labeling. defaults currently use number rows columns, respectively.","code":""},{"path":"/reference/plotcim.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plots a cluster image mapping of correlations between outcomes and all predictors â€” plotcim","text":"used small number predictors (<1,000).","code":""},{"path":"/reference/plotcim.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plots a cluster image mapping of correlations between outcomes and all predictors â€” plotcim","text":"Benoit Liquet, b.liquet@uq.edu.au,  Pierre Lafaye de Micheaux lafaye@dms.umontreal.ca","code":""},{"path":"/reference/PLS.html","id":null,"dir":"Reference","previous_headings":"","what":"Partial Least Squares (PLS) â€” PLS","title":"Partial Least Squares (PLS) â€” PLS","text":"function performing partial least squares (OLS) method two data sets columns rows. PLS approach therefore useful data sets OLS regression possible.","code":""},{"path":"/reference/PLS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Partial Least Squares (PLS) â€” PLS","text":"","code":"PLS(X, Y, ncomp, mode = \"regression\",scale=TRUE)"},{"path":"/reference/PLS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Partial Least Squares (PLS) â€” PLS","text":"X numeric matrix predictors. Y numeric vector matrix responses (multi-response models). ncomp number components include model (see Details). mode character string. type algorithm use, (partially) matching     one \"regression\" \"canonical\". See Details. scale logical indicating orignal data set need scaled. default scale=TRUE","code":""},{"path":"/reference/PLS.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Partial Least Squares (PLS) â€” PLS","text":"PLS function fits PLS models \\(1, \\ldots ,\\)ncomp components. Multi-response models fully supported. type algorithm use specified mode argument. Two PLS algorithms available: PLS regression (\"regression\") PLS canonical analysis (\"canonical\") (see References).","code":""},{"path":"/reference/PLS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Partial Least Squares (PLS) â€” PLS","text":"PLS returns object class \"PLS\", list contains following components:  X centered standardized original predictor matrix. Y centered standardized original response vector matrix. ncomp number components included model. mode algorithm used fit model. mat.c matrix coefficients used internally predict. variates list containing variates. loadings list containing estimated loadings \\(X\\) \t\\(Y\\) variates. names list containing names used individuals variables.","code":""},{"path":"/reference/PLS.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Partial Least Squares (PLS) â€” PLS","text":"Liquet Benoit, Lafaye de Micheaux Pierre, Hejblum Boris, Thiebaut Rodolphe. Group sparse group partial least square approaches applied genomics context. bioinformatics","code":""},{"path":"/reference/PLS.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Partial Least Squares (PLS) â€” PLS","text":"Daniel FLORES.","code":""},{"path":[]},{"path":"/reference/PLS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Partial Least Squares (PLS) â€” PLS","text":"","code":"# Simulation of datasets X and Y with group variables  library(sgPLSdevelop)  ## First example   ### paramaters n <- 100 sigma.gamma <- 1 sigma.e <- 1.5 p <- 400 q <- 500  theta.x1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5), rep(1.5,15),                rep(0, 5), rep(-1.5, 15), rep(0, 325)) theta.x2 <- c(rep(0, 320), rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),               rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 5))  theta.y1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5), rep(1.5, 15),               rep(0, 5), rep(-1.5, 15), rep(0, 425)) theta.y2 <- c(rep(0, 420), rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),               rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 5))                              ### covariance matrices Sigmax <- matrix(0, nrow = p, ncol = p) diag(Sigmax) <- sigma.e ^ 2 Sigmay <- matrix(0,nrow = q, ncol = q) diag(Sigmay) <- sigma.e ^ 2  set.seed(125)  gam1 <- rnorm(n) gam2 <- rnorm(n)  GAM <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) Thetax <- matrix(c(theta.x1, theta.x2), nrow = 2, byrow = TRUE) Thetay <- matrix(c(theta.y1, theta.y2), nrow = 2, byrow = TRUE) E1 <- rmvnorm(n, mean = rep(0, p), sigma = Sigmax, method = \"svd\") E2 <- rmvnorm(n, mean = rep(0, q), sigma = Sigmay, method = \"svd\")    X <- GAM %*% Thetax + E1                                                 Y <- GAM %*% Thetay + E2  ### PLS model model.PLS <- PLS(X, Y, ncomp = 2, mode = \"regression\")   ## Second example  train <- 1:40 test <- 41:50 n.test <- length(test)  d <- data.create(n = 50, p = 10, q = 2, list = TRUE)  X <- d$X[train,] Y <- d$Y[train,] X.test <- d$X[test,] Y.test <- d$Y[test,]  ncompmax <- 10 model.pls <- PLS(X = X, Y = Y, ncomp = ncompmax, mode = \"regression\") pred <- predict.PLS(model.pls, newdata = X.test)$predict"},{"path":"/reference/PLSda.html","id":null,"dir":"Reference","previous_headings":"","what":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” PLSda","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” PLSda","text":"Function perform Partial Least Squares classify samples (supervised analysis).","code":""},{"path":"/reference/PLSda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” PLSda","text":"","code":"PLSda(X, Y, ncomp = 2)"},{"path":"/reference/PLSda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” PLSda","text":"X numeric matrix predictors. NAs allowed. Y factor class vector discrete outcome. ncomp number components include model (see Details).","code":""},{"path":"/reference/PLSda.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” PLSda","text":"PLSda function fit PLS models \\(1, \\ldots ,\\)ncomp components factor class vector Y. appropriate indicator (dummy) matrix created.","code":""},{"path":"/reference/PLSda.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” PLSda","text":"PLSda returns object class \"PLSda\", list contains following components:  X centered standardized original predictor matrix. Y centered standardized indicator response vector matrix. ind.mat indicator matrix. ncomp number components included model. mat.c matrix coefficients used internally predict. variates list containing variates. loadings list containing estimated loadings X \tY variates. names list containing names used individuals variables.","code":""},{"path":"/reference/PLSda.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” PLSda","text":"Benoit Liquet Pierre Lafaye de Micheaux.","code":""},{"path":[]},{"path":"/reference/PLSda.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” PLSda","text":"","code":"library(sgPLSdevelop)  ncompmax <- 5 d <- data.cl.create(classes = 4, n = 30, list = TRUE) X <- d$X Y <- d$Y  modele <- PLSda(X = X, Y = Y, ncomp = ncompmax)"},{"path":"/reference/pls_perf.html","id":null,"dir":"Reference","previous_headings":"","what":"A Capitalized Title (ideally limited to 65 characters) â€” pls perf","title":"A Capitalized Title (ideally limited to 65 characters) â€” pls perf","text":"function ....","code":""},{"path":"/reference/pls_perf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A Capitalized Title (ideally limited to 65 characters) â€” pls perf","text":"","code":"pls perf(x)"},{"path":"/reference/pls_perf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A Capitalized Title (ideally limited to 65 characters) â€” pls perf","text":"x","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"/reference/pls_perf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A Capitalized Title (ideally limited to 65 characters) â€” pls perf","text":"","code":"##---- Should be DIRECTLY executable !! ---- ##-- ==>  Define data, use random, ##--  or standard data sets, see data().  ## The function is currently defined as function (x)  {   } #> function (x)  #> { #> } #> <environment: 0x000002023b754a80>"},{"path":"/reference/predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for PLS, sPLS, gPLS, sgPLS, sPLDda, gPLSda, sgPLSda â€” predict","title":"Predict Method for PLS, sPLS, gPLS, sgPLS, sPLDda, gPLSda, sgPLSda â€” predict","text":"Predicted values based PLS, sparse PLS, group PLS, sparse group PLS, PLSda, sparse PLSda, group PLSda, sparse group PLSda models. New responses variates predicted using fitted model new matrix observations.","code":""},{"path":"/reference/predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for PLS, sPLS, gPLS, sgPLS, sPLDda, gPLSda, sgPLSda â€” predict","text":"","code":"# S3 method for class 'PLS' predict(object, newdata, ...)  # S3 method for class 'sPLS' predict(object, newdata, ...)  # S3 method for class 'gPLS' predict(object, newdata, ...)  # S3 method for class 'sgPLS' predict(object, newdata, ...)  # S3 method for class 'PLSda' predict(object, newdata, method = c(\"all\", \"max.dist\",          \"centroids.dist\", \"mahalanobis.dist\"), ...)  # S3 method for class 'sPLSda' predict(object, newdata, method = c(\"all\", \"max.dist\",          \"centroids.dist\", \"mahalanobis.dist\"), ...)  # S3 method for class 'gPLSda' predict(object, newdata, method = c(\"all\", \"max.dist\",          \"centroids.dist\", \"mahalanobis.dist\"), ...)  # S3 method for class 'sgPLSda' predict(object, newdata, method = c(\"all\", \"max.dist\",          \"centroids.dist\", \"mahalanobis.dist\"), ...)"},{"path":"/reference/predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for PLS, sPLS, gPLS, sgPLS, sPLDda, gPLSda, sgPLSda â€” predict","text":"object object class inheriting \"PLS\", \"sPLS\", \"gPLS\", \"sgPLS\", \"PLSda\", \"sPLSda\", \"gPLSda\"  \"sgPLSda\". newdata data matrix look explanatory variables used prediction. method method applied \"PLSda\", sPLSda, gPLSda sgPLSda predict class new data,     subset \"centroids.dist\", \"mahalanobis.dist\" \"max.dist\" (see Details). \tDefaults \"\". ... used currently.","code":""},{"path":"/reference/predict.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict Method for PLS, sPLS, gPLS, sgPLS, sPLDda, gPLSda, sgPLSda â€” predict","text":"predict function pls spls object created Sebastien Dejean, Ignacio Gonzalez, Amrit Singh Kim-Anh Le Cao mixOmics package. Similar code used sPLS, gPLS, sgPLS, sPLSda, gPLSda, sgPLSda models performed sgPLS package. predict function produces predicted values, obtained evaluating sparse PLS, group PLS sparse group PLS model returned \"PLS\", sPLS, gPLS sgPLS frame newdata. Variates newdata also returned. prediction values calculated based regression coefficients object$Y onto object$variates$X. Different class prediction methods proposed \"PLSda\", sPLSda, gPLSda sgPLSda: \"max.dist\" naive method predict class. based predicted matrix (object$predict) can seen probability matrix assign test data class. class largest class value predicted class. \"centroids.dist\" allocates individual \\(x\\) class \\(Y\\) minimizing \\(dist(\\code{x-variate}, G_l)\\), \\(G_l\\), \\(l = 1,...,L\\) centroids classes calculated \\(X\\)-variates model. \"mahalanobis.dist\" allocates individual \\(x\\) class \\(Y\\) \"centroids.dist\" using Mahalanobis metric calculation distance.","code":""},{"path":"/reference/predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for PLS, sPLS, gPLS, sgPLS, sPLDda, gPLSda, sgPLSda â€” predict","text":"predict produces list following components: predict three dimensional array predicted response values. dimensions \tcorrespond observations, response variables model dimension, respectively. variates Matrix predicted variates. B.hat Matrix regression coefficients (without intercept). class vector matrix predicted class using \\(1,...,\\)ncomp     (sparse)PLS-DA components. centroids matrix coordinates centroids.","code":""},{"path":"/reference/predict.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Predict Method for PLS, sPLS, gPLS, sgPLS, sPLDda, gPLSda, sgPLSda â€” predict","text":"Tenenhaus, M. (1998). La r\\'egression PLS: th\\'eorie et pratique. Paris: Editions Technic.","code":""},{"path":"/reference/predict.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predict Method for PLS, sPLS, gPLS, sgPLS, sPLDda, gPLSda, sgPLSda â€” predict","text":"Benoit Liquet Pierre Lafaye de Micheaux","code":""},{"path":[]},{"path":"/reference/q2.PLS.html","id":null,"dir":"Reference","previous_headings":"","what":"PLS function performance assessment using Q2 indicator. â€” q2.PLS_doc","title":"PLS function performance assessment using Q2 indicator. â€” q2.PLS_doc","text":"q2.PLS function allows assess PLS models using \\(Q2\\) criterion. function returns list including \\(Q2\\) values number components. plot allows visualize model performance according number components.","code":""},{"path":"/reference/q2.PLS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"PLS function performance assessment using Q2 indicator. â€” q2.PLS_doc","text":"","code":"q2.PLS(object, ncomp = object$ncomp, mode = \"regression\")"},{"path":"/reference/q2.PLS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"PLS function performance assessment using Q2 indicator. â€” q2.PLS_doc","text":"object Object class inheriting \"pls\". ncomp number components desired q2 computing (number components computed model). mode Character string. type algorithm use, (partially) matching one \"regression\" \"canonical\".","code":""},{"path":"/reference/q2.PLS.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"PLS function performance assessment using Q2 indicator. â€” q2.PLS_doc","text":"suggestion number components selection also given Q2 criterion. Endly, indicators given PRESS RSS number components \\(PRESSj\\) \\(RSSj\\) (matrices) given number components given column Y dataset.","code":""},{"path":"/reference/q2.PLS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"PLS function performance assessment using Q2 indicator. â€” q2.PLS_doc","text":"\\(PRESSj\\) \\(RSSj\\) matrices size \\(ncomp\\) x \\(q\\). row sums \\(PRESSj\\) \\(RSSj\\) give respectively PRESS RSS.","code":""},{"path":"/reference/q2.PLS.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"PLS function performance assessment using Q2 indicator. â€” q2.PLS_doc","text":"K.-. Le Cao, Zoe Welham, Multivariate data integration using R (pages 172 174), MixOmics","code":""},{"path":[]},{"path":"/reference/q2.PLS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"PLS function performance assessment using Q2 indicator. â€” q2.PLS_doc","text":"","code":"library(sgPLSdevelop)  d <- data.create(n = 50, p = 10, q = 2, list = TRUE) X <- d$X Y <- d$Y  ncomp.max <- 10 model <- PLS(X = X, Y = Y, ncomp = ncomp.max, mode = \"regression\")  par(mfrow = c(1,2)) q2.res <- q2.PLS(model, ncomp = ncomp.max, mode = \"regression\") h.best <- q2.res$h.best q2.PLS(model, ncomp = min(h.best+1, ncomp.max))$q2  #> [1]  0.5991091  0.6717849 -0.1313998  # q2 values q2.res$q2 #>  [1]   0.5991091   0.6717849  -0.1313998  -0.7011579  -4.4219535 -12.3546642 #>  [7] -31.5921060 -39.3521016 -44.3679320 -52.3112505  # PRESS values q2.res$PRESS #>  [1] 39.287310 10.888879  6.471984  5.736846  6.598945  7.143261  8.134252 #>  [8]  9.127399 10.128897 11.856188  # RSS values #q2.res$RSS  # PRESS values by column q2.res$PRESSj #>              Y1        Y2 #>  [1,] 22.109356 17.177955 #>  [2,]  6.783288  4.105590 #>  [3,]  3.893774  2.578209 #>  [4,]  4.248296  1.488550 #>  [5,]  4.939311  1.659634 #>  [6,]  5.362923  1.780339 #>  [7,]  6.092753  2.041499 #>  [8,]  6.749920  2.377480 #>  [9,]  7.392741  2.736156 #> [10,]  8.587569  3.268619  # RSS values by column #q2.res$RSSj"},{"path":"/reference/select.sgpls.html","id":null,"dir":"Reference","previous_headings":"","what":"Output of selected variables from a gPLS model or a sgPLS model â€” select.sgpls","title":"Output of selected variables from a gPLS model or a sgPLS model â€” select.sgpls","text":"function outputs selected variables component group sparse group PLS.","code":""},{"path":"/reference/select.sgpls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Output of selected variables from a gPLS model or a sgPLS model â€” select.sgpls","text":"","code":"select.sgpls(model)"},{"path":"/reference/select.sgpls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Output of selected variables from a gPLS model or a sgPLS model â€” select.sgpls","text":"model object class inheriting  \"gPLS\" \"sgPLS\".","code":""},{"path":"/reference/select.sgpls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Output of selected variables from a gPLS model or a sgPLS model â€” select.sgpls","text":"select.sgpls produces list following components: group.size.X matrix containing first column size groups \\(X\\) dataset. , next columns indicate size groups selected component. select.group.X list containing element (corresponding group \\(X\\) dataset) indices variables selected. group.size.Y matrix containing first column size groups \\(Y\\) dataset. next columns indicate size groups selected component. select.group.Y list containing element (corresponding group \\(Y\\) dataset) indices variables selected. select.X list containing element (corresponding component gPLS sgPLS model) names selected variables \\(X\\) dataset. select.Y list containing element (corresponding component gPLS sgPLS model) names selected variables \\(Y\\) dataset. select.X.total names variables selected gPLS sgPLS model regarding \\(X\\) matrix. select.Y.total names variables selected gPLS sgPLS model regarding \\(Y\\) matrix.","code":""},{"path":"/reference/select.sgpls.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Output of selected variables from a gPLS model or a sgPLS model â€” select.sgpls","text":"Benoit Liquet, b.liquet@uq.edu.au,  Pierre Lafaye de Micheaux lafaye@dms.umontreal.ca","code":""},{"path":"/reference/select.sgpls.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Output of selected variables from a gPLS model or a sgPLS model â€” select.sgpls","text":"","code":"if (FALSE) { # \\dontrun{   ## Simulation of datasets X and Y with group variables n <- 100 sigma.gamma <- 1 sigma.e <- 1.5 p <- 400 q <- 500 theta.x1 <- c(rep(1,15),rep(0,5),rep(-1,15),rep(0,5),rep(1.5,15)       ,rep(0,5),rep(-1.5,15),rep(0,325)) theta.x2 <- c(rep(0,320),rep(1,15),rep(0,5),rep(-1,15),rep(0,5),       rep(1.5,15),rep(0,5),rep(-1.5,15),rep(0,5))  theta.y1 <- c(rep(1,15),rep(0,5),rep(-1,15),rep(0,5),rep(1.5,15)       ,rep(0,5),rep(-1.5,15),rep(0,425)) theta.y2 <- c(rep(0,420),rep(1,15),rep(0,5),rep(-1,15),rep(0,5),       rep(1.5,15),rep(0,5),rep(-1.5,15),rep(0,5))                               Sigmax <- matrix(0, nrow = p, ncol = p) diag(Sigmax) <- sigma.e ^ 2 Sigmay <- matrix(0, nrow = q, ncol = q) diag(Sigmay) <- sigma.e ^ 2  set.seed(125)  gam1 <- rnorm(n) gam2 <- rnorm(n)  X <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.x1, theta.x2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, p), sigma =      Sigmax, method = \"svd\") Y <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.y1, theta.y2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, q), sigma =      Sigmay, method = \"svd\")  ind.block.x <- seq(20, 380, 20) ind.block.y <- seq(20, 480, 20)  #### gPLS model model.sgPLS <- sgPLS(X, Y, ncomp = 2, mode = \"regression\", keepX = c(4, 4),                     keepY = c(4, 4), ind.block.x = ind.block.x,                    ind.block.y = ind.block.y,                    alpha.x = c(0.5, 0.5), alpha.y = c(0.5, 0.5))  result.sgPLS <- select.sgpls(model.sgPLS) result.sgPLS$group.size.X result.sgPLS$group.size.Y  #### gPLS model model.gPLS <- gPLS(X, Y, ncomp = 2, mode = \"regression\", keepX = c(4, 4),       keepY = c(4,4), ind.block.x = ind.block.x ,ind.block.y = ind.block.y)  result.gPLS <- select.sgpls(model.gPLS) result.gPLS$group.size.X result.gPLS$group.size.Y    } # }"},{"path":"/reference/select.spls.html","id":null,"dir":"Reference","previous_headings":"","what":"Output of selected variables from a sPLS model â€” select.spls","title":"Output of selected variables from a sPLS model â€” select.spls","text":"function outputs selected variables component sPLS.","code":""},{"path":"/reference/select.spls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Output of selected variables from a sPLS model â€” select.spls","text":"","code":"select.spls(model)"},{"path":"/reference/select.spls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Output of selected variables from a sPLS model â€” select.spls","text":"model object class inheriting \"sPLS\".","code":""},{"path":"/reference/select.spls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Output of selected variables from a sPLS model â€” select.spls","text":"select.spls produces list following components: select.X list containing element (corresponding component sPLS model) names selected variables \\(X\\) dataset. select.Y list containing element (corresponding component sPLS model) names selected variables \\(Y\\) dataset. select.X.total names variables selected sPLS model regarding \\(X\\) matrix. select.Y.total names variables selected sPLS model regarding \\(Y\\) matrix.","code":""},{"path":"/reference/select.spls.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Output of selected variables from a sPLS model â€” select.spls","text":"Benoit Liquet, b.liquet@uq.edu.au,  Pierre Lafaye de Micheaux lafaye@dms.umontreal.ca","code":""},{"path":"/reference/select.spls.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Output of selected variables from a sPLS model â€” select.spls","text":"","code":"if (FALSE) { # \\dontrun{   ## Simulation of datasets X and Y with group variables n <- 100 sigma.gamma <- 1 sigma.e <- 1.5 p <- 400 q <- 500 theta.x1 <- c(rep(1,15),rep(0,5),rep(-1,15),rep(0,5),rep(1.5,15)              ,rep(0,5),rep(-1.5,15),rep(0,325)) theta.x2 <- c(rep(0,320),rep(1,15),rep(0,5),rep(-1,15),rep(0,5)              ,rep(1.5,15),rep(0,5),rep(-1.5,15),rep(0,5))  theta.y1 <- c(rep(1,15),rep(0,5),rep(-1,15),rep(0,5),rep(1.5,15)              ,rep(0,5),rep(-1.5,15),rep(0,425)) theta.y2 <- c(rep(0,420),rep(1,15),rep(0,5),rep(-1,15),rep(0,5),              rep(1.5,15),rep(0,5),rep(-1.5,15),rep(0,5))                               temp <-  matrix(c(theta.y1, theta.y2), nrow = 2, byrow = TRUE)  Sigmax <- matrix(0, nrow = p, ncol = p) diag(Sigmax) <- sigma.e ^ 2 Sigmay <- matrix(0, nrow = q, ncol = q) diag(Sigmay) <- sigma.e ^ 2  set.seed(125)  gam1 <- rnorm(n) gam2 <- rnorm(n)  X <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.x1, theta.x2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, p), sigma =      Sigmax, method = \"svd\") Y <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.y1, theta.y2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, q), sigma =      Sigmay, method = \"svd\")  ind.block.x <- seq(20, 380, 20) ind.block.y <- seq(20, 480, 20)  #### sPLS model model.sPLS <- sPLS(X, Y, ncomp = 2, mode = \"regression\", keepX = c(60, 60),                       keepY = c(60, 60)) result.sPLS <- select.spls(model.sPLS) result.sPLS$select.X result.sPLS$select.Y    } # }"},{"path":"/reference/sgPLS-internal.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal Functions â€” sgPLS-internal","title":"Internal Functions â€” sgPLS-internal","text":"Internal functions used user.","code":""},{"path":"/reference/sgPLS-package.html","id":null,"dir":"Reference","previous_headings":"","what":"Group and Sparse Group Partial Least Square Model â€” sgPLS-package","title":"Group and Sparse Group Partial Least Square Model â€” sgPLS-package","text":"sgPLS package provides sparse, group sparse group   version PLS approaches. \tmain functions :  sPLS sparse PLS, gPLS group PLS sgPLS sparse group PLS.","code":""},{"path":"/reference/sgPLS-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Group and Sparse Group Partial Least Square Model â€” sgPLS-package","text":"Benoit Liquet <b.liquet@uq.edu.au>, Pierre Lafaye de Micheaux","code":""},{"path":"/reference/sgPLS-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Group and Sparse Group Partial Least Square Model â€” sgPLS-package","text":"Liquet Benoit, Lafaye de Micheaux Pierre, Hejblum Boris, Thiebaut Rodolphe. group Sparse Group Partial Least Square approach applied Genomics context. Submitted.","code":""},{"path":[]},{"path":"/reference/sgPLS.html","id":null,"dir":"Reference","previous_headings":"","what":"Sparse Group Partial Least Squares (sgPLS) â€” sgPLS","title":"Sparse Group Partial Least Squares (sgPLS) â€” sgPLS","text":"Function perform  sparse group Partial Least Squares (sgPLS) conext datasets divided groups variables. sgPLS approach enables selection groups single feature levels.","code":""},{"path":"/reference/sgPLS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sparse Group Partial Least Squares (sgPLS) â€” sgPLS","text":"","code":"sgPLS(X, Y, ncomp, mode = \"regression\",      max.iter = 500, tol = 1e-06, keepX,       keepY = NULL,ind.block.x, ind.block.y = NULL, alpha.x, alpha.y = NULL,      upper.lambda = 10 ^ 5,scale=TRUE)"},{"path":"/reference/sgPLS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sparse Group Partial Least Squares (sgPLS) â€” sgPLS","text":"X Numeric matrix predictors. Y Numeric vector matrix responses (multi-response models). ncomp number components include model (see Details). mode character string. type algorithm use, (partially) matching     one \"regression\" \"canonical\". See Details. max.iter Integer, maximum number iterations. tol positive real, tolerance used iterative algorithm. keepX Numeric vector length ncomp, number variables     keep \\(X\\)-loadings. default variables kept model. keepY Numeric vector length ncomp, number variables     keep \\(Y\\)-loadings. default variables kept model. ind.block.x vector integers describing grouping \\(X\\) variables. (see example Details section). ind.block.y vector integers describing grouping \\(Y\\) variables (see example Details section). alpha.x mixing parameter (value 0 1) related sparsity within group \\(X\\) dataset. alpha.y mixing parameter (value 0 1) related sparsity within group \\(Y\\) dataset. upper.lambda default upper.lambda=10 ^ 5. large value specifying upper bound intervall lambda values searching value tuning parameter (lambda) corresponding non-zero group variables. scale logical indicating orignal data set need scaled. default scale=TRUE","code":""},{"path":"/reference/sgPLS.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sparse Group Partial Least Squares (sgPLS) â€” sgPLS","text":"sgPLS function fit gPLS models \\(1, \\ldots ,\\)ncomp components. Multi-response models fully supported. type algorithm use specified mode argument. Two gPLS algorithms available: gPLS regression (\"regression\") gPLS canonical analysis (\"canonical\") (see References). ind.block.x <- c(3, 10, 15) means \\(X\\) structured 4 groups: X1 X3; X4 X10, X11 X15 X16 X\\(p\\) \\(p\\) number variables \\(X\\) matrix.","code":""},{"path":"/reference/sgPLS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sparse Group Partial Least Squares (sgPLS) â€” sgPLS","text":"sgPLS returns object class \"sgPLS\", list contains following components:  X centered standardized original predictor matrix. Y centered standardized original response vector matrix. ncomp number components included model. mode algorithm used fit model. keepX Number \\(X\\) variables kept model component. keepY Number \\(Y\\) variables kept model component. mat.c Matrix coefficients used internally predict. variates List containing variates. loadings List containing estimated loadings \\(X\\) \t\\(Y\\) variates. names List containing names used individuals variables. tol tolerance used iterative algorithm, used subsequent S3 methods. max.iter maximum number iterations, used subsequent S3 methods. iter Vector containing number iterations convergence component. ind.block.x vector integers describing grouping \\(X\\) variables. ind.block.y vector consecutive integers describing grouping \\(Y\\) variables. alpha.x mixing parameter related sparsity within group \\(X\\) dataset. alpha.y mixing parameter related sparsity within group \\(Y\\) dataset. upper.lambda upper bound intervall lambda values searching value tuning parameter (lambda) corresponding non-zero group variables.","code":""},{"path":"/reference/sgPLS.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sparse Group Partial Least Squares (sgPLS) â€” sgPLS","text":"Liquet Benoit, Lafaye de Micheaux, Boris Hejblum, Rodolphe Thiebaut (2016). group Sparse Group Partial Least Square approach applied Genomics context. Bioinformatics. Le Cao, K.-., Martin, P.G.P., Robert-Grani\\'e, C. Besse, P. (2009). Sparse canonical methods biological data integration: application cross-platform study. BMC Bioinformatics 10:34. Le Cao, K.-., Rossouw, D., Robert-Grani\\'e, C. Besse, P. (2008). sparse PLS variable selection integrating Omics data. Statistical Applications Genetics Molecular Biology 7, article 35. Shen, H. Huang, J. Z. (2008). Sparse principal component analysis via regularized low rank matrix approximation. Journal Multivariate Analysis 99, 1015-1034. Tenenhaus, M. (1998). La r\\'egression PLS: th\\'eorie et pratique. Paris: Editions Technic. Wold H. (1966). Estimation principal components related models iterative least squares. : Krishnaiah, P. R. (editors), Multivariate Analysis. Academic Press, N.Y., 391-420.","code":""},{"path":"/reference/sgPLS.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Sparse Group Partial Least Squares (sgPLS) â€” sgPLS","text":"Benoit Liquet Pierre Lafaye de Micheaux.","code":""},{"path":[]},{"path":"/reference/sgPLS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sparse Group Partial Least Squares (sgPLS) â€” sgPLS","text":"","code":"## Simulation of datasets X and Y with group variables n <- 100 sigma.gamma <- 1 sigma.e <- 1.5 p <- 400 q <- 500 theta.x1 <- c(rep(1,15),rep(0,5),rep(-1,15),rep(0,5),rep(1.5,15)              ,rep(0,5),rep(-1.5,15),rep(0,325)) theta.x2 <- c(rep(0,320),rep(1,15),rep(0,5),rep(-1,15),rep(0,5)              ,rep(1.5,15),rep(0,5),rep(-1.5,15),rep(0,5))  theta.y1 <- c(rep(1,15),rep(0,5),rep(-1,15),rep(0,5),rep(1.5,15)              ,rep(0,5),rep(-1.5,15),rep(0,425)) theta.y2 <- c(rep(0,420),rep(1,15),rep(0,5),rep(-1,15),rep(0,5),       rep(1.5,15),rep(0,5),rep(-1.5,15),rep(0,5))                                Sigmax <- matrix(0, nrow = p, ncol = p) diag(Sigmax) <- sigma.e ^ 2 Sigmay <- matrix(0, nrow = q, ncol = q) diag(Sigmay) <- sigma.e ^ 2  set.seed(125)  gam1 <- rnorm(n) gam2 <- rnorm(n)  X <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.x1, theta.x2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, p), sigma =      Sigmax, method = \"svd\") Y <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.y1, theta.y2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, q), sigma =      Sigmay, method = \"svd\")   ind.block.x <- seq(20, 380, 20) ind.block.y <- seq(20, 480, 20) ##   model.sgPLS <- sgPLS(X, Y, ncomp = 2, mode = \"regression\", keepX = c(4, 4),                     keepY = c(4, 4), ind.block.x = ind.block.x                    ,ind.block.y = ind.block.y,                    alpha.x = c(0.95, 0.95), alpha.y = c(0.95, 0.95))  result.sgPLS <- select.sgpls(model.sgPLS) result.sgPLS$group.size.X #>    size comp1 comp2 #> 1    20    15     0 #> 2    20    15     0 #> 3    20    16     0 #> 4    20    15     0 #> 5    20     0     0 #> 6    20     0     0 #> 7    20     0     0 #> 8    20     0     0 #> 9    20     0     0 #> 10   20     0     0 #> 11   20     0     0 #> 12   20     0     0 #> 13   20     0     0 #> 14   20     0     0 #> 15   20     0     0 #> 16   20     0     0 #> 17   20     0    15 #> 18   20     0    15 #> 19   20     0    16 #> 20   20     0    15 result.sgPLS$group.size.Y #>    size comp1 comp2 #> 1    20    15     0 #> 2    20    15     0 #> 3    20    16     0 #> 4    20    16     0 #> 5    20     0     0 #> 6    20     0     0 #> 7    20     0     0 #> 8    20     0     0 #> 9    20     0     0 #> 10   20     0     0 #> 11   20     0     0 #> 12   20     0     0 #> 13   20     0     0 #> 14   20     0     0 #> 15   20     0     0 #> 16   20     0     0 #> 17   20     0     0 #> 18   20     0     0 #> 19   20     0     0 #> 20   20     0     0 #> 21   20     0     0 #> 22   20     0    15 #> 23   20     0    15 #> 24   20     0    15 #> 25   20     0    15"},{"path":"/reference/sgPLSda.html","id":null,"dir":"Reference","previous_headings":"","what":"Sparse Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” sgPLSda","title":"Sparse Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” sgPLSda","text":"Function perform sparse group Partial Least Squares classify samples (supervised analysis) select variables.","code":""},{"path":"/reference/sgPLSda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sparse Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” sgPLSda","text":"","code":"sgPLSda(X, Y, ncomp = 2, keepX = rep(ncol(X), ncomp),        max.iter = 500, tol = 1e-06, ind.block.x,      alpha.x, upper.lambda = 10 ^ 5)"},{"path":"/reference/sgPLSda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sparse Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” sgPLSda","text":"X numeric matrix predictors. NAs allowed. Y factor class vector discrete outcome. ncomp number components include model (see Details). keepX numeric vector length ncomp, number variables     keep \\(X\\)-loadings. default variables kept model. max.iter integer, maximum number iterations. tol positive real, tolerance used iterative algorithm. ind.block.x vector integers describing grouping \\(X\\)-variables. (see example Details section) alpha.x mixing parameter (value 0 1) related sparsity within group \\(X\\) dataset. upper.lambda default upper.lambda=10 ^ 5. large value specifying upper bound intervall lambda values searching value tuning parameter (lambda) corresponding non-zero group variables.","code":""},{"path":"/reference/sgPLSda.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sparse Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” sgPLSda","text":"sgPLSda function fit sgPLS models \\(1, \\ldots ,\\)ncomp components factor class vector Y. appropriate indicator (dummy) matrix created. ind.block.x <- c(3,10,15) means \\(X\\) structured 4 groups: X1 X3; X4 X10, X11 X15 X16 X\\(p\\) \\(p\\) number variables \\(X\\) matrix.","code":""},{"path":"/reference/sgPLSda.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sparse Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” sgPLSda","text":"sPLSda returns object class \"sPLSda\", list contains following components:  X centered standardized original predictor matrix. Y centered standardized indicator response vector matrix. ind.mat indicator matrix. ncomp number components included model. keepX number \\(X\\) variables kept model component. mat.c matrix coefficients used internally predict. variates list containing variates. loadings list containing estimated loadings X \tY variates. names list containing names used individuals variables. tol tolerance used iterative algorithm, used subsequent S3 methods max.iter maximum number iterations, used subsequent S3 methods iter Number iterations algorthm component ind.block.x vector integers describing grouping X variables. alpha.x mixing parameter related sparsity within group \\(X\\) dataset. upper.lambda upper bound intervall lambda values searching value tuning parameter (lambda) corresponding non-zero group variables.","code":""},{"path":"/reference/sgPLSda.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sparse Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” sgPLSda","text":"Liquet Benoit, Lafaye de Micheaux Pierre , Hejblum Boris, Thiebaut Rodolphe (2016). group Sparse Group Partial Least Square approach applied Genomics context. Bioinformatics. sPLS-DA: Le Cao, K.-., Boitard, S. Besse, P. (2011). Sparse PLS Discriminant Analysis: biologically relevant feature selection graphical displays multiclass problems. BMC Bioinformatics 12:253.","code":""},{"path":"/reference/sgPLSda.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Sparse Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” sgPLSda","text":"Benoit Liquet Pierre Lafaye de Micheaux.","code":""},{"path":[]},{"path":"/reference/sgPLSda.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sparse Group Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” sgPLSda","text":"","code":"data(simuData) X <- simuData$X Y <- simuData$Y ind.block.x <- seq(100, 900, 100) ind.block.x[2] <- 250 #To add some noise in the second group model <- sgPLSda(X, Y, ncomp = 3,ind.block.x=ind.block.x, keepX = c(2, 2, 2) , alpha.x = c(0.5,0.5,0.99)) result.sgPLSda <- select.sgpls(model) result.sgPLSda$group.size.X #>    size comp1 comp2 comp3 #> 1   100     0   100     0 #> 2   150     0     0   101 #> 3    50     0     0     0 #> 4   100   100     0     0 #> 5   100     0     0     0 #> 6   100     0   100     0 #> 7   100     0     0   100 #> 8   100     0     0     0 #> 9   100   100     0     0 #> 10  100     0     0     0 ##perf(model,criterion=\"all\",validation=\"loo\") -> res ##res$error.rate"},{"path":"/reference/simuData.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulated Data for group PLS-DA model â€” simuData","title":"Simulated Data for group PLS-DA model â€” simuData","text":"simulated data set contains expression 1000 genes 4 clusters 48 different individuals.","code":""},{"path":"/reference/simuData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulated Data for group PLS-DA model â€” simuData","text":"","code":"data(simuData)"},{"path":"/reference/simuData.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Simulated Data for group PLS-DA model â€” simuData","text":"list containing following components: X data matrix 48 rows 1000 columns. row represents       experimental sample, column single gene. Y factor variable indicating cluster subject","code":""},{"path":"/reference/simuData.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulated Data for group PLS-DA model â€” simuData","text":"data simulated 6 groups 100 genes linked 4 clusters. others 4 groups 100 genes added represent noise. relevant groups group 1,2,4,6,7 9. groups 3,5,8, 10 noise groups.","code":""},{"path":"/reference/sPLS.html","id":null,"dir":"Reference","previous_headings":"","what":"Sparse Partial Least Squares (sPLS) â€” sPLS","title":"Sparse Partial Least Squares (sPLS) â€” sPLS","text":"Function perform sparse Partial Least Squares (sPLS). sPLS approach combines integration variable selection simultaneously two data sets one-step strategy.","code":""},{"path":"/reference/sPLS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sparse Partial Least Squares (sPLS) â€” sPLS","text":"","code":"sPLS(X, Y, ncomp, mode = \"regression\",      max.iter = 500, tol = 1e-06, keepX = rep(ncol(X), ncomp),       keepY = rep(ncol(Y), ncomp),scale=TRUE)"},{"path":"/reference/sPLS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sparse Partial Least Squares (sPLS) â€” sPLS","text":"X Numeric matrix predictors. Y Numeric vector matrix responses (multi-response models). ncomp number components include model (see Details). mode Character string. type algorithm use, (partially) matching     one \"regression\" \"canonical\". See Details. max.iter Integer, maximum number iterations. tol positive real, tolerance used iterative algorithm. keepX Numeric vector length ncomp, number variables     keep \\(X\\)-loadings. default variables kept model. keepY Numeric vector length ncomp, number variables     keep \\(Y\\)-loadings. default variables kept model. scale logical indicating orignal data set need scaled. default scale=TRUE","code":""},{"path":"/reference/sPLS.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sparse Partial Least Squares (sPLS) â€” sPLS","text":"sPLS function fit sPLS models \\(1, \\ldots ,\\)ncomp components. Multi-response models fully supported. type algorithm use specified mode argument. Two sPLS algorithms available: sPLS regression (\"regression\") sPLS canonical analysis (\"canonical\") (see References).","code":""},{"path":"/reference/sPLS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sparse Partial Least Squares (sPLS) â€” sPLS","text":"sPLS returns object class \"sPLS\", list contains following components:  X centered standardized original predictor matrix. Y centered standardized original response vector matrix. ncomp number components included model. mode algorithm used fit model. keepX Number \\(X\\) variables kept model component. keepY Number \\(Y\\) variables kept model component. mat.c Matrix coefficients used internally predict. variates List containing variates. loadings List containing estimated loadings \\(X\\) \t\\(Y\\) variates. names List containing names used individuals variables. tol tolerance used iterative algorithm, used subsequent S3 methods max.iter maximum number iterations, used subsequent S3 methods","code":""},{"path":"/reference/sPLS.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sparse Partial Least Squares (sPLS) â€” sPLS","text":"Liquet Benoit, Lafaye de Micheaux Pierre, Hejblum Boris, Thiebaut Rodolphe. group Sparse Group Partial Least Square approach applied Genomics context. Submitted. Le Cao, K.-., Martin, P.G.P., Robert-Grani\\', C. Besse, P. (2009). Sparse canonical methods biological data integration: application cross-platform study. BMC Bioinformatics 10:34. Le Cao, K.-., Rossouw, D., Robert-Grani\\'e, C. Besse, P. (2008). sparse PLS variable selection integrating Omics data. Statistical Applications Genetics Molecular Biology 7, article 35. Shen, H. Huang, J. Z. (2008). Sparse principal component analysis via regularized low rank matrix approximation. Journal Multivariate Analysis 99, 1015-1034. Tenenhaus, M. (1998). La r\\'egression PLS: th\\'eorie et pratique. Paris: Editions Technic. Wold H. (1966). Estimation principal components related models iterative least squares. : Krishnaiah, P. R. (editors), Multivariate Analysis. Academic Press, N.Y., 391-420.","code":""},{"path":"/reference/sPLS.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Sparse Partial Least Squares (sPLS) â€” sPLS","text":"Benoit Liquet Pierre Lafaye de Micheaux.","code":""},{"path":[]},{"path":"/reference/sPLS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sparse Partial Least Squares (sPLS) â€” sPLS","text":"","code":"## Simulation of datasets X and Y with group variables n <- 100 sigma.gamma <- 1 sigma.e <- 1.5 p <- 400 q <- 500 theta.x1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),       rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 325)) theta.x2 <- c(rep(0, 320), rep(1, 15), rep(0, 5), rep(-1, 15),       rep(0, 5), rep(1.5, 15), rep(0, 5), rep(-1.5, 15),       rep(0, 5))  theta.y1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),        rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 425)) theta.y2 <- c(rep(0, 420), rep(1, 15), rep(0, 5), rep(-1, 15)       ,rep(0, 5), rep(1.5, 15), rep(0, 5), rep(-1.5, 15)       , rep(0, 5))                               Sigmax <- matrix(0, nrow = p, ncol = p) diag(Sigmax) <- sigma.e ^ 2 Sigmay <- matrix(0, nrow = q, ncol = q) diag(Sigmay) <- sigma.e ^ 2  set.seed(125)  gam1 <- rnorm(n) gam2 <- rnorm(n)  X <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.x1, theta.x2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, p), sigma =      Sigmax, method = \"svd\") Y <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.y1, theta.y2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, q), sigma =      Sigmay, method = \"svd\")   ind.block.x <- seq(20, 380, 20) ind.block.y <- seq(20, 480, 20)   #### sPLS model model.sPLS <- sPLS(X, Y, ncomp = 2, mode = \"regression\", keepX = c(60, 60),                       keepY = c(60, 60)) result.sPLS <- select.spls(model.sPLS) result.sPLS$select.X #> [[1]] #>  X1  X2  X3  X4  X5  X6  X7  X8  X9 X10 X11 X12 X13 X14 X15 X21 X22 X23 X24 X25  #>   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  21  22  23  24  25  #> X26 X27 X28 X29 X30 X31 X32 X33 X34 X35 X41 X42 X43 X44 X45 X46 X47 X48 X49 X50  #>  26  27  28  29  30  31  32  33  34  35  41  42  43  44  45  46  47  48  49  50  #> X51 X52 X53 X54 X55 X61 X62 X63 X64 X65 X66 X67 X68 X69 X70 X71 X72 X73 X74 X75  #>  51  52  53  54  55  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  #>  #> [[2]] #> X321 X322 X323 X324 X325 X326 X327 X328 X329 X330 X331 X332 X333 X334 X335 X341  #>  321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  341  #> X342 X343 X344 X345 X346 X347 X348 X349 X350 X351 X352 X353 X354 X355 X361 X362  #>  342  343  344  345  346  347  348  349  350  351  352  353  354  355  361  362  #> X363 X364 X365 X366 X367 X368 X369 X370 X371 X372 X373 X374 X375 X381 X382 X383  #>  363  364  365  366  367  368  369  370  371  372  373  374  375  381  382  383  #> X384 X385 X386 X387 X388 X389 X390 X391 X392 X393 X394 X395  #>  384  385  386  387  388  389  390  391  392  393  394  395  #>  result.sPLS$select.Y #> [[1]] #>  Y1  Y2  Y3  Y4  Y5  Y6  Y7  Y8  Y9 Y10 Y11 Y12 Y13 Y14 Y15 Y21 Y22 Y23 Y24 Y25  #>   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  21  22  23  24  25  #> Y26 Y27 Y28 Y29 Y30 Y31 Y32 Y33 Y34 Y35 Y41 Y42 Y43 Y44 Y45 Y46 Y47 Y48 Y49 Y50  #>  26  27  28  29  30  31  32  33  34  35  41  42  43  44  45  46  47  48  49  50  #> Y51 Y52 Y53 Y54 Y55 Y61 Y62 Y63 Y64 Y65 Y66 Y67 Y68 Y69 Y70 Y71 Y72 Y73 Y74 Y75  #>  51  52  53  54  55  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  #>  #> [[2]] #> Y421 Y422 Y423 Y424 Y425 Y426 Y427 Y428 Y429 Y430 Y431 Y432 Y433 Y434 Y435 Y441  #>  421  422  423  424  425  426  427  428  429  430  431  432  433  434  435  441  #> Y442 Y443 Y444 Y445 Y446 Y447 Y448 Y449 Y450 Y451 Y452 Y453 Y454 Y455 Y461 Y462  #>  442  443  444  445  446  447  448  449  450  451  452  453  454  455  461  462  #> Y463 Y464 Y465 Y466 Y467 Y468 Y469 Y470 Y471 Y472 Y473 Y474 Y475 Y481 Y482 Y483  #>  463  464  465  466  467  468  469  470  471  472  473  474  475  481  482  483  #> Y484 Y485 Y486 Y487 Y488 Y489 Y490 Y491 Y492 Y493 Y494 Y495  #>  484  485  486  487  488  489  490  491  492  493  494  495  #>"},{"path":"/reference/sPLSda.html","id":null,"dir":"Reference","previous_headings":"","what":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” sPLSda","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” sPLSda","text":"Function perform sparse Partial Least Squares classify samples (supervised analysis) select variables.","code":""},{"path":"/reference/sPLSda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” sPLSda","text":"","code":"sPLSda(X, Y, ncomp = 2, keepX = rep(ncol(X), ncomp),        max.iter = 500, tol = 1e-06)"},{"path":"/reference/sPLSda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” sPLSda","text":"X numeric matrix predictors. NAs allowed. Y factor class vector discrete outcome. ncomp number components include model (see Details). keepX numeric vector length ncomp, number variables     keep \\(X\\)-loadings. default variables kept model. max.iter integer, maximum number iterations. tol positive real, tolerance used iterative algorithm.","code":""},{"path":"/reference/sPLSda.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” sPLSda","text":"sPLSda function fit sPLS models \\(1, \\ldots ,\\)ncomp components factor class vector Y. appropriate indicator (dummy) matrix created.","code":""},{"path":"/reference/sPLSda.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” sPLSda","text":"sPLSda returns object class \"sPLSda\", list contains following components:  X centered standardized original predictor matrix. Y centered standardized indicator response vector matrix. ind.mat indicator matrix. ncomp number components included model. keepX number \\(X\\) variables kept model component. mat.c matrix coefficients used internally predict. variates list containing variates. loadings list containing estimated loadings X \tY variates. names list containing names used individuals variables. tol tolerance used iterative algorithm, used subsequent S3 methods max.iter maximum number iterations, used subsequent S3 methods iter Number iterations algorthm component","code":""},{"path":"/reference/sPLSda.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” sPLSda","text":"sPLS-DA: Le Cao, K.-., Boitard, S. Besse, P. (2011). Sparse PLS Discriminant Analysis: biologically relevant feature selection graphical displays multiclass problems. BMC Bioinformatics 12:253.","code":""},{"path":"/reference/sPLSda.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” sPLSda","text":"Benoit Liquet Pierre Lafaye de Micheaux.","code":""},{"path":[]},{"path":"/reference/sPLSda.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sparse Partial Least Squares Discriminant Analysis (sPLS-DA) â€” sPLSda","text":"","code":"### Examples from mixOmics packages     data(liver.toxicity) X <- as.matrix(liver.toxicity$gene) # Y will be transformed as a factor in the function, # but we set it as a factor to set up the colors. Y <- as.factor(liver.toxicity$treatment[, 4])  model <- sPLSda(X, Y, ncomp = 2, keepX = c(20, 20))"},{"path":"/reference/tuning.gPLS.X.html","id":null,"dir":"Reference","previous_headings":"","what":"Choice of the tuning parameter (number of groups) related to predictor matrix for gPLS model (regression mode) â€” tuning.gPLS.X","title":"Choice of the tuning parameter (number of groups) related to predictor matrix for gPLS model (regression mode) â€” tuning.gPLS.X","text":"grid tuning parameter, function computes leave-one-M-fold cross-validation MSEP (Mean Square Error Prediction) gPLS model.","code":""},{"path":"/reference/tuning.gPLS.X.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Choice of the tuning parameter (number of groups) related to predictor matrix for gPLS model (regression mode) â€” tuning.gPLS.X","text":"","code":"tuning.gPLS.X(X,Y,folds=10,validation=c(\"Mfold\",\"loo\"),     ncomp,keepX=NULL,grid.X,setseed,progressBar=FALSE,     ind.block.x=ind.block.x)"},{"path":"/reference/tuning.gPLS.X.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Choice of the tuning parameter (number of groups) related to predictor matrix for gPLS model (regression mode) â€” tuning.gPLS.X","text":"X Numeric matrix data frame \\((n \\times p)\\), observations \\(X\\) variables. Y Numeric matrix data frame \\((n \\times q)\\), observations \\(Y\\) variables. folds Positive integer. Number folds use validation=\"Mfold\". Defaults folds=10. validation Character string. kind (internal) cross-validation method use, (partially) matching one \"Mfolds\" (M-folds) \"loo\" (leave-one-). ncomp Number component investigating choice tuning parameter. keepX Vector integer indicating number group variables keep component. See details information. grid.X Vector integers defining values tuning parameter (corresponding number group variables select) cross-validation score computed. setseed Integer indicating random number generation state. progressBar default set FALSE output progress bar computation. ind.block.x vector integers describing grouping X variables. (see example details section)","code":""},{"path":"/reference/tuning.gPLS.X.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Choice of the tuning parameter (number of groups) related to predictor matrix for gPLS model (regression mode) â€” tuning.gPLS.X","text":"validation=\"Mfolds\", M-fold cross-validation performed calling Mfold. folds generated. number cross-validation folds specified argument folds. validation=\"loo\", leave-one-cross-validation performed calling loo function. case arguments folds ignored. keepX specified (default NULL), element keepX indicates value tuning parameter corresponding component. choice tuning parameters corresponding remaining components investigating evaluating cross-validation score different values defining grid.X.","code":""},{"path":"/reference/tuning.gPLS.X.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Choice of the tuning parameter (number of groups) related to predictor matrix for gPLS model (regression mode) â€” tuning.gPLS.X","text":"returned value list components: MSEP Matrix containing cross-validation score computed grid. keepX Value tuning parameter (lambda)     cross-validation method reached minimum.","code":""},{"path":"/reference/tuning.gPLS.X.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Choice of the tuning parameter (number of groups) related to predictor matrix for gPLS model (regression mode) â€” tuning.gPLS.X","text":"Benoit Liquet Pierre Lafaye de Micheaux","code":""},{"path":"/reference/tuning.gPLS.X.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Choice of the tuning parameter (number of groups) related to predictor matrix for gPLS model (regression mode) â€” tuning.gPLS.X","text":"","code":"if (FALSE) { # \\dontrun{   ## Simulation of Datasets X (with group variables) and Y a multivariate response variable  n <- 200 sigma.e <- 0.5 p <- 400 q <- 10 theta.x1 <- c(rep(1,15),rep(0,5),rep(-1,15),rep(0,5),rep(1.5,15),       rep(0,5),rep(-1.5,15),rep(0,325)) theta.x2 <- c(rep(0,320),rep(1,15),rep(0,5),rep(-1,15),rep(0,5),       rep(1.5,15),rep(0,5),rep(-1.5,15),rep(0,5))  set.seed(125) theta.y1 <- runif(10,0.5,2) theta.y2 <- runif(10,0.5,2)    temp <-  matrix(c(theta.y1,theta.y2),nrow=2,byrow=TRUE)  Sigmax <- matrix(0,nrow=p,ncol=p) diag(Sigmax) <- sigma.e^2 Sigmay <- matrix(0,nrow=q,ncol=q) diag(Sigmay) <- sigma.e^2  gam1 <- rnorm(n,0,1) gam2 <- rnorm(n,0,1)  X <- matrix(c(gam1,gam2),ncol=2,byrow=FALSE)%*%matrix(c(theta.x1,theta.x2),nrow=2,byrow=TRUE) +rmvnorm(n,mean=rep(0,p),sigma=Sigmax,method=\"svd\") Y <- matrix(c(gam1,gam2),ncol=2,byrow=FALSE)%*%t(svd(temp)$v) +rmvnorm(n,mean=rep(0,q),sigma=Sigmay,method=\"svd\")  ind.block.x <- seq(20,380,20)  grid.X <- 1:16  ## Strategy with same value for both components tun.gPLS <- tuning.gPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"),      ncomp=2,keepX = NULL, grid.X=grid.X, setseed=1, progressBar = FALSE,      ind.block.x = ind.block.x)   tun.gPLS$keepX # for each component  ##For a sequential strategy tun.gPLS.1 <- tuning.gPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"),        ncomp=1, keepX = NULL, grid.X=grid.X, setseed=1,                              ind.block.x = ind.block.x)  tun.gPLS.1$keepX # for the first component  tun.gPLS.2 <- tuning.gPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"), ncomp=2,                              keepX = tun.gPLS.1$keepX , grid.X=grid.X, setseed=1,                              ind.block.x = ind.block.x)   tun.gPLS.2$keepX # for the second component } # }"},{"path":"/reference/tuning.sgPLS.X.html","id":null,"dir":"Reference","previous_headings":"","what":"Choice of the tuning parameters (number of groups and mixing parameter) related to predictor matrix for sgPLS model (regression mode) â€” tuning.sgPLS.X","title":"Choice of the tuning parameters (number of groups and mixing parameter) related to predictor matrix for sgPLS model (regression mode) â€” tuning.sgPLS.X","text":"grid two dimension tuning parameters, function computes leave-one-M-fold cross-validation MSEP (Mean Square Error Prediction) sgPLS model.","code":""},{"path":"/reference/tuning.sgPLS.X.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Choice of the tuning parameters (number of groups and mixing parameter) related to predictor matrix for sgPLS model (regression mode) â€” tuning.sgPLS.X","text":"","code":"tuning.sgPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"), ncomp,         keepX = NULL, alpha.x = NULL, grid.gX, grid.alpha.X,         setseed, progressBar = FALSE, ind.block.x = ind.block.x,         upper.lambda = 10 ^ 9)"},{"path":"/reference/tuning.sgPLS.X.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Choice of the tuning parameters (number of groups and mixing parameter) related to predictor matrix for sgPLS model (regression mode) â€” tuning.sgPLS.X","text":"X Numeric matrix data frame \\((n \\times p)\\), observations \\(X\\) variables. Y Numeric matrix data frame \\((n \\times q)\\), observations \\(Y\\) variables. folds Positive integer. Number folds use validation=\"Mfold\". Defaults     folds=10. validation Character string. kind (internal) cross-validation method use,     (partially) matching one \"Mfolds\" (M-folds) \"loo\" (leave-one-). ncomp Number component investigating choice tuning parameter. keepX Vector integer indicating number group variables keep component. See Details information. alpha.x Numeric vector indicating number group variables keep component. See Details information. grid.gX,grid.alpha.X Vector numeric defining values   tuning parameter lambda (number groups select) tuning   parameter alpha (mixing paramter values 0 1) cross-validation score computed setseed Integer indicating random number generation state. progressBar default set FALSE output progress bar computation. ind.block.x vector integers describing grouping X variables. (see example Details section). upper.lambda default upper.lambda=10 ^ 9. large value specifying upper bound intervall lambda values searching value tuning parameter (lambda) corresponding non-zero group variables.","code":""},{"path":"/reference/tuning.sgPLS.X.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Choice of the tuning parameters (number of groups and mixing parameter) related to predictor matrix for sgPLS model (regression mode) â€” tuning.sgPLS.X","text":"validation = \"Mfolds\", M-fold cross-validation performed calling Mfold. folds generated. number cross-validation folds specified argument folds. validation = \"loo\", leave-one-cross-validation performed calling loo function. case arguments folds ignored. keepX specified (default NULL), element keepX indicates value tuning parameter corresponding component. choice tuning parameters corresponding remaining components investigating evaluating cross-validation score different values defining grid.X. alpha.x specified (default NULL), element alpha.x indicates value tuning parameter (alpha) corresponding component. choice tuning parameters corresponding remaining components investigating evaluating cross-vlidation score different values defining grid.alpha.X.","code":""},{"path":"/reference/tuning.sgPLS.X.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Choice of the tuning parameters (number of groups and mixing parameter) related to predictor matrix for sgPLS model (regression mode) â€” tuning.sgPLS.X","text":"returned value list components: MSEP vector containing cross-validation score computed grid keepX value tuning parameter     cross-validation method reached minimum. alphaX value tuning parameter (alpha)     cross-validation method reached minimum.","code":""},{"path":"/reference/tuning.sgPLS.X.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Choice of the tuning parameters (number of groups and mixing parameter) related to predictor matrix for sgPLS model (regression mode) â€” tuning.sgPLS.X","text":"Benoit Liquet Pierre Lafaye de Micheaux","code":""},{"path":"/reference/tuning.sgPLS.X.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Choice of the tuning parameters (number of groups and mixing parameter) related to predictor matrix for sgPLS model (regression mode) â€” tuning.sgPLS.X","text":"","code":"if (FALSE) { # \\dontrun{   ## Simulation of datasets X (with group variables) and Y a multivariate response variable  n <- 200 sigma.e <- 0.5 p <- 400 q <- 10 theta.x1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5), rep(1.5, 15),       rep(0, 5), rep(-1.5, 15), rep(0, 325)) theta.x2 <- c(rep(0, 320), rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),       rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 5))  set.seed(125) theta.y1 <- runif(10, 0.5, 2) theta.y2 <- runif(10, 0.5, 2)    temp <-  matrix(c(theta.y1, theta.y2), nrow = 2, byrow = TRUE)  Sigmax <- matrix(0, nrow = p, ncol = p) diag(Sigmax) <- sigma.e ^ 2 Sigmay <- matrix(0, nrow = q, ncol = q) diag(Sigmay) <- sigma.e ^ 2  gam1 <- rnorm(n) gam2 <- rnorm(n)  X <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.x1, theta.x2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, p), sigma =      Sigmax, method = \"svd\") Y <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% t(svd(temp)$v)       + rmvnorm(n, mean = rep(0, q), sigma = Sigmay, method = \"svd\")  ind.block.x <- seq(20, 380, 20)  grid.X <- 2:16 grid.alpha.X <- c(0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8, 0.95) ## Strategy with same value of each tuning parameter for both components tun.sgPLS <- tuning.sgPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"),          ncomp = 2,keepX = NULL, alpha.x = NULL,grid.gX = grid.X,          grid.alpha.X = grid.alpha.X, setseed = 1, progressBar = FALSE,          ind.block.x = ind.block.x)   tun.sgPLS$keepX # for each component tun.sgPLS$alphaX # for each component ##For a sequential strategy tun.sgPLS.1 <- tuning.sgPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"),                ncomp = 1, keepX = NULL,  alpha.x = NULL, grid.gX = grid.X,          grid.alpha.X = grid.alpha.X, setseed = 1,           ind.block.x = ind.block.x)              tun.sgPLS.1$keepX # for the first component tun.sgPLS.1$alphaX # for the first component  tun.sgPLS.2 <- tuning.sgPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"),            ncomp = 2, keepX = tun.sgPLS.1$keepX,           alpha.x = tun.sgPLS.1$alphaX,           grid.gX = grid.X,           grid.alpha.X = grid.alpha.X,           setseed = 1,           ind.block.x = ind.block.x)   tun.sgPLS.2$keepX # for the second component tun.sgPLS.2$alphaX # for the second component } # }"},{"path":"/reference/tuning.sPLS.X.html","id":null,"dir":"Reference","previous_headings":"","what":"Choice of the tuning parameter (number of variables) related to predictor matrix for sPLS model (regression mode) â€” tuning.sPLS.X","title":"Choice of the tuning parameter (number of variables) related to predictor matrix for sPLS model (regression mode) â€” tuning.sPLS.X","text":"grid tuning parameter, function computes leave-one-M-fold cross-validation MSEP (Mean Square Error Prediction) sPLS model.","code":""},{"path":"/reference/tuning.sPLS.X.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Choice of the tuning parameter (number of variables) related to predictor matrix for sPLS model (regression mode) â€” tuning.sPLS.X","text":"","code":"tuning.sPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"), ncomp,     keepX = NULL, grid.X, setseed, progressBar = FALSE)"},{"path":"/reference/tuning.sPLS.X.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Choice of the tuning parameter (number of variables) related to predictor matrix for sPLS model (regression mode) â€” tuning.sPLS.X","text":"X Numeric matrix data frame \\((n \\times p)\\), observations \\(X\\) variables. Y Numeric matrix data frame \\((n \\times q)\\), observations \\(Y\\) variables. folds Positive integer. Number folds use validation=\"Mfold\". Defaults folds=10. validation Character string. kind (internal) cross-validation method use, (partially) matching one \"Mfolds\" (M-folds) \"loo\" (leave-one-). ncomp Number component investigating choice tuning parameter. keepX Vector integer indicating number variables keep component. See Details information. grid.X Vector integers defining values tuning parameter (corresponding number variables select) cross-validation score computed. setseed Integer indicating random number generation state. progressBar default set FALSE output progress bar computation.","code":""},{"path":"/reference/tuning.sPLS.X.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Choice of the tuning parameter (number of variables) related to predictor matrix for sPLS model (regression mode) â€” tuning.sPLS.X","text":"validation=\"Mfolds\", M-fold cross-validation performed calling Mfold. folds generated. number cross-validation folds specified argument folds. validation=\"loo\", leave-one-cross-validation performed calling loo function. case arguments folds ignored. keepX specified (default NULL), element keepX indicates value tuning parameter corresponding component. choice tuning parameters corresponding remaining components investigating evaluating cross-validation score different values defining grid.X.","code":""},{"path":"/reference/tuning.sPLS.X.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Choice of the tuning parameter (number of variables) related to predictor matrix for sPLS model (regression mode) â€” tuning.sPLS.X","text":"returned value list components: MSEP Vector containing cross-validation score computed grid keepX Value tuning parameter     cross-validation method reached minimum.","code":""},{"path":"/reference/tuning.sPLS.X.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Choice of the tuning parameter (number of variables) related to predictor matrix for sPLS model (regression mode) â€” tuning.sPLS.X","text":"Benoit Liquet Pierre Lafaye de Micheaux","code":""},{"path":"/reference/tuning.sPLS.X.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Choice of the tuning parameter (number of variables) related to predictor matrix for sPLS model (regression mode) â€” tuning.sPLS.X","text":"","code":"if (FALSE) { # \\dontrun{   ## Simulation of Datasets X (with group variables) and Y a multivariate response variable  n <- 200 sigma.e <- 0.5 p <- 400 q <- 10 theta.x1 <- c(rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5), rep(1.5, 15),       rep(0, 5), rep(-1.5, 15), rep(0, 325)) theta.x2 <- c(rep(0, 320), rep(1, 15), rep(0, 5), rep(-1, 15), rep(0, 5),       rep(1.5, 15), rep(0, 5), rep(-1.5, 15), rep(0, 5))  set.seed(125) theta.y1 <- runif(10, 0.5, 2) theta.y2 <- runif(10, 0.5, 2)    temp <-  matrix(c(theta.y1, theta.y2), nrow = 2, byrow = TRUE)  Sigmax <- matrix(0, nrow = p, ncol = p) diag(Sigmax) <- sigma.e ^ 2 Sigmay <- matrix(0, nrow = q, ncol = q) diag(Sigmay) <- sigma.e ^ 2  gam1 <- rnorm(n) gam2 <- rnorm(n)  X <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% matrix(c(theta.x1, theta.x2),      nrow = 2, byrow = TRUE) + rmvnorm(n, mean = rep(0, p), sigma =      Sigmax, method = \"svd\") Y <- matrix(c(gam1, gam2), ncol = 2, byrow = FALSE) %*% t(svd(temp)$v)      + rmvnorm(n, mean = rep(0, q), sigma = Sigmay, method = \"svd\")   grid.X <- c(20, 30, 40, 50, 60, 70, 80, 90, 100, 120, 150, 200, 250, 300)  ## Strategy with same value for both components tun.sPLS <- tuning.sPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"),      ncomp = 2, keepX = NULL, grid.X = grid.X, setseed = 1) tun.sPLS$keepX # for each component  ##For a sequential strategy tun.sPLS.1 <- tuning.sPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"),      ncomp = 1, keepX = NULL, grid.X = grid.X, setseed = 1)  tun.sPLS.1$keepX # for the first component  tun.sPLS.2 <- tuning.sPLS.X(X, Y, folds = 10, validation = c(\"Mfold\", \"loo\"),      ncomp = 2, keepX = tun.sPLS.1$keepX , grid.X = grid.X, setseed = 1) tun.sPLS.2$keepX # for the second component  } # }"}]
